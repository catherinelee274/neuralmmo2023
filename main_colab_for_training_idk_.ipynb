{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mount"
      ],
      "metadata": {
        "id": "h1Br03PGzCw8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkwE0p7_5nuf"
      },
      "outputs": [],
      "source": [
        "# Check if (NVIDIA) GPU is available\n",
        "import torch\n",
        "assert torch.cuda.is_available, \"CUDA gpu not available\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2ClGXe5915p",
        "outputId": "acc175d0-b378-4d78-b199-75691c2480d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE88bzEn97cW"
      },
      "outputs": [],
      "source": [
        "# Set up the work directory\n",
        "import os\n",
        "assert os.path.exists(\"/content/drive/MyDrive\"), \"Google Drive not mounted\"\n",
        "\n",
        "work_dir = \"/content/drive/MyDrive/nmmo/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdJ7At735tDo"
      },
      "source": [
        "## Install nmmo env and pufferlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R8o0m-i4zlh",
        "outputId": "646a8c91-ca2b-4bf9-950a-f050345088a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "tensorflow 2.14.0 requires numpy>=1.23.5, but you have numpy 1.23.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mName: nmmo\n",
            "Version: 2.0.3\n",
            "Summary: Neural MMO is a platform for multiagent intelligence research inspired by Massively Multiplayer Online (MMO) role-playing games. Documentation hosted at neuralmmo.github.io.\n",
            "Home-page: https://github.com/neuralmmo/environment\n",
            "Author: Joseph Suarez\n",
            "Author-email: jsuarez@mit.edu\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: autobahn, dill, gym, imageio, numpy, ordered-set, pettingzoo, psutil, py, pylint, pytest, pytest-benchmark, scipy, tqdm, Twisted, vec-noise\n",
            "Required-by: \n",
            "Name: pufferlib\n",
            "Version: 0.4.5\n",
            "Summary: PufferAI LibraryPufferAI's library of RL tools and utilities\n",
            "Home-page: https://github.com/PufferAI/PufferLib\n",
            "Author: Joseph Suarez\n",
            "Author-email: jsuarez@mit.edu\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: cython, filelock, gym, numpy, opencv-python, openskill, pettingzoo\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "# Install nmmo env and pufferlib\n",
        "!pip install nmmo pufferlib > /dev/null\n",
        "!pip show nmmo  # should be 2.0.3\n",
        "!pip show pufferlib # should be 0.4.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hZSA88YY534n",
        "outputId": "662b076e-6e7c-4de1-df28-56bd87d95ca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate==0.21.0\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/244.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (1.23.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (5.9.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.21.0\n",
            "Collecting bitsandbytes==0.41.1\n",
            "  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.41.1\n",
            "Collecting dash==2.11.1\n",
            "  Downloading dash-2.11.1-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Flask<2.3.0,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash==2.11.1) (2.2.5)\n",
            "Collecting Werkzeug<2.3.0 (from dash==2.11.1)\n",
            "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash==2.11.1) (5.15.0)\n",
            "Collecting dash-html-components==2.0.0 (from dash==2.11.1)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash==2.11.1)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting dash-table==5.0.0 (from dash==2.11.1)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash==2.11.1) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dash==2.11.1) (2.31.0)\n",
            "Collecting retrying (from dash==2.11.1)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Collecting ansi2html (from dash==2.11.1)\n",
            "  Downloading ansi2html-1.9.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash==2.11.1) (1.5.8)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask<2.3.0,>=1.0.4->dash==2.11.1) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<2.3.0,>=1.0.4->dash==2.11.1) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask<2.3.0,>=1.0.4->dash==2.11.1) (8.1.7)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash==2.11.1) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash==2.11.1) (23.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from Werkzeug<2.3.0->dash==2.11.1) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dash==2.11.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dash==2.11.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dash==2.11.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dash==2.11.1) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying->dash==2.11.1) (1.16.0)\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, Werkzeug, retrying, ansi2html, dash\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 3.0.1\n",
            "    Uninstalling Werkzeug-3.0.1:\n",
            "      Successfully uninstalled Werkzeug-3.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.14.0 requires numpy>=1.23.5, but you have numpy 1.23.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Werkzeug-2.2.3 ansi2html-1.9.1 dash-2.11.1 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 retrying-1.3.4\n",
            "Collecting openelm\n",
            "  Downloading OpenELM-0.9.2-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.2.0 (from openelm)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb>=0.13 (from openelm)\n",
            "  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openelm) (1.23.3)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from openelm) (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from openelm) (4.35.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from openelm) (0.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from openelm) (2.31.0)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.10/dist-packages (from openelm) (2.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from openelm) (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from openelm) (3.7.1)\n",
            "Collecting langchain==0.0.225 (from openelm)\n",
            "  Downloading langchain-0.0.225-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions==4.5 in /usr/local/lib/python3.10/dist-packages (from openelm) (4.5.0)\n",
            "Collecting openai (from openelm)\n",
            "  Downloading openai-1.3.9-py3-none-any.whl (221 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from openelm) (5.15.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.225->openelm) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.225->openelm) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.225->openelm) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.225->openelm) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.225->openelm)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk<0.0.21,>=0.0.20 (from langchain==0.0.225->openelm)\n",
            "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.225->openelm) (2.8.7)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.225->openelm)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.225->openelm) (1.10.13)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.225->openelm) (8.2.3)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core>=1.2.0->openelm)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.2.0->openelm)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.2.0->openelm) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->openelm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->openelm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->openelm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->openelm) (2023.11.17)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->openelm) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->openelm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->openelm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->openelm) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->openelm) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->openelm) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.22.0->openelm) (0.19.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.22.0->openelm) (2023.6.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.22.0->openelm) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.22.0->openelm) (4.66.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13->openelm) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb>=0.13->openelm)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13->openelm) (5.9.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb>=0.13->openelm)\n",
            "  Downloading sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb>=0.13->openelm)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle (from wandb>=0.13->openelm)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13->openelm) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13->openelm) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13->openelm) (3.20.3)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask->openelm) (2.2.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask->openelm) (2.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->openelm) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->openelm) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->openelm) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->openelm) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->openelm) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->openelm) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->openelm) (2.8.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->openelm) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->openelm) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai->openelm)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->openelm) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->openelm) (1.10.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->openelm) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->openelm) (3.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.225->openelm) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.225->openelm) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.225->openelm) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.225->openelm) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.225->openelm) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->openelm) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.225->openelm)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.225->openelm)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13->openelm) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13->openelm)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore==1.* (from httpx<1,>=0.23.0->openai->openelm)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai->openelm)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->openelm) (2.1.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.225->openelm) (3.0.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->openelm) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13->openelm)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.225->openelm)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=8532feb73da2709b277a881b88a6f11fdf92b2c2f8b1f91c70b5532d8317644f\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, smmap, setproctitle, sentry-sdk, omegaconf, mypy-extensions, marshmallow, h11, docker-pycreds, typing-inspect, openapi-schema-pydantic, langchainplus-sdk, hydra-core, httpcore, gitdb, httpx, GitPython, dataclasses-json, wandb, openai, langchain, openelm\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.40 antlr4-python3-runtime-4.9.3 dataclasses-json-0.5.14 docker-pycreds-0.4.0 gitdb-4.0.11 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 hydra-core-1.3.2 langchain-0.0.225 langchainplus-sdk-0.0.20 marshmallow-3.20.1 mypy-extensions-1.0.0 omegaconf-2.3.0 openai-1.3.9 openapi-schema-pydantic-1.2.4 openelm-0.9.2 sentry-sdk-1.39.1 setproctitle-1.3.3 smmap-5.0.1 typing-inspect-0.9.0 wandb-0.16.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==2.0.3\n",
            "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (2023.3.post1)\n",
            "Collecting tzdata>=2022.1 (from pandas==2.0.3)\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3) (1.23.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.16.0)\n",
            "Installing collected packages: tzdata, pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.0.3 tzdata-2023.3\n",
            "Requirement already satisfied: plotly==5.15.0 in /usr/local/lib/python3.10/dist-packages (5.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly==5.15.0) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly==5.15.0) (23.2)\n",
            "Requirement already satisfied: psutil==5.9.3 in /usr/local/lib/python3.10/dist-packages (5.9.3)\n",
            "Collecting scikit-learn==1.3.0\n",
            "  Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.0) (1.23.3)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.0) (1.10.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.0) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.0) (3.2.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.3.0\n",
            "Collecting tensorboard==2.11.2\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11.2) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11.2) (1.59.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11.2) (2.17.3)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard==2.11.2)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11.2) (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11.2) (1.23.3)\n",
            "Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11.2) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11.2) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11.2) (67.7.2)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard==2.11.2)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard==2.11.2)\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11.2) (2.2.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11.2) (0.42.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.11.2) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.11.2) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.11.2) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.11.2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.11.2) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.11.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.11.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.11.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.11.2) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard==2.11.2) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.11.2) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.11.2) (3.2.2)\n",
            "\n",
            "Installing collected packages: tensorboard-plugin-wit, tensorboard-data-server, google-auth-oauthlib, tensorboard\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.14.1\n",
            "    Uninstalling tensorboard-2.14.1:\n",
            "      Successfully uninstalled tensorboard-2.14.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow 2.14.0 requires numpy>=1.23.5, but you have numpy 1.23.3 which is incompatible.\n",
            "tensorflow 2.14.0 requires tensorboard<2.15,>=2.14, but you have tensorboard 2.11.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1\n",
            "Collecting tiktoken==0.4.0\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.4.0) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.4.0\n",
            "Collecting torch==1.13.1\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (4.5.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.42.0)\n",
            "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu118\n",
            "    Uninstalling torch-2.1.0+cu118:\n",
            "      Successfully uninstalled torch-2.1.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting traitlets==5.9.0\n",
            "  Downloading traitlets-5.9.0-py3-none-any.whl (117 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/117.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m112.6/117.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: traitlets\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.7.1\n",
            "    Uninstalling traitlets-5.7.1:\n",
            "      Successfully uninstalled traitlets-5.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed traitlets-5.9.0\n",
            "Collecting transformers==4.31.0\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (1.23.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2023.11.17)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.31.0\n",
            "Collecting wandb==0.13.7\n",
            "  Downloading wandb-0.13.7-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.7) (8.1.7)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.7) (3.1.40)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.7) (2.31.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.7) (2.3)\n",
            "Collecting shortuuid>=0.5.0 (from wandb==0.13.7)\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.7) (5.9.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.7) (1.39.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.7) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.7) (6.0.1)\n",
            "Collecting pathtools (from wandb==0.13.7)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.7) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.7) (67.7.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.13.7) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb==0.13.7) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython>=1.0.0->wandb==0.13.7) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb==0.13.7) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb==0.13.7) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb==0.13.7) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb==0.13.7) (2023.11.17)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb==0.13.7) (5.0.1)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8793 sha256=735365d24ff602c64a39bb6dd1203c826c437ed323e797eabb279270ef326112\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, shortuuid, wandb\n",
            "  Attempting uninstall: wandb\n",
            "    Found existing installation: wandb 0.16.1\n",
            "    Uninstalling wandb-0.16.1:\n",
            "      Successfully uninstalled wandb-0.16.1\n",
            "Successfully installed pathtools-0.1.2 shortuuid-1.0.11 wandb-0.13.7\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate==0.21.0\n",
        "!pip install bitsandbytes==0.41.1\n",
        "!pip install dash==2.11.1\n",
        "!pip install openelm\n",
        "!pip install pandas==2.0.3\n",
        "!pip install plotly==5.15.0\n",
        "!pip install psutil==5.9.3\n",
        "!pip install scikit-learn==1.3.0\n",
        "!pip install tensorboard==2.11.2\n",
        "!pip install tiktoken==0.4.0\n",
        "!pip install torch==1.13.1\n",
        "!pip install traitlets==5.9.0\n",
        "!pip install transformers==4.31.0\n",
        "!pip install wandb==0.13.7"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "MZXUo-xfIe61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: PPO + LSTM"
      ],
      "metadata": {
        "id": "Z6Ky2YGKH6JL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6h8VPC_6BrI"
      },
      "outputs": [],
      "source": [
        "# pylint: disable=all\n",
        "# PufferLib's customized CleanRL PPO + LSTM implementation\n",
        "from pdb import set_trace as T\n",
        "\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from datetime import timedelta\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import numpy as np\n",
        "import psutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pufferlib\n",
        "import pufferlib.emulation\n",
        "import pufferlib.frameworks.cleanrl\n",
        "import pufferlib.policy_pool\n",
        "import pufferlib.policy_ranker\n",
        "import pufferlib.utils\n",
        "import pufferlib.vectorization\n",
        "\n",
        "\n",
        "def unroll_nested_dict(d):\n",
        "    if not isinstance(d, dict):\n",
        "        return d\n",
        "\n",
        "    for k, v in d.items():\n",
        "        if isinstance(v, dict):\n",
        "            for k2, v2 in unroll_nested_dict(v):\n",
        "                yield f\"{k}/{k2}\", v2\n",
        "        else:\n",
        "            yield k, v\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class CleanPuffeRL:\n",
        "    env_creator: callable = None\n",
        "    env_creator_kwargs: dict = None\n",
        "    agent: nn.Module = None\n",
        "    agent_creator: callable = None\n",
        "    agent_kwargs: dict = None\n",
        "\n",
        "    path = '/content/drive/MyDrive/nmmo'\n",
        "\n",
        "\n",
        "    exp_name: str = os.path.basename(path)\n",
        "\n",
        "    data_dir: str = 'data'\n",
        "    record_loss: bool = False\n",
        "    checkpoint_interval: int = 1\n",
        "    seed: int = 1\n",
        "    torch_deterministic: bool = True\n",
        "    vectorization: ... = pufferlib.vectorization.Serial\n",
        "    device: str = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
        "    total_timesteps: int = 10_000_000\n",
        "    learning_rate: float = 2.5e-4\n",
        "    num_buffers: int = 1\n",
        "    num_envs: int = 8\n",
        "    num_cores: int = psutil.cpu_count(logical=False)\n",
        "    cpu_offload: bool = True\n",
        "    verbose: bool = True\n",
        "    batch_size: int = 2**14\n",
        "    policy_store: pufferlib.policy_store.PolicyStore = None\n",
        "    policy_ranker: pufferlib.policy_ranker.PolicyRanker = None\n",
        "\n",
        "    policy_pool: pufferlib.policy_pool.PolicyPool = None\n",
        "    policy_selector: pufferlib.policy_ranker.PolicySelector = None\n",
        "\n",
        "    # Wandb\n",
        "    wandb_entity: str = None\n",
        "    wandb_project: str = None\n",
        "    wandb_extra_data: dict = None\n",
        "\n",
        "    # Selfplay\n",
        "    selfplay_learner_weight: float = 1.0\n",
        "    selfplay_num_policies: int = 1\n",
        "\n",
        "    def __post_init__(self, *args, **kwargs):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "        # If data_dir is provided, load the resume state\n",
        "        resume_state = {}\n",
        "        if self.data_dir is not None:\n",
        "          path = os.path.join(self.data_dir, f\"trainer.pt\")\n",
        "          if os.path.exists(path):\n",
        "            print(f\"Loaded checkpoint from {path}\")\n",
        "            resume_state = torch.load(path)\n",
        "            print(f\"Resuming from update {resume_state['update']} \"\n",
        "                  f\"with policy {resume_state['policy_checkpoint_name']}\")\n",
        "\n",
        "        self.wandb_run_id = resume_state.get(\"wandb_run_id\", None)\n",
        "        self.learning_rate = resume_state.get(\"learning_rate\", self.learning_rate)\n",
        "\n",
        "        self.global_step = resume_state.get(\"global_step\", 0)\n",
        "        self.agent_step = resume_state.get(\"agent_step\", 0)\n",
        "        self.update = resume_state.get(\"update\", 0)\n",
        "\n",
        "        self.total_updates = self.total_timesteps // self.batch_size\n",
        "        self.envs_per_worker = self.num_envs // self.num_cores\n",
        "        assert self.num_cores * self.envs_per_worker == self.num_envs\n",
        "\n",
        "        # Seed everything\n",
        "        random.seed(self.seed)\n",
        "        np.random.seed(self.seed)\n",
        "        if self.seed is not None:\n",
        "            torch.manual_seed(self.seed)\n",
        "        torch.backends.cudnn.deterministic = self.torch_deterministic\n",
        "\n",
        "        # Create environments\n",
        "        self.process = psutil.Process()\n",
        "        allocated = self.process.memory_info().rss\n",
        "        self.buffers = [\n",
        "            self.vectorization(\n",
        "                self.env_creator,\n",
        "                env_kwargs=self.env_creator_kwargs,\n",
        "                num_workers=self.num_cores,\n",
        "                envs_per_worker=self.envs_per_worker,\n",
        "            )\n",
        "            for _ in range(self.num_buffers)\n",
        "        ]\n",
        "        self.num_agents = self.buffers[0].num_agents\n",
        "\n",
        "        # If an agent_creator is provided, use envs (=self.buffers[0]) to create the agent\n",
        "        self.agent = pufferlib.emulation.make_object(\n",
        "            self.agent, self.agent_creator, self.buffers[:1], self.agent_kwargs)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                \"Allocated %.2f MB to environments. Only accurate for Serial backend.\"\n",
        "                % ((self.process.memory_info().rss - allocated) / 1e6)\n",
        "            )\n",
        "\n",
        "        # Create policy store\n",
        "        if self.policy_store is None:\n",
        "            if self.data_dir is not None:\n",
        "                self.policy_store = pufferlib.policy_store.DirectoryPolicyStore(\n",
        "                    os.path.join(self.data_dir, \"policies\")\n",
        "                )\n",
        "\n",
        "        # Create policy ranker\n",
        "        if self.policy_ranker is None:\n",
        "            if self.data_dir is not None:\n",
        "                db_file = os.path.join(self.data_dir, \"ranking.sqlite\")\n",
        "                self.policy_ranker = pufferlib.policy_ranker.OpenSkillRanker(db_file, \"anchor\")\n",
        "            if \"learner\" not in self.policy_ranker.ratings():\n",
        "                self.policy_ranker.add_policy(\"learner\")\n",
        "\n",
        "        # Setup agent\n",
        "        if \"policy_checkpoint_name\" in resume_state:\n",
        "          self.agent = self.policy_store.get_policy(\n",
        "            resume_state[\"policy_checkpoint_name\"]\n",
        "          ).policy(policy_args=[self.buffers[0]])\n",
        "\n",
        "        # TODO: this can be cleaned up\n",
        "        self.agent.is_recurrent = hasattr(self.agent, \"lstm\")\n",
        "        self.agent = self.agent.to(self.device)\n",
        "\n",
        "        # Setup policy pool\n",
        "        if self.policy_pool is None:\n",
        "            self.policy_pool = pufferlib.policy_pool.PolicyPool(\n",
        "                self.agent,\n",
        "                \"learner\",\n",
        "                num_envs=self.num_envs,\n",
        "                num_agents=self.num_agents,\n",
        "                learner_weight=self.selfplay_learner_weight,\n",
        "                num_policies=self.selfplay_num_policies,\n",
        "            )\n",
        "\n",
        "        # Setup policy selector\n",
        "        if self.policy_selector is None:\n",
        "            self.policy_selector = pufferlib.policy_ranker.PolicySelector(\n",
        "                self.selfplay_num_policies - 1, exclude_names=\"learner\"\n",
        "            )\n",
        "\n",
        "        # Setup optimizer\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.agent.parameters(), lr=self.learning_rate, eps=1e-5\n",
        "        )\n",
        "        if \"optimizer_state_dict\" in resume_state:\n",
        "          self.optimizer.load_state_dict(resume_state[\"optimizer_state_dict\"])\n",
        "\n",
        "        ### Allocate Storage\n",
        "        next_obs, next_done, next_lstm_state = [], [], []\n",
        "        for i, envs in enumerate(self.buffers):\n",
        "            envs.async_reset(self.seed + i)\n",
        "            next_done.append(\n",
        "                torch.zeros((self.num_envs * self.num_agents,)).to(self.device)\n",
        "            )\n",
        "            next_obs.append([])\n",
        "\n",
        "            if self.agent.is_recurrent:\n",
        "                shape = (\n",
        "                    self.agent.lstm.num_layers,\n",
        "                    self.num_envs * self.num_agents,\n",
        "                    self.agent.lstm.hidden_size,\n",
        "                )\n",
        "                next_lstm_state.append(\n",
        "                    (\n",
        "                        torch.zeros(shape).to(self.device),\n",
        "                        torch.zeros(shape).to(self.device),\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                next_lstm_state.append(None)\n",
        "\n",
        "        allocated_torch = torch.cuda.memory_allocated(self.device)\n",
        "        allocated_cpu = self.process.memory_info().rss\n",
        "        self.data = SimpleNamespace(\n",
        "            buf=0,\n",
        "            sort_keys=[],\n",
        "            next_obs=next_obs,\n",
        "            next_done=next_done,\n",
        "            next_lstm_state=next_lstm_state,\n",
        "            obs=torch.zeros(\n",
        "                self.batch_size + 1, *self.buffers[0].single_observation_space.shape\n",
        "            ).to(\"cpu\" if self.cpu_offload else self.device),\n",
        "            actions=torch.zeros(\n",
        "                self.batch_size + 1, *self.buffers[0].single_action_space.shape, dtype=int\n",
        "            ).to(self.device),\n",
        "            logprobs=torch.zeros(self.batch_size + 1).to(self.device),\n",
        "            rewards=torch.zeros(self.batch_size + 1).to(self.device),\n",
        "            dones=torch.zeros(self.batch_size + 1).to(self.device),\n",
        "            values=torch.zeros(self.batch_size + 1).to(self.device),\n",
        "        )\n",
        "\n",
        "        allocated_torch = torch.cuda.memory_allocated(self.device) - allocated_torch\n",
        "        allocated_cpu = self.process.memory_info().rss - allocated_cpu\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                \"Allocated to storage - Pytorch: %.2f GB, System: %.2f GB\"\n",
        "                % (allocated_torch / 1e9, allocated_cpu / 1e9)\n",
        "            )\n",
        "\n",
        "        if self.record_loss and self.data_dir is not None:\n",
        "            self.loss_file = os.path.join(self.data_dir, \"loss.txt\")\n",
        "            with open(self.loss_file, \"w\") as f:\n",
        "                pass\n",
        "            self.action_file = os.path.join(self.data_dir, \"actions.txt\")\n",
        "            with open(self.action_file, \"w\") as f:\n",
        "                pass\n",
        "\n",
        "        if self.wandb_entity is not None:\n",
        "            self.wandb_run_id = self.wandb_run_id or wandb.util.generate_id()\n",
        "\n",
        "            wandb.init(\n",
        "                id=self.wandb_run_id,\n",
        "                project=self.wandb_project,\n",
        "                entity=self.wandb_entity,\n",
        "                config=self.wandb_extra_data or {},\n",
        "                sync_tensorboard=True,\n",
        "                name=self.exp_name,\n",
        "                monitor_gym=True,\n",
        "                save_code=True,\n",
        "                resume=\"allow\",\n",
        "            )\n",
        "\n",
        "    @pufferlib.utils.profile\n",
        "    def evaluate(self, show_progress=False):\n",
        "        # Pick new policies for the policy pool\n",
        "        # TODO: find a way to not switch mid-stream\n",
        "        self.policy_pool.update_policies({\n",
        "            p.name: p.policy(\n",
        "                policy_args=[self.buffers[0]],\n",
        "                device=self.device,\n",
        "            ) for p in self.policy_store.select_policies(self.policy_selector)\n",
        "        })\n",
        "\n",
        "        allocated_torch = torch.cuda.memory_allocated(self.device)\n",
        "        allocated_cpu = self.process.memory_info().rss\n",
        "        ptr = env_step_time = inference_time = agent_steps_collected = 0\n",
        "        padded_steps_collected = 0\n",
        "\n",
        "        step = 0\n",
        "        infos = defaultdict(lambda: defaultdict(list))\n",
        "        stats = defaultdict(lambda: defaultdict(list))\n",
        "        performance = defaultdict(list)\n",
        "        progress_bar = tqdm(total=self.batch_size, disable=not show_progress)\n",
        "\n",
        "        data = self.data\n",
        "        while True:\n",
        "            buf = data.buf\n",
        "\n",
        "            step += 1\n",
        "            if ptr == self.batch_size + 1:\n",
        "                break\n",
        "\n",
        "            start = time.time()\n",
        "            o, r, d, i = self.buffers[buf].recv() # obs, rewards, dones, infos\n",
        "            print(\"dones:\", d)\n",
        "            print(\"infos\", i)\n",
        "            env_step_time += time.time() - start\n",
        "\n",
        "            i = self.policy_pool.update_scores(i, \"return\")\n",
        "\n",
        "            '''\n",
        "            for profile in self.buffers[buf].profile():\n",
        "                for k, v in profile.items():\n",
        "                    performance[k].append(v[\"delta\"])\n",
        "            '''\n",
        "\n",
        "            o = torch.Tensor(o)\n",
        "            if not self.cpu_offload:\n",
        "                o = o.to(self.device)\n",
        "\n",
        "            r = torch.Tensor(r).float().to(self.device).view(-1) # is this encoding the rewarrd?\n",
        "\n",
        "            if len(d) != 0 and len(data.next_done[buf]) != 0:\n",
        "                alive_mask = (data.next_done[buf].cpu() + torch.Tensor(d)) != 2\n",
        "                data.next_done[buf] = torch.Tensor(d).to(self.device)\n",
        "            else:\n",
        "                alive_mask = [1 for _ in range(len(o))]\n",
        "\n",
        "            agent_steps_collected += sum(alive_mask)\n",
        "            padded_steps_collected += len(alive_mask)\n",
        "\n",
        "            # ALGO LOGIC: action logic\n",
        "            start = time.time()\n",
        "            with torch.no_grad():\n",
        "                (\n",
        "                    actions,\n",
        "                    logprob,\n",
        "                    value,\n",
        "                    data.next_lstm_state[buf], #lstm here ?\n",
        "                ) = self.policy_pool.forwards(\n",
        "                    o.to(self.device),\n",
        "                    data.next_lstm_state[buf],\n",
        "                    data.next_done[buf],\n",
        "                )\n",
        "                value = value.flatten()\n",
        "            inference_time += time.time() - start\n",
        "\n",
        "            # TRY NOT TO MODIFY: execute the game\n",
        "            start = time.time()\n",
        "            self.buffers[buf].send(actions.cpu().numpy(), None)\n",
        "            env_step_time += time.time() - start\n",
        "            data.buf = (data.buf + 1) % self.num_buffers\n",
        "\n",
        "            # Index alive mask with policy pool idxs...\n",
        "            # TODO: Find a way to avoid having to do this\n",
        "            if self.selfplay_learner_weight > 0:\n",
        "              alive_mask = np.array(alive_mask) * self.policy_pool.learner_mask\n",
        "\n",
        "            for idx in np.where(alive_mask)[0]:\n",
        "                if ptr == self.batch_size + 1:\n",
        "                    break\n",
        "\n",
        "                data.obs[ptr] = o[idx]\n",
        "                data.values[ptr] = value[idx]\n",
        "                data.actions[ptr] = actions[idx]\n",
        "                data.logprobs[ptr] = logprob[idx]\n",
        "                data.sort_keys.append((buf, idx, step))\n",
        "\n",
        "                if len(d) != 0:\n",
        "                    data.rewards[ptr] = r[idx]\n",
        "                    data.dones[ptr] = d[idx]\n",
        "\n",
        "                ptr += 1\n",
        "                progress_bar.update(1)\n",
        "\n",
        "            '''\n",
        "            for ii in i:\n",
        "                if not ii:\n",
        "                    continue\n",
        "\n",
        "                for agent_i, values in ii.items():\n",
        "                    for name, stat in unroll_nested_dict(values):\n",
        "                        infos[name].append(stat)\n",
        "                        try:\n",
        "                            stat = float(stat)\n",
        "                            stats[name].append(stat)\n",
        "                        except:\n",
        "                            continue\n",
        "            '''\n",
        "\n",
        "            for policy_name, policy_i in i.items():\n",
        "                for agent_i in policy_i:\n",
        "                    if not agent_i:\n",
        "                        continue\n",
        "\n",
        "                    for name, stat in unroll_nested_dict(agent_i):\n",
        "                        infos[policy_name][name].append(stat)\n",
        "                        if 'Task_eval_fn' in name:\n",
        "                            # Temporary hack for NMMO competition\n",
        "                            continue\n",
        "                        try:\n",
        "                            stat = float(stat)\n",
        "                            stats[policy_name][name].append(stat)\n",
        "                        except:\n",
        "                            continue\n",
        "\n",
        "        if self.policy_pool.scores and self.policy_ranker is not None:\n",
        "          self.policy_ranker.update_ranks(\n",
        "              self.policy_pool.scores,\n",
        "              wandb_policies=[self.policy_pool._learner_name]\n",
        "              if self.wandb_entity\n",
        "              else [],\n",
        "              step=self.global_step,\n",
        "          )\n",
        "          self.policy_pool.scores = {}\n",
        "\n",
        "        env_sps = int(agent_steps_collected / env_step_time)\n",
        "        inference_sps = int(padded_steps_collected / inference_time)\n",
        "\n",
        "        progress_bar.set_description(\n",
        "            \"Eval: \"\n",
        "            + \", \".join(\n",
        "                [\n",
        "                    f\"Env SPS: {env_sps}\",\n",
        "                    f\"Inference SPS: {inference_sps}\",\n",
        "                    f\"Agent Steps: {agent_steps_collected}\",\n",
        "                    *[f\"{k}: {np.mean(v):.2f}\" for k, v in stats['learner'].items()],\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.global_step += self.batch_size\n",
        "\n",
        "        if self.wandb_entity:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"performance/env_time\": env_step_time,\n",
        "                    \"performance/env_sps\": env_sps,\n",
        "                    \"performance/inference_time\": inference_time,\n",
        "                    \"performance/inference_sps\": inference_sps,\n",
        "                    **{\n",
        "                        f\"performance/env/{k}\": np.mean(v)\n",
        "                        for k, v in performance.items()\n",
        "                    },\n",
        "                    **{f\"charts/{k}\": np.mean(v) for k, v in stats['learner'].items()},\n",
        "                    \"charts/reward\": float(torch.mean(data.rewards)),\n",
        "                    \"agent_steps\": self.global_step,\n",
        "                    \"global_step\": self.global_step,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        allocated_torch = torch.cuda.memory_allocated(self.device) - allocated_torch\n",
        "        allocated_cpu = self.process.memory_info().rss - allocated_cpu\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                \"Allocated during evaluation - Pytorch: %.2f GB, System: %.2f GB\"\n",
        "                % (allocated_torch / 1e9, allocated_cpu / 1e9)\n",
        "            )\n",
        "\n",
        "        uptime = timedelta(seconds=int(time.time() - self.start_time))\n",
        "        print(\n",
        "            f\"Epoch: {self.update} - {self.global_step // 1000}K steps - {uptime} Elapsed\\n\"\n",
        "            f\"\\tSteps Per Second: Env={env_sps}, Inference={inference_sps}\"\n",
        "        )\n",
        "\n",
        "        progress_bar.close()\n",
        "        return data, stats, infos\n",
        "\n",
        "    @pufferlib.utils.profile\n",
        "    def train(\n",
        "        self,\n",
        "        batch_rows=32,\n",
        "        update_epochs=4,\n",
        "        bptt_horizon=16,\n",
        "        gamma=0.99,\n",
        "        gae_lambda=0.95,\n",
        "        anneal_lr=True,\n",
        "        norm_adv=True,\n",
        "        clip_coef=0.1,\n",
        "        clip_vloss=True,\n",
        "        ent_coef=0.01,\n",
        "        vf_coef=0.5,\n",
        "        max_grad_norm=0.5,\n",
        "        target_kl=None,\n",
        "    ):\n",
        "        if self.done_training():\n",
        "            raise RuntimeError(\n",
        "                f\"Trying to train for more than max_updates={self.total_updates} updates\"\n",
        "            )\n",
        "\n",
        "        # assert self.num_steps % bptt_horizon == 0, \"num_steps must be divisible by bptt_horizon\"\n",
        "        allocated_torch = torch.cuda.memory_allocated(self.device)\n",
        "        allocated_cpu = self.process.memory_info().rss\n",
        "\n",
        "        # Annealing the rate if instructed to do so.\n",
        "        if anneal_lr:\n",
        "            frac = 1.0 - (self.update - 1.0) / self.total_updates\n",
        "            lrnow = frac * self.learning_rate\n",
        "            self.optimizer.param_groups[0][\"lr\"] = lrnow\n",
        "\n",
        "        # Sort here\n",
        "        data = self.data\n",
        "        idxs = sorted(range(len(data.sort_keys)), key=data.sort_keys.__getitem__)\n",
        "        data.sort_keys = []\n",
        "\n",
        "        num_minibatches = self.batch_size // bptt_horizon // batch_rows\n",
        "        b_idxs = (\n",
        "            torch.Tensor(idxs)\n",
        "            .long()[:-1]\n",
        "            .reshape(batch_rows, num_minibatches, bptt_horizon)\n",
        "            .transpose(0, 1)\n",
        "        )\n",
        "\n",
        "        # bootstrap value if not done\n",
        "        with torch.no_grad():\n",
        "            advantages = torch.zeros(self.batch_size, device=self.device)\n",
        "            lastgaelam = 0\n",
        "            for t in reversed(range(self.batch_size)):\n",
        "                i, i_nxt = idxs[t], idxs[t + 1]\n",
        "                nextnonterminal = 1.0 - data.dones[i_nxt]\n",
        "                nextvalues = data.values[i_nxt]\n",
        "                delta = (\n",
        "                    data.rewards[i_nxt]\n",
        "                    + gamma * nextvalues * nextnonterminal\n",
        "                    - data.values[i]\n",
        "                )\n",
        "                print(delta)\n",
        "                advantages[t] = lastgaelam = (\n",
        "                    delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
        "                )\n",
        "\n",
        "        # Flatten the batch\n",
        "        data.b_obs = b_obs = data.obs[b_idxs]\n",
        "        b_actions = data.actions[b_idxs]\n",
        "        b_logprobs = data.logprobs[b_idxs]\n",
        "        b_dones = data.dones[b_idxs]\n",
        "        b_values = data.values[b_idxs]\n",
        "        b_advantages = advantages.reshape(\n",
        "            batch_rows, num_minibatches, bptt_horizon\n",
        "        ).transpose(0, 1)\n",
        "        b_returns = b_advantages + b_values\n",
        "\n",
        "        # Optimizing the policy and value network\n",
        "        train_time = time.time()\n",
        "        clipfracs = []\n",
        "        for epoch in range(update_epochs): #epoch\n",
        "            lstm_state = None\n",
        "            for mb in range(num_minibatches):\n",
        "                mb_obs = b_obs[mb].to(self.device)\n",
        "                mb_actions = b_actions[mb].contiguous()\n",
        "                mb_values = b_values[mb].reshape(-1)\n",
        "                mb_advantages = b_advantages[mb].reshape(-1)\n",
        "                mb_returns = b_returns[mb].reshape(-1)\n",
        "\n",
        "                if self.agent.is_recurrent:\n",
        "                    (\n",
        "                        _,\n",
        "                        newlogprob,\n",
        "                        entropy,\n",
        "                        newvalue,\n",
        "                        lstm_state,\n",
        "                    ) = self.agent.get_action_and_value(\n",
        "                        mb_obs, state=lstm_state, done=b_dones[mb], action=mb_actions\n",
        "                    )\n",
        "                    lstm_state = (lstm_state[0].detach(), lstm_state[1].detach()) # lstm\n",
        "                    print(\"lstm_state:\", lstm_state)\n",
        "                else:\n",
        "                    _, newlogprob, entropy, newvalue = self.agent.get_action_and_value(\n",
        "                        mb_obs.reshape(\n",
        "                            -1, *self.buffers[0].single_observation_space.shape\n",
        "                        ),\n",
        "                        action=mb_actions,\n",
        "                    )\n",
        "\n",
        "                logratio = newlogprob - b_logprobs[mb].reshape(-1)\n",
        "                ratio = logratio.exp()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
        "                    old_approx_kl = (-logratio).mean()\n",
        "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
        "                    clipfracs += [\n",
        "                        ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
        "                    ]\n",
        "\n",
        "                mb_advantages = mb_advantages.reshape(-1)\n",
        "                if norm_adv:\n",
        "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (\n",
        "                        mb_advantages.std() + 1e-8\n",
        "                    )\n",
        "\n",
        "                # Policy loss\n",
        "                pg_loss1 = -mb_advantages * ratio\n",
        "                pg_loss2 = -mb_advantages * torch.clamp(\n",
        "                    ratio, 1 - clip_coef, 1 + clip_coef\n",
        "                )\n",
        "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "                # Value loss\n",
        "                newvalue = newvalue.view(-1)\n",
        "                if clip_vloss:\n",
        "                    v_loss_unclipped = (newvalue - mb_returns) ** 2\n",
        "                    v_clipped = mb_values + torch.clamp(\n",
        "                        newvalue - mb_values,\n",
        "                        -clip_coef,\n",
        "                        clip_coef,\n",
        "                    )\n",
        "                    v_loss_clipped = (v_clipped - mb_returns) ** 2\n",
        "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
        "                    v_loss = 0.5 * v_loss_max.mean()\n",
        "                else:\n",
        "                    v_loss = 0.5 * ((newvalue - mb_returns) ** 2).mean()\n",
        "\n",
        "                # loss here\n",
        "                entropy_loss = entropy.mean()\n",
        "                loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
        "\n",
        "                if self.record_loss:\n",
        "                    with open(self.loss_file, \"a\") as f:\n",
        "                        print(f\"# mini batch ({epoch}, {mb}) -- pg_loss:{pg_loss.item():.4f}, value_loss:{v_loss.item():.4f}, \" + \\\n",
        "                              f\"entropy:{entropy_loss.item():.4f}, approx_kl: {approx_kl.item():.4f}\",\n",
        "                                file=f)\n",
        "                    with open(self.action_file, \"a\") as f:\n",
        "                        print(f\"# mini batch ({epoch}, {mb}) -- pg_loss:{pg_loss.item():.4f}, value_loss:{v_loss.item():.4f}, \" + \\\n",
        "                              f\"entropy:{entropy_loss.item():.4f}, approx_kl: {approx_kl.item():.4f}\",\n",
        "                                file=f)\n",
        "                        atn_list = mb_actions.cpu().numpy().tolist()\n",
        "                        for atns in atn_list:\n",
        "                            for atn in atns:\n",
        "                                print(f\"{atn}\", file=f)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.agent.parameters(), max_grad_norm)\n",
        "                self.optimizer.step()\n",
        "\n",
        "            # kl divergence ?\n",
        "            if target_kl is not None:\n",
        "                if approx_kl > target_kl:\n",
        "                    break\n",
        "\n",
        "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
        "        var_y = np.var(y_true)\n",
        "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
        "\n",
        "        # TIMING: performance metrics to evaluate cpu/gpu usage\n",
        "        train_time = time.time() - train_time\n",
        "        train_sps = int(self.batch_size / train_time)\n",
        "        self.update += 1\n",
        "\n",
        "        print(f\"\\tTrain={train_sps}\\n\")\n",
        "\n",
        "        allocated_torch = torch.cuda.memory_allocated(self.device) - allocated_torch\n",
        "        allocated_cpu = self.process.memory_info().rss - allocated_cpu\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                \"Allocated during training - Pytorch: %.2f GB, System: %.2f GB\"\n",
        "                % (allocated_torch / 1e9, allocated_cpu / 1e9)\n",
        "            )\n",
        "\n",
        "        if self.record_loss:\n",
        "            with open(self.loss_file, \"a\") as f:\n",
        "                print(f\"Epoch -- policy_loss:{pg_loss.item():.4f}, value_loss:{v_loss.item():.4f}, \",\n",
        "                      f\"entropy:{entropy_loss.item():.4f}, approx_kl:{approx_kl.item():.4f}\",\n",
        "                      f\"clipfrac:{np.mean(clipfracs):.4f}, explained_var:{explained_var:.4f}\",\n",
        "                      file=f)\n",
        "\n",
        "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
        "        if self.wandb_entity:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"performance/train_sps\": train_sps,\n",
        "                    \"performance/train_time\": train_time,\n",
        "                    \"charts/learning_rate\": self.optimizer.param_groups[0][\"lr\"],\n",
        "                    \"losses/value_loss\": v_loss.item(),\n",
        "                    \"losses/policy_loss\": pg_loss.item(),\n",
        "                    \"losses/entropy\": entropy_loss.item(),\n",
        "                    \"losses/old_approx_kl\": old_approx_kl.item(),\n",
        "                    \"losses/approx_kl\": approx_kl.item(),\n",
        "                    \"losses/clipfrac\": np.mean(clipfracs),\n",
        "                    \"losses/explained_variance\": explained_var,\n",
        "                    \"agent_steps\": self.global_step,\n",
        "                    \"global_step\": self.global_step,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        if self.update % self.checkpoint_interval == 1 or self.done_training():\n",
        "           self._save_checkpoint()\n",
        "\n",
        "    def done_training(self):\n",
        "        return self.update >= self.total_updates\n",
        "\n",
        "    def close(self):\n",
        "        for envs in self.buffers:\n",
        "            envs.close()\n",
        "\n",
        "        if self.wandb_entity:\n",
        "            wandb.finish()\n",
        "\n",
        "    def _save_checkpoint(self):\n",
        "        if self.data_dir is None:\n",
        "            return\n",
        "\n",
        "        policy_name = f\"{self.exp_name}.{self.update:06d}\"\n",
        "        state = {\n",
        "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
        "            \"global_step\": self.global_step,\n",
        "            \"agent_step\": self.agent_step,\n",
        "            \"update\": self.update,\n",
        "            \"learning_rate\": self.learning_rate,\n",
        "            \"policy_checkpoint_name\": policy_name,\n",
        "            \"wandb_run_id\": self.wandb_run_id,\n",
        "        }\n",
        "        path = os.path.join(self.data_dir, f\"trainer.pt\")\n",
        "        tmp_path = path + \".tmp\"\n",
        "        torch.save(state, tmp_path)\n",
        "        os.rename(tmp_path, path)\n",
        "\n",
        "        # NOTE: as the agent_creator has args internally, the policy args are not passed\n",
        "        self.policy_store.add_policy(policy_name, self.agent)\n",
        "\n",
        "        if self.policy_ranker:\n",
        "            self.policy_ranker.add_policy_copy(\n",
        "                policy_name, self.policy_pool._learner_name\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "muOg6MfamfIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Model 2: 2022 Transformer + LSTM for rl\n"
      ],
      "metadata": {
        "id": "pyZFAv7rIaqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/NeuralMMO/NeurIPS2022NMMO-Submission-Pool/blob/main/submission_pool/baseline-RL/neural_mmo/reward_parser.py"
      ],
      "metadata": {
        "id": "LPKcpqDcMZ1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### network"
      ],
      "metadata": {
        "id": "W6e15l6DA_No"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from core.mask import MaskedPolicy\n",
        "\n",
        "\n",
        "class ActionHead(nn.Module):\n",
        "    name2dim = {\"move\": 5, \"attack_target\": 16}\n",
        "\n",
        "    def __init__(self, input_dim: int):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleDict({\n",
        "            name: nn.Linear(input_dim, output_dim)\n",
        "            for name, output_dim in self.name2dim.items()\n",
        "        })\n",
        "\n",
        "    def forward(self, x) -> Dict[str, torch.Tensor]:\n",
        "        out = {name: self.heads[name](x) for name in self.name2dim}\n",
        "        return out\n",
        "\n",
        "\n",
        "class NMMONet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.local_map_cnn = nn.Sequential(\n",
        "            nn.Conv2d(24, 32, 3, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.local_map_fc = nn.Linear(32 * 4 * 4, 64)\n",
        "\n",
        "        self.self_entity_fc1 = nn.Linear(26, 32)\n",
        "        self.self_entity_fc2 = nn.Linear(32, 32)\n",
        "\n",
        "        self.other_entity_fc1 = nn.Linear(26, 32)\n",
        "        self.other_entity_fc2 = nn.Linear(15 * 32, 32)\n",
        "\n",
        "        self.fc = nn.Linear(64 + 32 + 32, 64)\n",
        "        self.action_head = ActionHead(64)\n",
        "        self.value_head = nn.Linear(64, 1)\n",
        "\n",
        "    def local_map_embedding(self, input_dict):\n",
        "        terrain = input_dict[\"terrain\"]\n",
        "        death_fog_damage = input_dict[\"death_fog_damage\"]\n",
        "        reachable = input_dict[\"reachable\"]\n",
        "        population = input_dict[\"entity_population\"]\n",
        "\n",
        "        T, B, *_ = terrain.shape\n",
        "\n",
        "        terrain = F.one_hot(terrain, num_classes=16).permute(0, 1, 4, 2, 3)\n",
        "        population = F.one_hot(population,\n",
        "                               num_classes=6).permute(0, 1, 4, 2, 3)\n",
        "        death_fog_damage = death_fog_damage.unsqueeze(dim=2)\n",
        "        reachable = reachable.unsqueeze(dim=2)\n",
        "        local_map = torch.cat(\n",
        "            [terrain, reachable, population, death_fog_damage], dim=2)\n",
        "\n",
        "        local_map = torch.flatten(local_map, 0, 1).to(torch.float32)\n",
        "        local_map_emb = self.local_map_cnn(local_map)\n",
        "        local_map_emb = local_map_emb.view(T * B, -1).view(T, B, -1)\n",
        "        local_map_emb = F.relu(self.local_map_fc(local_map_emb))\n",
        "\n",
        "        return local_map_emb\n",
        "\n",
        "    def entity_embedding(self, input_dict):\n",
        "        self_entity = input_dict[\"self_entity\"]\n",
        "        other_entity = input_dict[\"other_entity\"]\n",
        "\n",
        "        T, B, *_ = self_entity.shape\n",
        "\n",
        "        self_entity_emb = F.relu(self.self_entity_fc1(self_entity))\n",
        "        self_entity_emb = self_entity_emb.view(T, B, -1)\n",
        "        self_entity_emb = F.relu(self.self_entity_fc2(self_entity_emb))\n",
        "\n",
        "        other_entity_emb = F.relu(self.other_entity_fc1(other_entity))\n",
        "        other_entity_emb = other_entity_emb.view(T, B, -1)\n",
        "        other_entity_emb = F.relu(self.other_entity_fc2(other_entity_emb))\n",
        "\n",
        "        return self_entity_emb, other_entity_emb\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_dict: Dict,\n",
        "        training: bool = False,\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        T, B, *_ = input_dict[\"terrain\"].shape\n",
        "        local_map_emb = self.local_map_embedding(input_dict)\n",
        "        self_entity_emb, other_entity_emb = self.entity_embedding(input_dict)\n",
        "\n",
        "        x = torch.cat([local_map_emb, self_entity_emb, other_entity_emb],\n",
        "                      dim=-1)\n",
        "        x = F.relu(self.fc(x))\n",
        "\n",
        "        logits = self.action_head(x)\n",
        "        value = self.value_head(x).view(T, B)\n",
        "\n",
        "        output = {\"value\": value}\n",
        "        for key, val in logits.items():\n",
        "            if not training:\n",
        "                dist = MaskedPolicy(val, input_dict[f\"va_{key}\"])\n",
        "                action = dist.sample()\n",
        "                logprob = dist.log_prob(action)\n",
        "                output[key] = action\n",
        "                output[f\"{key}_logp\"] = logprob\n",
        "            else:\n",
        "                output[f\"{key}_logits\"] = val\n",
        "        return output"
      ],
      "metadata": {
        "id": "ph4_XJSLBAL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3: MAPPO"
      ],
      "metadata": {
        "id": "epU7THDVMar5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/marlbenchmark/on-policy/blob/main/onpolicy/algorithms/r_mappo/algorithm/r_actor_critic.py"
      ],
      "metadata": {
        "id": "6MNJLHNFzrvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility files"
      ],
      "metadata": {
        "id": "LHQ-H_8DDLQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/marlbenchmark/on-policy.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhQIf8cMEy3j",
        "outputId": "086fc7a7-6e97-458b-ddb4-612469d55531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'on-policy'...\n",
            "remote: Enumerating objects: 834, done.\u001b[K\n",
            "remote: Counting objects: 100% (414/414), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 834 (delta 354), reused 328 (delta 328), pack-reused 420\u001b[K\n",
            "Receiving objects: 100% (834/834), 294.35 KiB | 8.66 MiB/s, done.\n",
            "Resolving deltas: 100% (495/495), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# # Replace 'example_repo' with the actual name of your repository\n",
        "repository_name = 'on-policy'\n",
        "\n",
        "# # Navigate to the repository directory\n",
        "os.chdir(repository_name)\n",
        "\n",
        "# List the contents of the current directory to confirm that you are in the right place\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMA3yZISDMsz",
        "outputId": "db164839-1f67-4778-89db-fc347a2f6921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "environment.yaml  LICENSE  onpolicy  README.md\trequirements.txt  setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### policy files"
      ],
      "metadata": {
        "id": "U66rTLjnmiFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Actor Critic Policy\n"
      ],
      "metadata": {
        "id": "xtiElAnn_PE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from onpolicy.algorithms.utils.util import init, check\n",
        "from onpolicy.algorithms.utils.cnn import CNNBase\n",
        "from onpolicy.algorithms.utils.mlp import MLPBase\n",
        "from onpolicy.algorithms.utils.rnn import RNNLayer\n",
        "from onpolicy.algorithms.utils.act import ACTLayer\n",
        "from onpolicy.algorithms.utils.popart import PopArt\n",
        "from onpolicy.utils.util import get_shape_from_obs_space\n",
        "\n",
        "\n",
        "class R_Actor(nn.Module):\n",
        "    \"\"\"\n",
        "    Actor network class for MAPPO. Outputs actions given observations.\n",
        "    :param args: (argparse.Namespace) arguments containing relevant model information.\n",
        "    :param obs_space: (gym.Space) observation space.\n",
        "    :param action_space: (gym.Space) action space.\n",
        "    :param device: (torch.device) specifies the device to run on (cpu/gpu).\n",
        "    \"\"\"\n",
        "    def __init__(self, args, obs_space, action_space, device=torch.device(\"cuda\")):\n",
        "        super(R_Actor, self).__init__()\n",
        "        self.hidden_size = args.hidden_size\n",
        "\n",
        "        self._gain = args.gain\n",
        "        self._use_orthogonal = args.use_orthogonal\n",
        "        self._use_policy_active_masks = args.use_policy_active_masks\n",
        "        self._use_naive_recurrent_policy = args.use_naive_recurrent_policy\n",
        "        self._use_recurrent_policy = args.use_recurrent_policy\n",
        "        self._recurrent_N = args.recurrent_N\n",
        "        self.tpdv = dict(dtype=torch.float32, device=device)\n",
        "\n",
        "        obs_shape = get_shape_from_obs_space(obs_space)\n",
        "        base = CNNBase if len(obs_shape) == 3 else MLPBase\n",
        "        self.base = base(args, obs_shape)\n",
        "\n",
        "        if self._use_naive_recurrent_policy or self._use_recurrent_policy:\n",
        "            self.rnn = RNNLayer(self.hidden_size, self.hidden_size, self._recurrent_N, self._use_orthogonal)\n",
        "\n",
        "        self.act = ACTLayer(action_space, self.hidden_size, self._use_orthogonal, self._gain, args)\n",
        "\n",
        "        self.to(device)\n",
        "        self.algo = args.algorithm_name\n",
        "\n",
        "    def forward(self, obs, rnn_states, masks, available_actions=None, deterministic=False):\n",
        "        \"\"\"\n",
        "        Compute actions from the given inputs.\n",
        "        :param obs: (np.ndarray / torch.Tensor) observation inputs into network.\n",
        "        :param rnn_states: (np.ndarray / torch.Tensor) if RNN network, hidden states for RNN.\n",
        "        :param masks: (np.ndarray / torch.Tensor) mask tensor denoting if hidden states should be reinitialized to zeros.\n",
        "        :param available_actions: (np.ndarray / torch.Tensor) denotes which actions are available to agent\n",
        "                                                              (if None, all actions available)\n",
        "        :param deterministic: (bool) whether to sample from action distribution or return the mode.\n",
        "\n",
        "        :return actions: (torch.Tensor) actions to take.\n",
        "        :return action_log_probs: (torch.Tensor) log probabilities of taken actions.\n",
        "        :return rnn_states: (torch.Tensor) updated RNN hidden states.\n",
        "        \"\"\"\n",
        "        obs = check(obs).to(**self.tpdv)\n",
        "        rnn_states = check(rnn_states).to(**self.tpdv)\n",
        "        masks = check(masks).to(**self.tpdv)\n",
        "        if available_actions is not None:\n",
        "            available_actions = check(available_actions).to(**self.tpdv)\n",
        "\n",
        "        actor_features = self.base(obs)\n",
        "\n",
        "        if self._use_naive_recurrent_policy or self._use_recurrent_policy:\n",
        "            actor_features, rnn_states = self.rnn(actor_features, rnn_states, masks)\n",
        "\n",
        "        actions, action_log_probs = self.act(actor_features, available_actions, deterministic)\n",
        "\n",
        "        return actions, action_log_probs, rnn_states\n",
        "\n",
        "    # used to get the final action ? idk\n",
        "    def evaluate_actions(self, obs, rnn_states, action, masks, available_actions=None, active_masks=None):\n",
        "        \"\"\"\n",
        "        Compute log probability and entropy of given actions.\n",
        "        :param obs: (torch.Tensor) observation inputs into network.\n",
        "        :param action: (torch.Tensor) actions whose entropy and log probability to evaluate.\n",
        "        :param rnn_states: (torch.Tensor) if RNN network, hidden states for RNN.\n",
        "        :param masks: (torch.Tensor) mask tensor denoting if hidden states should be reinitialized to zeros.\n",
        "        :param available_actions: (torch.Tensor) denotes which actions are available to agent\n",
        "                                                              (if None, all actions available)\n",
        "        :param active_masks: (torch.Tensor) denotes whether an agent is active or dead.\n",
        "\n",
        "        :return action_log_probs: (torch.Tensor) log probabilities of the input actions.\n",
        "        :return dist_entropy: (torch.Tensor) action distribution entropy for the given inputs.\n",
        "        \"\"\"\n",
        "        obs = check(obs).to(**self.tpdv)\n",
        "        rnn_states = check(rnn_states).to(**self.tpdv)\n",
        "        action = check(action).to(**self.tpdv)\n",
        "        masks = check(masks).to(**self.tpdv)\n",
        "        if available_actions is not None:\n",
        "            available_actions = check(available_actions).to(**self.tpdv)\n",
        "\n",
        "        if active_masks is not None:\n",
        "            active_masks = check(active_masks).to(**self.tpdv)\n",
        "\n",
        "        actor_features = self.base(obs)\n",
        "\n",
        "        if self._use_naive_recurrent_policy or self._use_recurrent_policy:\n",
        "            actor_features, rnn_states = self.rnn(actor_features, rnn_states, masks)\n",
        "\n",
        "        # if self.algo == \"hatrpo\":\n",
        "        #     action_log_probs, dist_entropy ,action_mu, action_std, all_probs= self.act.evaluate_actions_trpo(actor_features,\n",
        "        #                                                             action, available_actions,\n",
        "        #                                                             active_masks=\n",
        "        #                                                             active_masks if self._use_policy_active_masks\n",
        "        #                                                             else None)\n",
        "\n",
        "        #     return action_log_probs, dist_entropy, action_mu, action_std, all_probs\n",
        "        # else:\n",
        "            action_log_probs, dist_entropy = self.act.evaluate_actions(actor_features,\n",
        "                                                                    action, available_actions,\n",
        "                                                                    active_masks=\n",
        "                                                                    active_masks if self._use_policy_active_masks\n",
        "                                                                    else None)\n",
        "\n",
        "        return action_log_probs, dist_entropy\n",
        "\n",
        "\n",
        "class R_Critic(nn.Module):\n",
        "    \"\"\"\n",
        "    Critic network class for MAPPO. Outputs value function predictions given centralized input (MAPPO) or\n",
        "                            local observations (IPPO).\n",
        "    :param args: (argparse.Namespace) arguments containing relevant model information.\n",
        "    :param cent_obs_space: (gym.Space) (centralized) observation space.\n",
        "    :param device: (torch.device) specifies the device to run on (cpu/gpu).\n",
        "    \"\"\"\n",
        "    def __init__(self, args, cent_obs_space, device=torch.device(\"cpu\")):\n",
        "        super(R_Critic, self).__init__()\n",
        "        self.hidden_size = args.hidden_size\n",
        "        self._use_orthogonal = args.use_orthogonal\n",
        "        self._use_naive_recurrent_policy = args.use_naive_recurrent_policy\n",
        "        self._use_recurrent_policy = args.use_recurrent_policy\n",
        "        self._recurrent_N = args.recurrent_N\n",
        "        self._use_popart = args.use_popart\n",
        "        self.tpdv = dict(dtype=torch.float32, device=device)\n",
        "        init_method = [nn.init.xavier_uniform_, nn.init.orthogonal_][self._use_orthogonal]\n",
        "\n",
        "        cent_obs_shape = get_shape_from_obs_space(cent_obs_space)\n",
        "        base = CNNBase if len(cent_obs_shape) == 3 else MLPBase\n",
        "        self.base = base(args, cent_obs_shape)\n",
        "\n",
        "        if self._use_naive_recurrent_policy or self._use_recurrent_policy:\n",
        "            self.rnn = RNNLayer(self.hidden_size, self.hidden_size, self._recurrent_N, self._use_orthogonal)\n",
        "\n",
        "        def init_(m):\n",
        "            return init(m, init_method, lambda x: nn.init.constant_(x, 0))\n",
        "\n",
        "        if self._use_popart:\n",
        "            self.v_out = init_(PopArt(self.hidden_size, 1, device=device))\n",
        "        else:\n",
        "            self.v_out = init_(nn.Linear(self.hidden_size, 1))\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, cent_obs, rnn_states, masks):\n",
        "        \"\"\"\n",
        "        Compute actions from the given inputs.\n",
        "        :param cent_obs: (np.ndarray / torch.Tensor) observation inputs into network.\n",
        "        :param rnn_states: (np.ndarray / torch.Tensor) if RNN network, hidden states for RNN.\n",
        "        :param masks: (np.ndarray / torch.Tensor) mask tensor denoting if RNN states should be reinitialized to zeros.\n",
        "\n",
        "        :return values: (torch.Tensor) value function predictions.\n",
        "        :return rnn_states: (torch.Tensor) updated RNN hidden states.\n",
        "        \"\"\"\n",
        "        cent_obs = check(cent_obs).to(**self.tpdv)\n",
        "        rnn_states = check(rnn_states).to(**self.tpdv)\n",
        "        masks = check(masks).to(**self.tpdv)\n",
        "\n",
        "        critic_features = self.base(cent_obs)\n",
        "        if self._use_naive_recurrent_policy or self._use_recurrent_policy:\n",
        "            critic_features, rnn_states = self.rnn(critic_features, rnn_states, masks)\n",
        "        values = self.v_out(critic_features)\n",
        "\n",
        "        return values, rnn_states"
      ],
      "metadata": {
        "id": "k7uCa6ur6M5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### mappo main polilcy"
      ],
      "metadata": {
        "id": "d-uPCvznFiAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# from onpolicy.algorithms.r_mappo.algorithm.r_actor_critic import R_Actor, R_Critic #TODO: ADD THIS BAKC LATER\n",
        "from onpolicy.utils.util import update_linear_schedule\n",
        "\n",
        "\n",
        "class R_MAPPOPolicy:\n",
        "    \"\"\"\n",
        "    MAPPO Policy  class. Wraps actor and critic networks to compute actions and value function predictions.\n",
        "\n",
        "    :param args: (argparse.Namespace) arguments containing relevant model and policy information.\n",
        "    :param obs_space: (gym.Space) observation space.\n",
        "    :param cent_obs_space: (gym.Space) value function input space (centralized input for MAPPO, decentralized for IPPO).\n",
        "    :param action_space: (gym.Space) action space.\n",
        "    :param device: (torch.device) specifies the device to run on (cpu/gpu).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args, obs_space, cent_obs_space, act_space, device=torch.device(\"cuda\")):\n",
        "        self.device = device\n",
        "        self.lr = args.lr\n",
        "        self.critic_lr = args.critic_lr\n",
        "        self.opti_eps = args.opti_eps\n",
        "        self.weight_decay = args.weight_decay\n",
        "\n",
        "        self.obs_space = obs_space\n",
        "        self.share_obs_space = cent_obs_space\n",
        "        self.act_space = act_space\n",
        "\n",
        "\n",
        "\n",
        "        self.actor = R_Actor(args, self.obs_space, self.act_space, self.device)\n",
        "        self.critic = R_Critic(args, self.share_obs_space, self.device)\n",
        "\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
        "                                                lr=self.lr, eps=self.opti_eps,\n",
        "                                                weight_decay=self.weight_decay)\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n",
        "                                                 lr=self.critic_lr,\n",
        "                                                 eps=self.opti_eps,\n",
        "                                                 weight_decay=self.weight_decay)\n",
        "\n",
        "    def lr_decay(self, episode, episodes):\n",
        "        \"\"\"\n",
        "        Decay the actor and critic learning rates.\n",
        "        :param episode: (int) current training episode.\n",
        "        :param episodes: (int) total number of training episodes.\n",
        "        \"\"\"\n",
        "        update_linear_schedule(self.actor_optimizer, episode, episodes, self.lr)\n",
        "        update_linear_schedule(self.critic_optimizer, episode, episodes, self.critic_lr)\n",
        "\n",
        "    def get_actions(self, cent_obs, obs, rnn_states_actor, rnn_states_critic, masks, available_actions=None,\n",
        "                    deterministic=False):\n",
        "        \"\"\"\n",
        "        Compute actions and value function predictions for the given inputs.\n",
        "        :param cent_obs (np.ndarray): centralized input to the critic.\n",
        "        :param obs (np.ndarray): local agent inputs to the actor.\n",
        "        :param rnn_states_actor: (np.ndarray) if actor is RNN, RNN states for actor.\n",
        "        :param rnn_states_critic: (np.ndarray) if critic is RNN, RNN states for critic.\n",
        "        :param masks: (np.ndarray) denotes points at which RNN states should be reset.\n",
        "        :param available_actions: (np.ndarray) denotes which actions are available to agent\n",
        "                                  (if None, all actions available)\n",
        "        :param deterministic: (bool) whether the action should be mode of distribution or should be sampled.\n",
        "\n",
        "        :return values: (torch.Tensor) value function predictions.\n",
        "        :return actions: (torch.Tensor) actions to take.\n",
        "        :return action_log_probs: (torch.Tensor) log probabilities of chosen actions.\n",
        "        :return rnn_states_actor: (torch.Tensor) updated actor network RNN states.\n",
        "        :return rnn_states_critic: (torch.Tensor) updated critic network RNN states.\n",
        "        \"\"\"\n",
        "        actions, action_log_probs, rnn_states_actor = self.actor(obs,\n",
        "                                                                 rnn_states_actor,\n",
        "                                                                 masks,\n",
        "                                                                 available_actions,\n",
        "                                                                 deterministic)\n",
        "\n",
        "        values, rnn_states_critic = self.critic(cent_obs, rnn_states_critic, masks)\n",
        "        return values, actions, action_log_probs, rnn_states_actor, rnn_states_critic\n",
        "\n",
        "    def get_values(self, cent_obs, rnn_states_critic, masks):\n",
        "        \"\"\"\n",
        "        Get value function predictions.\n",
        "        :param cent_obs (np.ndarray): centralized input to the critic.\n",
        "        :param rnn_states_critic: (np.ndarray) if critic is RNN, RNN states for critic.\n",
        "        :param masks: (np.ndarray) denotes points at which RNN states should be reset.\n",
        "\n",
        "        :return values: (torch.Tensor) value function predictions.\n",
        "        \"\"\"\n",
        "        values, _ = self.critic(cent_obs, rnn_states_critic, masks)\n",
        "        return values\n",
        "\n",
        "    def evaluate_actions(self, cent_obs, obs, rnn_states_actor, rnn_states_critic, action, masks,\n",
        "                         available_actions=None, active_masks=None):\n",
        "        \"\"\"\n",
        "        Get action logprobs / entropy and value function predictions for actor update.\n",
        "        :param cent_obs (np.ndarray): centralized input to the critic.\n",
        "        :param obs (np.ndarray): local agent inputs to the actor.\n",
        "        :param rnn_states_actor: (np.ndarray) if actor is RNN, RNN states for actor.\n",
        "        :param rnn_states_critic: (np.ndarray) if critic is RNN, RNN states for critic.\n",
        "        :param action: (np.ndarray) actions whose log probabilites and entropy to compute.\n",
        "        :param masks: (np.ndarray) denotes points at which RNN states should be reset.\n",
        "        :param available_actions: (np.ndarray) denotes which actions are available to agent\n",
        "                                  (if None, all actions available)\n",
        "        :param active_masks: (torch.Tensor) denotes whether an agent is active or dead.\n",
        "\n",
        "        :return values: (torch.Tensor) value function predictions.\n",
        "        :return action_log_probs: (torch.Tensor) log probabilities of the input actions.\n",
        "        :return dist_entropy: (torch.Tensor) action distribution entropy for the given inputs.\n",
        "        \"\"\"\n",
        "        action_log_probs, dist_entropy = self.actor.evaluate_actions(obs,\n",
        "                                                                     rnn_states_actor,\n",
        "                                                                     action,\n",
        "                                                                     masks,\n",
        "                                                                     available_actions,\n",
        "                                                                     active_masks)\n",
        "\n",
        "        values, _ = self.critic(cent_obs, rnn_states_critic, masks)\n",
        "        return values, action_log_probs, dist_entropy\n",
        "\n",
        "    def act(self, obs, rnn_states_actor, masks, available_actions=None, deterministic=False):\n",
        "        \"\"\"\n",
        "        Compute actions using the given inputs.\n",
        "        :param obs (np.ndarray): local agent inputs to the actor.\n",
        "        :param rnn_states_actor: (np.ndarray) if actor is RNN, RNN states for actor.\n",
        "        :param masks: (np.ndarray) denotes points at which RNN states should be reset.\n",
        "        :param available_actions: (np.ndarray) denotes which actions are available to agent\n",
        "                                  (if None, all actions available)\n",
        "        :param deterministic: (bool) whether the action should be mode of distribution or should be sampled.\n",
        "        \"\"\"\n",
        "        actions, _, rnn_states_actor = self.actor(obs, rnn_states_actor, masks, available_actions, deterministic)\n",
        "        return actions, rnn_states_actor"
      ],
      "metadata": {
        "id": "1dyW2gdP6GwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WhbJToUf_eZn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eFxwl8_N6NAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### main model class"
      ],
      "metadata": {
        "id": "dr-9QYRw-HYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from onpolicy.utils.util import get_gard_norm, huber_loss, mse_loss\n",
        "from onpolicy.utils.valuenorm import ValueNorm\n",
        "from onpolicy.algorithms.utils.util import check\n",
        "\n",
        "class R_MAPPO():\n",
        "    \"\"\"\n",
        "    Trainer class for MAPPO to update policies.\n",
        "    :param args: (argparse.Namespace) arguments containing relevant model, policy, and env information.\n",
        "    :param policy: (R_MAPPO_Policy) policy to update.\n",
        "    :param device: (torch.device) specifies the device to run on (cpu/gpu).\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 args,\n",
        "                 policy,\n",
        "                 device=torch.device(\"cuda\")):\n",
        "\n",
        "        self.device = device\n",
        "        self.tpdv = dict(dtype=torch.float32, device=device)\n",
        "        self.policy = policy\n",
        "\n",
        "        self.clip_param = args.clip_param\n",
        "        self.ppo_epoch = args.ppo_epoch\n",
        "        self.num_mini_batch = args.num_mini_batch\n",
        "        self.data_chunk_length = args.data_chunk_length\n",
        "        self.value_loss_coef = args.value_loss_coef\n",
        "        self.entropy_coef = args.entropy_coef\n",
        "        self.max_grad_norm = args.max_grad_norm\n",
        "        self.huber_delta = args.huber_delta\n",
        "\n",
        "        self._use_recurrent_policy = args.use_recurrent_policy\n",
        "        self._use_naive_recurrent = args.use_naive_recurrent_policy\n",
        "        self._use_max_grad_norm = args.use_max_grad_norm\n",
        "        self._use_clipped_value_loss = args.use_clipped_value_loss\n",
        "        self._use_huber_loss = args.use_huber_loss\n",
        "        self._use_popart = args.use_popart\n",
        "        self._use_valuenorm = args.use_valuenorm\n",
        "        self._use_value_active_masks = args.use_value_active_masks\n",
        "        self._use_policy_active_masks = args.use_policy_active_masks\n",
        "\n",
        "        assert (self._use_popart and self._use_valuenorm) == False, (\"self._use_popart and self._use_valuenorm can not be set True simultaneously\")\n",
        "\n",
        "        if self._use_popart:\n",
        "            self.value_normalizer = self.policy.critic.v_out\n",
        "        elif self._use_valuenorm:\n",
        "            self.value_normalizer = ValueNorm(1).to(self.device)\n",
        "        else:\n",
        "            self.value_normalizer = None\n",
        "\n",
        "    def cal_value_loss(self, values, value_preds_batch, return_batch, active_masks_batch):\n",
        "        \"\"\"\n",
        "        Calculate value function loss.\n",
        "        :param values: (torch.Tensor) value function predictions.\n",
        "        :param value_preds_batch: (torch.Tensor) \"old\" value  predictions from data batch (used for value clip loss)\n",
        "        :param return_batch: (torch.Tensor) reward to go returns.\n",
        "        :param active_masks_batch: (torch.Tensor) denotes if agent is active or dead at a given timesep.\n",
        "\n",
        "        :return value_loss: (torch.Tensor) value function loss.\n",
        "        \"\"\"\n",
        "        value_pred_clipped = value_preds_batch + (values - value_preds_batch).clamp(-self.clip_param,\n",
        "                                                                                        self.clip_param)\n",
        "        if self._use_popart or self._use_valuenorm:\n",
        "            self.value_normalizer.update(return_batch)\n",
        "            error_clipped = self.value_normalizer.normalize(return_batch) - value_pred_clipped\n",
        "            error_original = self.value_normalizer.normalize(return_batch) - values\n",
        "        else:\n",
        "            error_clipped = return_batch - value_pred_clipped\n",
        "            error_original = return_batch - values\n",
        "\n",
        "        if self._use_huber_loss:\n",
        "            value_loss_clipped = huber_loss(error_clipped, self.huber_delta)\n",
        "            value_loss_original = huber_loss(error_original, self.huber_delta)\n",
        "        else:\n",
        "            value_loss_clipped = mse_loss(error_clipped)\n",
        "            value_loss_original = mse_loss(error_original)\n",
        "\n",
        "        if self._use_clipped_value_loss:\n",
        "            value_loss = torch.max(value_loss_original, value_loss_clipped)\n",
        "        else:\n",
        "            value_loss = value_loss_original\n",
        "\n",
        "        if self._use_value_active_masks:\n",
        "            value_loss = (value_loss * active_masks_batch).sum() / active_masks_batch.sum()\n",
        "        else:\n",
        "            value_loss = value_loss.mean()\n",
        "\n",
        "        return value_loss\n",
        "\n",
        "    def ppo_update(self, sample, update_actor=True):\n",
        "        \"\"\"\n",
        "        Update actor and critic networks.\n",
        "        :param sample: (Tuple) contains data batch with which to update networks.\n",
        "        :update_actor: (bool) whether to update actor network.\n",
        "\n",
        "        :return value_loss: (torch.Tensor) value function loss.\n",
        "        :return critic_grad_norm: (torch.Tensor) gradient norm from critic up9date.\n",
        "        ;return policy_loss: (torch.Tensor) actor(policy) loss value.\n",
        "        :return dist_entropy: (torch.Tensor) action entropies.\n",
        "        :return actor_grad_norm: (torch.Tensor) gradient norm from actor update.\n",
        "        :return imp_weights: (torch.Tensor) importance sampling weights.\n",
        "        \"\"\"\n",
        "        share_obs_batch, obs_batch, rnn_states_batch, rnn_states_critic_batch, actions_batch, \\\n",
        "        value_preds_batch, return_batch, masks_batch, active_masks_batch, old_action_log_probs_batch, \\\n",
        "        adv_targ, available_actions_batch = sample\n",
        "\n",
        "        old_action_log_probs_batch = check(old_action_log_probs_batch).to(**self.tpdv)\n",
        "        adv_targ = check(adv_targ).to(**self.tpdv)\n",
        "        value_preds_batch = check(value_preds_batch).to(**self.tpdv)\n",
        "        return_batch = check(return_batch).to(**self.tpdv)\n",
        "        active_masks_batch = check(active_masks_batch).to(**self.tpdv)\n",
        "\n",
        "        # Reshape to do in a single forward pass for all steps\n",
        "        values, action_log_probs, dist_entropy = self.policy.evaluate_actions(share_obs_batch,\n",
        "                                                                              obs_batch,\n",
        "                                                                              rnn_states_batch,\n",
        "                                                                              rnn_states_critic_batch,\n",
        "                                                                              actions_batch,\n",
        "                                                                              masks_batch,\n",
        "                                                                              available_actions_batch,\n",
        "                                                                              active_masks_batch)\n",
        "        # actor update\n",
        "        imp_weights = torch.exp(action_log_probs - old_action_log_probs_batch)\n",
        "\n",
        "        surr1 = imp_weights * adv_targ\n",
        "        surr2 = torch.clamp(imp_weights, 1.0 - self.clip_param, 1.0 + self.clip_param) * adv_targ\n",
        "\n",
        "        if self._use_policy_active_masks:\n",
        "            policy_action_loss = (-torch.sum(torch.min(surr1, surr2),\n",
        "                                             dim=-1,\n",
        "                                             keepdim=True) * active_masks_batch).sum() / active_masks_batch.sum()\n",
        "        else:\n",
        "            policy_action_loss = -torch.sum(torch.min(surr1, surr2), dim=-1, keepdim=True).mean()\n",
        "\n",
        "        policy_loss = policy_action_loss\n",
        "\n",
        "        self.policy.actor_optimizer.zero_grad()\n",
        "\n",
        "        if update_actor:\n",
        "            (policy_loss - dist_entropy * self.entropy_coef).backward()\n",
        "\n",
        "        if self._use_max_grad_norm:\n",
        "            actor_grad_norm = nn.utils.clip_grad_norm_(self.policy.actor.parameters(), self.max_grad_norm)\n",
        "        else:\n",
        "            actor_grad_norm = get_gard_norm(self.policy.actor.parameters())\n",
        "\n",
        "        self.policy.actor_optimizer.step()\n",
        "\n",
        "        # critic update\n",
        "        value_loss = self.cal_value_loss(values, value_preds_batch, return_batch, active_masks_batch)\n",
        "\n",
        "        self.policy.critic_optimizer.zero_grad()\n",
        "\n",
        "        (value_loss * self.value_loss_coef).backward()\n",
        "\n",
        "        if self._use_max_grad_norm:\n",
        "            critic_grad_norm = nn.utils.clip_grad_norm_(self.policy.critic.parameters(), self.max_grad_norm)\n",
        "        else:\n",
        "            critic_grad_norm = get_gard_norm(self.policy.critic.parameters())\n",
        "\n",
        "        self.policy.critic_optimizer.step()\n",
        "\n",
        "        return value_loss, critic_grad_norm, policy_loss, dist_entropy, actor_grad_norm, imp_weights\n",
        "\n",
        "    def train(self, buffer, update_actor=True):\n",
        "        \"\"\"\n",
        "        Perform a training update using minibatch GD.\n",
        "        :param buffer: (SharedReplayBuffer) buffer containing training data.\n",
        "        :param update_actor: (bool) whether to update actor network.\n",
        "\n",
        "        :return train_info: (dict) contains information regarding training update (e.g. loss, grad norms, etc).\n",
        "        \"\"\"\n",
        "        if self._use_popart or self._use_valuenorm:\n",
        "            advantages = buffer.returns[:-1] - self.value_normalizer.denormalize(buffer.value_preds[:-1])\n",
        "        else:\n",
        "            advantages = buffer.returns[:-1] - buffer.value_preds[:-1]\n",
        "        advantages_copy = advantages.copy()\n",
        "        advantages_copy[buffer.active_masks[:-1] == 0.0] = np.nan\n",
        "        mean_advantages = np.nanmean(advantages_copy)\n",
        "        std_advantages = np.nanstd(advantages_copy)\n",
        "        advantages = (advantages - mean_advantages) / (std_advantages + 1e-5)\n",
        "\n",
        "\n",
        "        train_info = {}\n",
        "\n",
        "        train_info['value_loss'] = 0\n",
        "        train_info['policy_loss'] = 0\n",
        "        train_info['dist_entropy'] = 0\n",
        "        train_info['actor_grad_norm'] = 0\n",
        "        train_info['critic_grad_norm'] = 0\n",
        "        train_info['ratio'] = 0\n",
        "\n",
        "        for _ in range(self.ppo_epoch):\n",
        "            if self._use_recurrent_policy:\n",
        "                data_generator = buffer.recurrent_generator(advantages, self.num_mini_batch, self.data_chunk_length)\n",
        "            elif self._use_naive_recurrent:\n",
        "                data_generator = buffer.naive_recurrent_generator(advantages, self.num_mini_batch)\n",
        "            else:\n",
        "                data_generator = buffer.feed_forward_generator(advantages, self.num_mini_batch)\n",
        "\n",
        "            for sample in data_generator:\n",
        "\n",
        "                value_loss, critic_grad_norm, policy_loss, dist_entropy, actor_grad_norm, imp_weights \\\n",
        "                    = self.ppo_update(sample, update_actor)\n",
        "\n",
        "                train_info['value_loss'] += value_loss.item()\n",
        "                train_info['policy_loss'] += policy_loss.item()\n",
        "                train_info['dist_entropy'] += dist_entropy.item()\n",
        "                train_info['actor_grad_norm'] += actor_grad_norm\n",
        "                train_info['critic_grad_norm'] += critic_grad_norm\n",
        "                train_info['ratio'] += imp_weights.mean()\n",
        "\n",
        "        num_updates = self.ppo_epoch * self.num_mini_batch\n",
        "\n",
        "        for k in train_info.keys():\n",
        "            train_info[k] /= num_updates\n",
        "\n",
        "        return train_info\n",
        "\n",
        "    def prep_training(self):\n",
        "        self.policy.actor.train()\n",
        "        self.policy.critic.train()\n",
        "\n",
        "    def prep_rollout(self):\n",
        "        self.policy.actor.eval()\n",
        "        self.policy.critic.eval()\n",
        "\n",
        "\n",
        "# IN PROGRSESS\n",
        "    def evaluate(self, show_progress=False):\n",
        "      # Pick new policies for the policy pool\n",
        "      # TODO: find a way to not switch mid-stream\n",
        "      self.policy_pool.update_policies({\n",
        "          p.name: p.policy(\n",
        "              policy_args=[self.buffers[0]],\n",
        "              device=self.device,\n",
        "          ) for p in self.policy_store.select_policies(self.policy_selector)\n",
        "      })\n",
        "\n",
        "      allocated_torch = torch.cuda.memory_allocated(self.device)\n",
        "      allocated_cpu = self.process.memory_info().rss\n",
        "      ptr = env_step_time = inference_time = agent_steps_collected = 0\n",
        "      padded_steps_collected = 0\n",
        "\n",
        "      step = 0\n",
        "      infos = defaultdict(lambda: defaultdict(list))\n",
        "      stats = defaultdict(lambda: defaultdict(list))\n",
        "      performance = defaultdict(list)\n",
        "      progress_bar = tqdm(total=self.batch_size, disable=not show_progress)\n",
        "\n",
        "      data = self.data\n",
        "      while True:\n",
        "          buf = data.buf\n",
        "\n",
        "          step += 1\n",
        "          if ptr == self.batch_size + 1:\n",
        "              break\n",
        "\n",
        "          start = time.time()\n",
        "          o, r, d, i = self.buffers[buf].recv()\n",
        "          env_step_time += time.time() - start\n",
        "\n",
        "          i = self.policy_pool.update_scores(i, \"return\")\n",
        "\n",
        "          '''\n",
        "          for profile in self.buffers[buf].profile():\n",
        "              for k, v in profile.items():\n",
        "                  performance[k].append(v[\"delta\"])\n",
        "          '''\n",
        "\n",
        "          o = torch.Tensor(o)\n",
        "          if not self.cpu_offload:\n",
        "              o = o.to(self.device)\n",
        "\n",
        "          r = torch.Tensor(r).float().to(self.device).view(-1)\n",
        "\n",
        "          if len(d) != 0 and len(data.next_done[buf]) != 0:\n",
        "              alive_mask = (data.next_done[buf].cpu() + torch.Tensor(d)) != 2\n",
        "              data.next_done[buf] = torch.Tensor(d).to(self.device)\n",
        "          else:\n",
        "              alive_mask = [1 for _ in range(len(o))]\n",
        "\n",
        "          agent_steps_collected += sum(alive_mask)\n",
        "          padded_steps_collected += len(alive_mask)\n",
        "\n",
        "          # ALGO LOGIC: action logic\n",
        "          start = time.time()\n",
        "          with torch.no_grad():\n",
        "              (\n",
        "                  actions,\n",
        "                  logprob,\n",
        "                  value,\n",
        "                  data.next_lstm_state[buf],\n",
        "              ) = self.policy_pool.forwards(\n",
        "                  o.to(self.device),\n",
        "                  data.next_lstm_state[buf],\n",
        "                  data.next_done[buf],\n",
        "              )\n",
        "              value = value.flatten()\n",
        "          inference_time += time.time() - start\n",
        "\n",
        "          # TRY NOT TO MODIFY: execute the game\n",
        "          start = time.time()\n",
        "          self.buffers[buf].send(actions.cpu().numpy(), None)\n",
        "          env_step_time += time.time() - start\n",
        "          data.buf = (data.buf + 1) % self.num_buffers\n",
        "\n",
        "          # Index alive mask with policy pool idxs...\n",
        "          # TODO: Find a way to avoid having to do this\n",
        "          if self.selfplay_learner_weight > 0:\n",
        "            alive_mask = np.array(alive_mask) * self.policy_pool.learner_mask\n",
        "\n",
        "          for idx in np.where(alive_mask)[0]:\n",
        "              if ptr == self.batch_size + 1:\n",
        "                  break\n",
        "\n",
        "              data.obs[ptr] = o[idx]\n",
        "              data.values[ptr] = value[idx]\n",
        "              data.actions[ptr] = actions[idx]\n",
        "              data.logprobs[ptr] = logprob[idx]\n",
        "              data.sort_keys.append((buf, idx, step))\n",
        "\n",
        "              if len(d) != 0:\n",
        "                  data.rewards[ptr] = r[idx]\n",
        "                  data.dones[ptr] = d[idx]\n",
        "\n",
        "              ptr += 1\n",
        "              progress_bar.update(1)\n",
        "\n",
        "          '''\n",
        "          for ii in i:\n",
        "              if not ii:\n",
        "                  continue\n",
        "\n",
        "              for agent_i, values in ii.items():\n",
        "                  for name, stat in unroll_nested_dict(values):\n",
        "                      infos[name].append(stat)\n",
        "                      try:\n",
        "                          stat = float(stat)\n",
        "                          stats[name].append(stat)\n",
        "                      except:\n",
        "                          continue\n",
        "          '''\n",
        "\n",
        "          for policy_name, policy_i in i.items():\n",
        "              for agent_i in policy_i:\n",
        "                  if not agent_i:\n",
        "                      continue\n",
        "\n",
        "                  for name, stat in unroll_nested_dict(agent_i):\n",
        "                      infos[policy_name][name].append(stat)\n",
        "                      if 'Task_eval_fn' in name:\n",
        "                          # Temporary hack for NMMO competition\n",
        "                          continue\n",
        "                      try:\n",
        "                          stat = float(stat)\n",
        "                          stats[policy_name][name].append(stat)\n",
        "                      except:\n",
        "                          continue\n",
        "\n",
        "      if self.policy_pool.scores and self.policy_ranker is not None:\n",
        "        self.policy_ranker.update_ranks(\n",
        "            self.policy_pool.scores,\n",
        "            wandb_policies=[self.policy_pool._learner_name]\n",
        "            if self.wandb_entity\n",
        "            else [],\n",
        "            step=self.global_step,\n",
        "        )\n",
        "        self.policy_pool.scores = {}\n",
        "\n",
        "      env_sps = int(agent_steps_collected / env_step_time)\n",
        "      inference_sps = int(padded_steps_collected / inference_time)\n",
        "\n",
        "      progress_bar.set_description(\n",
        "          \"Eval: \"\n",
        "          + \", \".join(\n",
        "              [\n",
        "                  f\"Env SPS: {env_sps}\",\n",
        "                  f\"Inference SPS: {inference_sps}\",\n",
        "                  f\"Agent Steps: {agent_steps_collected}\",\n",
        "                  *[f\"{k}: {np.mean(v):.2f}\" for k, v in stats['learner'].items()],\n",
        "              ]\n",
        "          )\n",
        "      )\n",
        "\n",
        "      self.global_step += self.batch_size\n",
        "\n",
        "      if self.wandb_entity:\n",
        "          wandb.log(\n",
        "              {\n",
        "                  \"performance/env_time\": env_step_time,\n",
        "                  \"performance/env_sps\": env_sps,\n",
        "                  \"performance/inference_time\": inference_time,\n",
        "                  \"performance/inference_sps\": inference_sps,\n",
        "                  **{\n",
        "                      f\"performance/env/{k}\": np.mean(v)\n",
        "                      for k, v in performance.items()\n",
        "                  },\n",
        "                  **{f\"charts/{k}\": np.mean(v) for k, v in stats['learner'].items()},\n",
        "                  \"charts/reward\": float(torch.mean(data.rewards)),\n",
        "                  \"agent_steps\": self.global_step,\n",
        "                  \"global_step\": self.global_step,\n",
        "              }\n",
        "          )\n",
        "\n",
        "      allocated_torch = torch.cuda.memory_allocated(self.device) - allocated_torch\n",
        "      allocated_cpu = self.process.memory_info().rss - allocated_cpu\n",
        "      if self.verbose:\n",
        "          print(\n",
        "              \"Allocated during evaluation - Pytorch: %.2f GB, System: %.2f GB\"\n",
        "              % (allocated_torch / 1e9, allocated_cpu / 1e9)\n",
        "          )\n",
        "\n",
        "      uptime = timedelta(seconds=int(time.time() - self.start_time))\n",
        "      print(\n",
        "          f\"Epoch: {self.update} - {self.global_step // 1000}K steps - {uptime} Elapsed\\n\"\n",
        "          f\"\\tSteps Per Second: Env={env_sps}, Inference={inference_sps}\"\n",
        "      )\n",
        "\n",
        "      progress_bar.close()\n",
        "      return data, stats, infos\n",
        "\n"
      ],
      "metadata": {
        "id": "wF-3z-U75kxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train config"
      ],
      "metadata": {
        "id": "3_d-KSjf9oKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "\n",
        "class Config:\n",
        "    # Run a smaller config on your local machine\n",
        "    local_mode = False  # Run in local mode\n",
        "    # Track to run - options: reinforcement_learning, curriculum_generation\n",
        "    track = \"rl\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    #record_loss = False  # log all minibatch loss and actions, for debugging\n",
        "\n",
        "    # Trainer Args\n",
        "    seed = 1\n",
        "    num_cores = None  # Number of cores to use for training\n",
        "    num_envs = 6  # Number of environments to use for training\n",
        "    num_buffers = 2  # Number of buffers to use for training\n",
        "    rollout_batch_size = 2**15 # Number of steps to rollout\n",
        "    eval_batch_size = 2**15 # Number of steps to rollout for eval\n",
        "    train_num_steps = 10_000_000  # Number of steps to train\n",
        "    eval_num_steps = 1_000_000  # Number of steps to evaluate\n",
        "    checkpoint_interval = 30  # Interval to save models\n",
        "    run_name = f\"nmmo_{time.strftime('%Y%m%d_%H%M%S')}\"  # Run name\n",
        "    runs_dir = \"/tmp/runs\"  # Directory for runs\n",
        "    policy_store_dir = None # Policy store directory\n",
        "    use_serial_vecenv = False  # Use serial vecenv implementation\n",
        "    learner_weight = 1.0  # Weight of learner policy\n",
        "    max_opponent_policies = 0  # Maximum number of opponent policies to train against\n",
        "    eval_num_policies = 2 # Number of policies to use for evaluation\n",
        "    eval_num_rounds = 1 # Number of rounds to use for evaluation\n",
        "    wandb_project = None  # WandB project name\n",
        "    wandb_entity = None  # WandB entity name\n",
        "\n",
        "    # PPO Args\n",
        "    bptt_horizon = 8  # Train on this number of steps of a rollout at a time. Used to reduce GPU memory.\n",
        "    ppo_training_batch_size = 128  # Number of rows in a training batch\n",
        "    ppo_update_epochs = 3  # Number of update epochs to use for training\n",
        "    ppo_learning_rate = 0.00015  # Learning rate\n",
        "    clip_coef = 0.1  # PPO clip coefficient\n",
        "\n",
        "    # Environment Args\n",
        "    num_agents = 128  # Number of agents to use for training\n",
        "    num_npcs = 256  # Number of NPCs to use for training\n",
        "    max_episode_length = 1024  # Number of steps per episode\n",
        "    death_fog_tick = None  # Number of ticks before death fog starts\n",
        "    num_maps = 128  # Number of maps to use for training\n",
        "    maps_path = \"maps/train/\"  # Path to maps to use for training\n",
        "    map_size = 128  # Size of maps to use for training\n",
        "    resilient_population = 0.2  # Percentage of agents to be resilient to starvation/dehydration\n",
        "    tasks_path = None  # Path to tasks to use for training\n",
        "    eval_mode = False # Run the postprocessor in the eval mode\n",
        "    early_stop_agent_num = 8  # Stop the episode when the number of agents reaches this number\n",
        "    sqrt_achievement_rewards=False # Use the log of achievement rewards\n",
        "    heal_bonus_weight = 0.03\n",
        "    meander_bonus_weight = 0.02\n",
        "    explore_bonus_weight = 0.01\n",
        "    spawn_immunity = 20\n",
        "\n",
        "    # Policy Args\n",
        "    input_size = 256\n",
        "    hidden_size = 256\n",
        "    num_lstm_layers = 0  # Number of LSTM layers to use\n",
        "    task_size = 4096  # Size of task embedding\n",
        "    encode_task = True  # Encode task\n",
        "    attend_task = \"none\"  # Attend task - options: none, pytorch, nikhil\n",
        "    attentional_decode = True  # Use attentional action decoder\n",
        "    extra_encoders = True  # Use inventory and market encoders\n",
        "\n",
        "    @classmethod\n",
        "    def asdict(cls):\n",
        "        return {attr: getattr(cls, attr) for attr in dir(cls)\n",
        "                if not callable(getattr(cls, attr)) and not attr.startswith(\"__\")}\n",
        "\n",
        "def create_config(config_cls):\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Get attribute names and their values from the static class\n",
        "    attrs = config_cls.asdict()\n",
        "\n",
        "    # Iterate over these attributes and set the default values of arguments to the corresponding attribute values\n",
        "    for attr, value in attrs.items():\n",
        "        # Convert underscores to hyphens to match the argparse argument format\n",
        "        arg_name = f'--{attr.replace(\"_\", \"-\")}'\n",
        "\n",
        "        parser.add_argument(\n",
        "            arg_name,\n",
        "            dest=attr,\n",
        "            type=type(value) if value is not None else str,\n",
        "            default=value,\n",
        "            help=f\"{arg_name} (default: {value})\"\n",
        "        )\n",
        "\n",
        "    return parser.parse_args()"
      ],
      "metadata": {
        "id": "u2OH44p39nb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "6_lTQMjUhSzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "notes on train\n",
        "- initalize polciy\n",
        "- initialize model\n",
        "- intilailze buffer\n"
      ],
      "metadata": {
        "id": "HZYI-2frgBmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycodestyle pycodestyle_magic\n",
        "!pip install flake8\n",
        "%load_ext pycodestyle_magic"
      ],
      "metadata": {
        "id": "Y14-GjXv8Jnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%pycodestyle\n",
        "\n"
      ],
      "metadata": {
        "id": "Ds3faqI78A21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pylint: disable=bad-builtin, no-member, protected-access\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "from types import SimpleNamespace\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from dataclasses import asdict\n",
        "from itertools import cycle\n",
        "\n",
        "import dill\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from nmmo.render.replay_helper import FileReplayHelper\n",
        "from nmmo.task.task_spec import make_task_from_spec\n",
        "\n",
        "import pufferlib\n",
        "from pufferlib.vectorization import Serial, Multiprocessing\n",
        "from pufferlib.policy_store import DirectoryPolicyStore\n",
        "from pufferlib.frameworks import cleanrl\n",
        "import pufferlib.policy_ranker\n",
        "import pufferlib.utils\n",
        "\n",
        "# import environment # TODO: ADD THIS BACCK WHEN SUBMIT\n",
        "\n",
        "# from reinforcement_learning import config, clean_pufferl # TODO: ADD THIS BACCK WHEN SUBMIT\n",
        "\n",
        "def setup_policy_store(policy_store_dir):\n",
        "    # CHECK ME: can be custom models with different architectures loaded here?\n",
        "    if not os.path.exists(policy_store_dir):\n",
        "        raise ValueError(\"Policy store directory does not exist\")\n",
        "    if os.path.exists(os.path.join(policy_store_dir, \"trainer.pt\")):\n",
        "        raise ValueError(\"Policy store directory should not contain trainer.pt\")\n",
        "    logging.info(\"Using policy store from %s\", policy_store_dir)\n",
        "    policy_store = DirectoryPolicyStore(policy_store_dir)\n",
        "    return policy_store\n",
        "\n",
        "def save_replays(policy_store_dir, save_dir, curriculum_file, task_to_assign=None):\n",
        "    # load the checkpoints into the policy store\n",
        "    policy_store = setup_policy_store(policy_store_dir)\n",
        "    policy_ranker = create_policy_ranker(policy_store_dir)\n",
        "    num_policies = len(policy_store._all_policies())\n",
        "\n",
        "    # setup the replay path\n",
        "    save_dir = os.path.join(save_dir, policy_store_dir)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    logging.info(\"Replays will be saved to %s\", save_dir)\n",
        "\n",
        "    # Use 1 env and 1 buffer for replay generation\n",
        "    # TODO: task-condition agents when generating replays\n",
        "    args = SimpleNamespace(**config.Config.asdict())\n",
        "    args.num_envs = 1\n",
        "    args.num_buffers = 1\n",
        "    args.use_serial_vecenv = True\n",
        "    args.learner_weight = 0  # evaluate mode\n",
        "    args.selfplay_num_policies = num_policies + 1\n",
        "    args.early_stop_agent_num = 0  # run the full episode\n",
        "    args.resilient_population = 0  # no resilient agents\n",
        "    args.tasks_path = curriculum_file  # task-conditioning\n",
        "\n",
        "    # NOTE: This creates a dummy learner agent. Is it necessary?\n",
        "    from reinforcement_learning import policy  # import your policy\n",
        "    def make_policy(envs):\n",
        "        learner_policy = policy.Baseline(\n",
        "            envs.driver_env,\n",
        "            input_size=args.input_size,\n",
        "            hidden_size=args.hidden_size,\n",
        "            task_size=args.task_size\n",
        "        )\n",
        "        return cleanrl.Policy(learner_policy)\n",
        "\n",
        "    # Setup the evaluator. No training during evaluation\n",
        "    evaluator = clean_pufferl.CleanPuffeRL(\n",
        "        seed=args.seed,\n",
        "        env_creator=environment.make_env_creator(args),\n",
        "        env_creator_kwargs={},\n",
        "        agent_creator=make_policy,\n",
        "        vectorization=Serial,\n",
        "        num_envs=args.num_envs,\n",
        "        num_cores=args.num_envs,\n",
        "        num_buffers=args.num_buffers,\n",
        "        selfplay_learner_weight=args.learner_weight,\n",
        "        selfplay_num_policies=args.selfplay_num_policies,\n",
        "        policy_store=policy_store,\n",
        "        policy_ranker=policy_ranker, # so that a new ranker is created\n",
        "        data_dir=save_dir,\n",
        "    )\n",
        "\n",
        "    # Load the policies into the policy pool\n",
        "    evaluator.policy_pool.update_policies({\n",
        "        p.name: p.policy(\n",
        "            policy_args=[evaluator.buffers[0]],\n",
        "            device=evaluator.device\n",
        "        ) for p in list(policy_store._all_policies().values())\n",
        "    })\n",
        "\n",
        "    # Set up the replay helper\n",
        "    o, r, d, i = evaluator.buffers[0].recv()  # reset the env\n",
        "    replay_helper = FileReplayHelper()\n",
        "    nmmo_env = evaluator.buffers[0].envs[0].envs[0].env\n",
        "    nmmo_env.realm.record_replay(replay_helper)\n",
        "\n",
        "    if task_to_assign is not None:\n",
        "        with open(curriculum_file, 'rb') as f:\n",
        "            task_with_embedding = dill.load(f) # a list of TaskSpec\n",
        "        assert 0 <= task_to_assign < len(task_with_embedding), \"Task index out of range\"\n",
        "        select_task = task_with_embedding[task_to_assign]\n",
        "\n",
        "        # Assign the task to the env\n",
        "        tasks = make_task_from_spec(nmmo_env.possible_agents,\n",
        "                                    [select_task] * len(nmmo_env.possible_agents))\n",
        "        nmmo_env.tasks = tasks  # this is a hack\n",
        "        print(\"seed:\", args.seed,\n",
        "              \", task:\", nmmo_env.tasks[0].spec_name)\n",
        "\n",
        "    # Run an episode to generate the replay\n",
        "    replay_helper.reset()\n",
        "    while True:\n",
        "        with torch.no_grad():\n",
        "            actions, logprob, value, _ = evaluator.policy_pool.forwards(\n",
        "                torch.Tensor(o).to(evaluator.device),\n",
        "                None,  # dummy lstm state\n",
        "                torch.Tensor(d).to(evaluator.device),\n",
        "            )\n",
        "            value = value.flatten()\n",
        "        evaluator.buffers[0].send(actions.cpu().numpy(), None)\n",
        "        o, r, d, i = evaluator.buffers[0].recv()\n",
        "\n",
        "        num_alive = len(nmmo_env.realm.players)\n",
        "        task_done = sum(1 for task in nmmo_env.tasks if task.completed)\n",
        "        alive_done = sum(1 for task in nmmo_env.tasks\n",
        "                         if task.completed and task.assignee[0] in nmmo_env.realm.players)\n",
        "        print(\"Tick:\", nmmo_env.realm.tick, \", alive agents:\", num_alive, \", task done:\", task_done)\n",
        "        if num_alive == alive_done:\n",
        "            print(\"All alive agents completed the task.\")\n",
        "            break\n",
        "        if num_alive == 0 or nmmo_env.realm.tick == args.max_episode_length:\n",
        "            print(\"All agents died or reached the max episode length.\")\n",
        "            break\n",
        "\n",
        "    # Count how many agents completed the task\n",
        "    print(\"--------------------------------------------------\")\n",
        "    print(\"Task:\", nmmo_env.tasks[0].spec_name)\n",
        "    num_completed = sum(1 for task in nmmo_env.tasks if task.completed)\n",
        "    print(\"Number of agents completed the task:\", num_completed)\n",
        "    avg_progress = np.mean([task.progress_info[\"max_progress\"] for task in nmmo_env.tasks])\n",
        "    print(f\"Average maximum progress (max=1): {avg_progress:.3f}\")\n",
        "    avg_completed_tick = np.mean([task.progress_info[\"completed_tick\"]\n",
        "                                  for task in nmmo_env.tasks if task.completed])\n",
        "    print(f\"Average completed tick: {avg_completed_tick:.1f}\")\n",
        "\n",
        "    # Save the replay file\n",
        "    replay_file = os.path.join(save_dir, f\"replay_{time.strftime('%Y%m%d_%H%M%S')}\")\n",
        "    logging.info(\"Saving replay to %s\", replay_file)\n",
        "    replay_helper.save(replay_file, compress=False)\n",
        "    evaluator.close()\n",
        "\n",
        "def create_policy_ranker(policy_store_dir, ranker_file=\"ranker.pickle\", db_file=\"ranking.sqlite\"):\n",
        "    file = os.path.join(policy_store_dir, ranker_file)\n",
        "    if os.path.exists(file):\n",
        "        logging.info(\"Using existing policy ranker from %s\", file)\n",
        "        policy_ranker = pufferlib.policy_ranker.OpenSkillRanker.load_from_file(file)\n",
        "    else:\n",
        "        logging.info(\"Creating a new policy ranker and db under %s\", policy_store_dir)\n",
        "        db_file = os.path.join(policy_store_dir, db_file)\n",
        "        policy_ranker = pufferlib.policy_ranker.OpenSkillRanker(db_file, \"anchor\")\n",
        "    return policy_ranker\n",
        "\n",
        "class AllPolicySelector(pufferlib.policy_ranker.PolicySelector):\n",
        "    def select_policies(self, policies):\n",
        "        # Return all policy names in the alpahebetical order\n",
        "        # Loops circularly if more policies are needed than available\n",
        "        loop = cycle([\n",
        "            policies[name] for name in sorted(policies.keys()\n",
        "        )])\n",
        "        return [next(loop) for _ in range(self._num)]\n",
        "\n",
        "def rank_policies(policy_store_dir, eval_curriculum_file, device):\n",
        "    # CHECK ME: can be custom models with different architectures loaded here?\n",
        "    policy_store = setup_policy_store(policy_store_dir)\n",
        "    policy_ranker = create_policy_ranker(policy_store_dir)\n",
        "    num_policies = len(policy_store._all_policies())\n",
        "    policy_selector = AllPolicySelector(num_policies)\n",
        "\n",
        "    args = SimpleNamespace(**config.Config.asdict())\n",
        "    args.data_dir = policy_store_dir\n",
        "    args.eval_mode = True\n",
        "    args.num_envs = 5  # sample a bit longer in each env\n",
        "    args.num_buffers = 1\n",
        "    args.learner_weight = 0  # evaluate mode\n",
        "    args.selfplay_num_policies = num_policies + 1\n",
        "    args.early_stop_agent_num = 0  # run the full episode\n",
        "    args.resilient_population = 0  # no resilient agents\n",
        "    args.tasks_path = eval_curriculum_file  # task-conditioning\n",
        "\n",
        "    # NOTE: This creates a dummy learner agent. Is it necessary?\n",
        "    from reinforcement_learning import policy  # import your policy\n",
        "    def make_policy(envs):\n",
        "        learner_policy = policy.Baseline(\n",
        "            envs.driver_env,\n",
        "            input_size=args.input_size,\n",
        "            hidden_size=args.hidden_size,\n",
        "            task_size=args.task_size\n",
        "        )\n",
        "        return cleanrl.Policy(learner_policy)\n",
        "\n",
        "    # Setup the evaluator. No training during evaluation\n",
        "    evaluator = clean_pufferl.CleanPuffeRL(\n",
        "        device=torch.device(device),\n",
        "        seed=args.seed,\n",
        "        env_creator=environment.make_env_creator(args),\n",
        "        env_creator_kwargs={},\n",
        "        agent_creator=make_policy,\n",
        "        data_dir=policy_store_dir,\n",
        "        vectorization=Multiprocessing,\n",
        "        num_envs=args.num_envs,\n",
        "        num_cores=args.num_envs,\n",
        "        num_buffers=args.num_buffers,\n",
        "        selfplay_learner_weight=args.learner_weight,\n",
        "        selfplay_num_policies=args.selfplay_num_policies,\n",
        "        batch_size=args.eval_batch_size,\n",
        "        policy_store=policy_store,\n",
        "        policy_ranker=policy_ranker, # so that a new ranker is created\n",
        "        policy_selector=policy_selector,\n",
        "    )\n",
        "\n",
        "    ranker_file = os.path.join(policy_store_dir, \"ranker.pickle\")\n",
        "    # This is for quick viewing of the ranks, not for the actual ranking\n",
        "    rank_txt = os.path.join(policy_store_dir, \"ranking.txt\")\n",
        "    with open(rank_txt, \"w\") as f:\n",
        "        pass\n",
        "\n",
        "    results = defaultdict(list)\n",
        "    while evaluator.global_step < args.eval_num_steps:\n",
        "        _, stats, infos = evaluator.evaluate()\n",
        "\n",
        "        for pol, vals in infos.items():\n",
        "            results[pol].extend([\n",
        "                e[1] for e in infos[pol]['team_results']\n",
        "            ])\n",
        "\n",
        "        ratings = evaluator.policy_ranker.ratings()\n",
        "        dataframe = pd.DataFrame(\n",
        "            {\n",
        "                (\"Rating\"): [ratings.get(n).get(\"mu\") for n in ratings],\n",
        "                (\"Policy\"): ratings.keys(),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        ratings = evaluator.policy_ranker.save_to_file(ranker_file)\n",
        "        with open(rank_txt, \"a\") as f:\n",
        "            f.write(\n",
        "                \"\\n\\n\"\n",
        "                + dataframe.round(2)\n",
        "                .sort_values(by=[\"Rating\"], ascending=False)\n",
        "                .to_string(index=False)\n",
        "                + \"\\n\\n\"\n",
        "            )\n",
        "\n",
        "    evaluator.close()\n",
        "    for pol, res in results.items():\n",
        "        aggregated = {}\n",
        "        keys = asdict(res[0]).keys()\n",
        "        for k in keys:\n",
        "            if k == 'policy_id':\n",
        "                continue\n",
        "            aggregated[k] = np.mean([asdict(e)[k] for e in res])\n",
        "        results[pol] = aggregated\n",
        "    print('Evaluation complete. Average stats:\\n', results)\n",
        "\n",
        "\n",
        "def start_it(policy_store_dir, replay_save_dir = \"replays\" , replay_mode=False, device = \"cude\",\n",
        "             task_file ='reinforcement_learning/eval_task_with_embedding.pkl',task_index =None ):\n",
        "    \"\"\"Usage: python evaluate.py -p <policy_store_dir> -s <replay_save_dir>\n",
        "\n",
        "    -p, --policy-store-dir: Directory to load policy checkpoints from for evaluation/ranking\n",
        "    -s, --replay-save-dir: Directory to save replays (Default: replays/)\n",
        "    -r, --replay-mode: Replay save mode (Default: False)\n",
        "    -d, --device: Device to use for evaluation/ranking (Default: cuda if available, otherwise cpu)\n",
        "    -t, --task-file: Task file to use for evaluation (Default: reinforcement_learning/eval_task_with_embedding.pkl)\n",
        "    -i, --task-index: The index of the task to assign in the curriculum file (Default: None)\n",
        "\n",
        "    To generate replay from your checkpoints, put them together in policy_store_dir, run the following command,\n",
        "    and replays will be saved under the replays/. The script will only use 1 environment.\n",
        "    $ python evaluate.py -p <policy_store_dir>\n",
        "\n",
        "    To rank your checkpoints, set the eval-mode to true, and the rankings will be printed out.\n",
        "    The replay files will NOT be generated in the eval mode.:\n",
        "    $ python evaluate.py -p <policy_store_dir> -e true\n",
        "\n",
        "    TODO: Pass in the task embedding?\n",
        "    \"\"\"\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    # Process the evaluate.py command line arguments\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument(\n",
        "    #     \"-p\",\n",
        "    #     \"--policy-store-dir\",\n",
        "    #     dest=\"policy_store_dir\",\n",
        "    #     type=str,\n",
        "    #     default=None,\n",
        "    #     help=\"Directory to load policy checkpoints from\",\n",
        "    # )\n",
        "\n",
        "    # parser.add_argument(\n",
        "    #     \"-s\",\n",
        "    #     \"--replay-save-dir\",\n",
        "    #     dest=\"replay_save_dir\",\n",
        "    #     type=str,\n",
        "    #     default=\"replays\",\n",
        "    #     help=\"Directory to save replays (Default: replays/)\",\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"-r\",\n",
        "    #     \"--replay-mode\",\n",
        "    #     dest=\"replay_mode\",\n",
        "    #     action=\"store_true\",\n",
        "    #     help=\"Replay mode (Default: False)\",\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"-d\",\n",
        "    #     \"--device\",\n",
        "    #     dest=\"device\",\n",
        "    #     type=str,\n",
        "    #     default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    #     help=\"Device to use for evaluation/ranking (Default: cuda if available, otherwise cpu)\",\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"-t\",\n",
        "    #     \"--task-file\",\n",
        "    #     dest=\"task_file\",\n",
        "    #     type=str,\n",
        "    #     default=\"reinforcement_learning/eval_task_with_embedding.pkl\",\n",
        "    #     help=\"Task file to use for evaluation\",\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"-i\",\n",
        "    #     \"--task-index\",\n",
        "    #     dest=\"task_index\",\n",
        "    #     type=int,\n",
        "    #     default=None,\n",
        "    #     help=\"The index of the task to assign in the curriculum file\",\n",
        "    # )\n",
        "\n",
        "    # Parse and check the arguments\n",
        "    # eval_args = parser.parse_args()\n",
        "    # assert eval_args.policy_store_dir is not None, \"Policy store directory must be specified\"\n",
        "    if policy_store_dir is None:\n",
        "      print(\"Policy store directory must be specified\")\n",
        "      return\n",
        "\n",
        "    if replay_mode == False:\n",
        "        logging.info(\"Generating replays from the checkpoints in %s\", policy_store_dir)\n",
        "        save_replays(policy_store_dir,replay_save_dir,\n",
        "                     task_file, task_index)\n",
        "    else:\n",
        "        logging.info(\"Ranking checkpoints from %s\", policy_store_dir)\n",
        "        logging.info(\"Replays will NOT be generated\")\n",
        "        rank_policies(policy_store_dir, task_file, device)bghy\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import torch\n",
        "\n",
        "from pufferlib.vectorization import Serial, Multiprocessing\n",
        "from pufferlib.policy_store import DirectoryPolicyStore\n",
        "from pufferlib.frameworks import cleanrl\n",
        "\n",
        "import environment as envs\n",
        "# from reinforcement_learning import config\n",
        "\n",
        "# NOTE: this file changes when running curriculum generation track\n",
        "# Run test_task_encoder.py to regenerate this file (or get it from the repo)\n",
        "BASELINE_CURRICULUM_FILE = \"reinforcement_learning/curriculum_with_embedding.pkl\"\n",
        "CUSTOM_CURRICULUM_FILE = \"curriculum_generation/custom_curriculum_with_embedding.pkl\"\n",
        "\n",
        "\n",
        "class MyArgs:\n",
        "    def __init__(self):\n",
        "        self.local_mode = False\n",
        "        self.track = \"rl\"\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.seed = 1\n",
        "        self.num_cores = 6\n",
        "        self.num_envs = 2\n",
        "        self.num_buffers = 2\n",
        "        self.rollout_batch_size = 2**15\n",
        "        self.eval_batch_size = 2**15\n",
        "        self.train_num_steps = 10_000_000\n",
        "        self.eval_num_steps = 10_000_000\n",
        "        self.checkpoint_interval = 30\n",
        "        self.run_name = f\"nmmo_mappo_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
        "        self.runs_dir = \"/tmp/runs\"\n",
        "        self.policy_store_dir = None\n",
        "        self.use_serial_vecenv = False\n",
        "        self.learner_weight = 1.0\n",
        "        self.max_opponent_policies = 0\n",
        "        self.eval_num_policies = 2\n",
        "        self.eval_num_rounds = 1\n",
        "        self.wandb_project = None\n",
        "        self.wandb_entity = None\n",
        "\n",
        "        # ppo args\n",
        "        self.bptt_horizon = 8  # Train on this number of steps of a rollout at a time. Used to reduce GPU memory.\n",
        "        self.ppo_training_batch_size = 128  # Number of rows in a training batch\n",
        "        self.ppo_update_epochs = 3  # Number of update epochs to use for training\n",
        "        self.ppo_learning_rate = 0.00015  # Learning rate\n",
        "        self.clip_coef = 0.1  # PPO clip coefficient\n",
        "\n",
        "        # env args  args\n",
        "        self.num_agents = 128  # Number of agents to use for training\n",
        "        self.num_npcs = 256  # Number of NPCs to use for training\n",
        "        self.max_episode_length = 1024  # Number of steps per episode\n",
        "        self.death_fog_tick = None  # Number of ticks before death fog starts\n",
        "        self.num_maps = 128  # Number of maps to use for training\n",
        "        self.maps_path = \"maps/train/\"  # Path to maps to use for training\n",
        "        self.map_size = 128  # Size of maps to use for training\n",
        "        self.resilient_population = 0.2  # Percentage of agents to be resilient to starvation/dehydration\n",
        "        self.tasks_path = None  # Path to tasks to use for training\n",
        "        self.eval_mode = False  # Run the postprocessor in eval mode\n",
        "        self.early_stop_agent_num = 8  # Stop the episode when the number of agents reaches this number\n",
        "        self.sqrt_achievement_rewards = False  # Use the log of achievement rewards\n",
        "        self.heal_bonus_weight = 0.03\n",
        "        self.meander_bonus_weight = 0.02\n",
        "        self.explore_bonus_weight = 0.01\n",
        "        self.spawn_immunity = 20\n",
        "\n",
        "        # oplicy\n",
        "        self.input_size = 256\n",
        "        self.hidden_size = 256\n",
        "        self.num_lstm_layers = 0  # Number of LSTM layers to use\n",
        "        self.task_size = 4096  # Size of task embedding\n",
        "        self.encode_task = True  # Encode task\n",
        "        self.attend_task = \"none\"  # Attend task - options: none, pytorch, nikhil\n",
        "        self.attentional_decode = True  # Use attentional action decoder\n",
        "        self.extra_encoders = True  # Use inventory and market encoders\n",
        "\n",
        "        #mappo policy args\n",
        "        # self.device = device\n",
        "        # self.lr = args.lr\n",
        "        # self.critic_lr = args.critic_lr\n",
        "        # self.opti_eps = args.opti_eps\n",
        "        # self.weight_decay = args.weight_decay\n",
        "\n",
        "        # self.obs_space = obs_space\n",
        "        # self.share_obs_space = cent_obs_space\n",
        "        # self.act_space = act_space\n",
        "\n",
        "\n",
        "\n",
        "def setup_env(args):\n",
        "    run_dir = os.path.join(args.runs_dir, args.run_name)\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "    logging.info(\"Training run: %s (%s)\", args.run_name, run_dir)\n",
        "    logging.info(\"Training args: %s\", args)\n",
        "\n",
        "    policy_store = None\n",
        "    if args.policy_store_dir is None:\n",
        "        args.policy_store_dir = os.path.join(run_dir, \"policy_store\")\n",
        "        logging.info(\"Using policy store from %s\", args.policy_store_dir)\n",
        "        policy_store = DirectoryPolicyStore(args.policy_store_dir)\n",
        "\n",
        "\n",
        "\n",
        "    policy = []\n",
        "    for agent_id in range(args.num_agents):\n",
        "        share_observation_space = envs.observation_space[agent_id]\n",
        "        # policy network\n",
        "        po = R_MAPPOPolicy(args,\n",
        "                    envs.observation_space[agent_id],\n",
        "                      envs.action_space,\n",
        "        envs.observation_space,\n",
        "\n",
        "        # input_size=args.input_size,\n",
        "        # hidden_size=args.hidden_size,\n",
        "        # task_size=args.task_size\n",
        "        )\n",
        "        self.policy.append(po)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # chagne this with our trainer\n",
        "\n",
        "    trainer = R_MAPPO(\n",
        "        args,\n",
        "        policy,\n",
        "        torch.device(\"cuda\")\n",
        "\n",
        "        # device=torch.device(device),\n",
        "        # seed=seed,\n",
        "        # env_creator=environment.make_env_creator(),\n",
        "        # env_creator_kwargs={},\n",
        "        # agent_creator=make_policy,\n",
        "        # data_dir=run_dir,\n",
        "        # exp_name=run_name,\n",
        "        # policy_store=policy_store,\n",
        "        # wandb_entity=wandb_entity,\n",
        "        # wandb_project=wandb_project,\n",
        "        # wandb_extra_data=args,\n",
        "        # checkpoint_interval=checkpoint_interval,\n",
        "        # vectorization=Serial if use_serial_vecenv else Multiprocessing,\n",
        "        # total_timesteps=train_num_steps,\n",
        "        # num_envs=num_envs,\n",
        "        # num_cores=num_cores or num_envs,\n",
        "        # num_buffers=num_buffers,\n",
        "        # batch_size=rollout_batch_size,\n",
        "        # learning_rate=ppo_learning_rate,\n",
        "        # selfplay_learner_weight=learner_weight,\n",
        "        # selfplay_num_policies=max_opponent_policies + 1,\n",
        "    )\n",
        "    return trainer\n",
        "\n",
        "def reinforcement_learning_track(trainer, args):\n",
        "    while not trainer.done_training():\n",
        "        trainer.evaluate()\n",
        "        trainer.train(\n",
        "            update_epochs=args.ppo_update_epochs,\n",
        "            bptt_horizon=args.bptt_horizon,\n",
        "            batch_rows=args.ppo_training_batch_size // args.bptt_horizon,\n",
        "            clip_coef=args.clip_coef,\n",
        "        )\n",
        "\n",
        "def curriculum_generation_track(trainer, args, use_elm=True):\n",
        "    from curriculum_generation.task_encoder import TaskEncoder\n",
        "    LLM_CHECKPOINT = \"Salesforce/codegen25-7b-instruct\"\n",
        "\n",
        "    if use_elm:\n",
        "        from curriculum_generation import manual_curriculum\n",
        "        from curriculum_generation.elm import OpenELMTaskGenerator\n",
        "        NUM_SEED_TASKS = 20\n",
        "        NUM_NEW_TASKS = 5\n",
        "        ELM_DEBUG = True\n",
        "\n",
        "        task_encoder = TaskEncoder(LLM_CHECKPOINT, manual_curriculum, batch_size=2)\n",
        "        task_generator = OpenELMTaskGenerator(manual_curriculum.curriculum, LLM_CHECKPOINT)\n",
        "\n",
        "        # Generating new tasks and evaluating all candidate training tasks\n",
        "        for _ in range(3):\n",
        "            # NOTE: adjust NUM_SEED_TASKS to fit your gpu\n",
        "            seed_task_list = task_generator.sample_tasks(NUM_SEED_TASKS, random_ratio=1)\n",
        "            new_task_list = task_generator.evolve_tasks(seed_task_list, NUM_NEW_TASKS, debug=ELM_DEBUG)\n",
        "            task_generator.add_tasks(new_task_list)\n",
        "            task_encoder.get_task_embedding(seed_task_list + new_task_list, save_to_file=CUSTOM_CURRICULUM_FILE)\n",
        "            # CHECK ME: the trainer will automatically use the new task embedding file\n",
        "            _, _, infos = trainer.evaluate()\n",
        "            task_generator.update(infos) # update the task stats\n",
        "\n",
        "        # NOTE: sample_tasks() uses task stats to sample learnable tasks\n",
        "        curriculum = task_generator.sample_tasks(NUM_SEED_TASKS*3, random_ratio=0.3) # NOTE: arbitrary numbers\n",
        "\n",
        "    else:\n",
        "        from curriculum_generation import curriculum_tutorial  # custom tutorial\n",
        "        task_encoder = TaskEncoder(LLM_CHECKPOINT, curriculum_tutorial, batch_size=2)\n",
        "        curriculum = curriculum_tutorial.curriculum\n",
        "\n",
        "    # Use the train_task_spec to train agents\n",
        "    task_encoder.get_task_embedding(curriculum, save_to_file=CUSTOM_CURRICULUM_FILE)\n",
        "    task_encoder.close()\n",
        "    trainer.data.sort_keys = []\n",
        "    reinforcement_learning_track(trainer, args)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    # You can either edit the defaults in config.py or set args\n",
        "    # from the commandline.\n",
        "\n",
        "    # Avoid OOMing your machine for local testing\n",
        "    # wont use this anymore\n",
        "    # if args.local_mode:\n",
        "    #     args.num_envs = 1\n",
        "    #     args.num_buffers = 1\n",
        "    #     args.use_serial_vecenv = True\n",
        "    #     args.rollout_batch_size = 2**10\n",
        "\n",
        "\n",
        "    args = MyArgs()\n",
        "    args.tasks_path = BASELINE_CURRICULUM_FILE\n",
        "    trainer = setup_env(args)\n",
        "    reinforcement_learning_track(trainer, args)\n",
        "\n",
        "      # trainer.prep_training()\n",
        "      #   train_infos = self.trainer.train(self.buffer)\n",
        "      #   self.buffer.after_update()\n",
        "      #   return train_infos\n",
        "    # elif args.track == \"curriculum\":\n",
        "    #   args.tasks_path = CUSTOM_CURRICULUM_FILE\n",
        "    #   trainer = setup_env(args)\n",
        "    #   curriculum_generation_track(trainer, args, use_elm=True)\n",
        "    # else:\n",
        "    #   raise ValueError(f\"Unknown track {args.track}, must be 'rl' or 'curriculum'\")\n",
        "\n",
        "    trainer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "hPPu7gfXazYu",
        "outputId": "966201fc-e5c3-461b-9627-85f3937a09f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-47cf671fe62a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpufferlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSerial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpufferlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_store\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDirectoryPolicyStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpufferlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeworks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcleanrl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pufferlib'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### edits to model3 (leave for later)"
      ],
      "metadata": {
        "id": "ags5cEjPAszR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluate.py"
      ],
      "metadata": {
        "id": "BHmKeQIHZ1Qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# pylint: disable=bad-builtin, no-member, protected-access\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "from types import SimpleNamespace\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from dataclasses import asdict\n",
        "from itertools import cycle\n",
        "\n",
        "import dill\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from nmmo.render.replay_helper import FileReplayHelper\n",
        "from nmmo.task.task_spec import make_task_from_spec\n",
        "\n",
        "import pufferlib\n",
        "from pufferlib.vectorization import Serial, Multiprocessing\n",
        "from pufferlib.policy_store import DirectoryPolicyStore\n",
        "from pufferlib.frameworks import cleanrl\n",
        "import pufferlib.policy_ranker\n",
        "import pufferlib.utils\n",
        "\n",
        "# import environment # TODO: ADD THIS BACCK WHEN SUBMIT\n",
        "\n",
        "# from reinforcement_learning import config, clean_pufferl # TODO: ADD THIS BACCK WHEN SUBMIT\n",
        "\n",
        "def setup_policy_store(policy_store_dir):\n",
        "    # CHECK ME: can be custom models with different architectures loaded here?\n",
        "    if not os.path.exists(policy_store_dir):\n",
        "        raise ValueError(\"Policy store directory does not exist\")\n",
        "    if os.path.exists(os.path.join(policy_store_dir, \"trainer.pt\")):\n",
        "        raise ValueError(\"Policy store directory should not contain trainer.pt\")\n",
        "    logging.info(\"Using policy store from %s\", policy_store_dir)\n",
        "    policy_store = DirectoryPolicyStore(policy_store_dir)\n",
        "    return policy_store\n",
        "\n",
        "def save_replays(policy_store_dir, save_dir, curriculum_file, task_to_assign=None):\n",
        "    # load the checkpoints into the policy store\n",
        "    policy_store = setup_policy_store(policy_store_dir)\n",
        "    policy_ranker = create_policy_ranker(policy_store_dir)\n",
        "    num_policies = len(policy_store._all_policies())\n",
        "\n",
        "    # setup the replay path\n",
        "    save_dir = os.path.join(save_dir, policy_store_dir)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    logging.info(\"Replays will be saved to %s\", save_dir)\n",
        "\n",
        "    # Use 1 env and 1 buffer for replay generation\n",
        "    # TODO: task-condition agents when generating replays\n",
        "    args = SimpleNamespace(**config.Config.asdict())\n",
        "    args.num_envs = 1\n",
        "    args.num_buffers = 1\n",
        "    args.use_serial_vecenv = True\n",
        "    args.learner_weight = 0  # evaluate mode\n",
        "    args.selfplay_num_policies = num_policies + 1\n",
        "    args.early_stop_agent_num = 0  # run the full episode\n",
        "    args.resilient_population = 0  # no resilient agents\n",
        "    args.tasks_path = curriculum_file  # task-conditioning\n",
        "\n",
        "    # NOTE: This creates a dummy learner agent. Is it necessary?\n",
        "    from reinforcement_learning import policy  # import your policy\n",
        "    def make_policy(envs):\n",
        "        learner_policy = policy.Baseline(\n",
        "            envs.driver_env,\n",
        "            input_size=args.input_size,\n",
        "            hidden_size=args.hidden_size,\n",
        "            task_size=args.task_size\n",
        "        )\n",
        "        return cleanrl.Policy(learner_policy)\n",
        "\n",
        "    # Setup the evaluator. No training during evaluation\n",
        "    evaluator = clean_pufferl.CleanPuffeRL(\n",
        "        seed=args.seed,\n",
        "        env_creator=environment.make_env_creator(args),\n",
        "        env_creator_kwargs={},\n",
        "        agent_creator=make_policy,\n",
        "        vectorization=Serial,\n",
        "        num_envs=args.num_envs,\n",
        "        num_cores=args.num_envs,\n",
        "        num_buffers=args.num_buffers,\n",
        "        selfplay_learner_weight=args.learner_weight,\n",
        "        selfplay_num_policies=args.selfplay_num_policies,\n",
        "        policy_store=policy_store,\n",
        "        policy_ranker=policy_ranker, # so that a new ranker is created\n",
        "        data_dir=save_dir,\n",
        "    )\n",
        "\n",
        "    # Load the policies into the policy pool\n",
        "    evaluator.policy_pool.update_policies({\n",
        "        p.name: p.policy(\n",
        "            policy_args=[evaluator.buffers[0]],\n",
        "            device=evaluator.device\n",
        "        ) for p in list(policy_store._all_policies().values())\n",
        "    })\n",
        "\n",
        "    # Set up the replay helper\n",
        "    o, r, d, i = evaluator.buffers[0].recv()  # reset the env\n",
        "    replay_helper = FileReplayHelper()\n",
        "    nmmo_env = evaluator.buffers[0].envs[0].envs[0].env\n",
        "    nmmo_env.realm.record_replay(replay_helper)\n",
        "\n",
        "    if task_to_assign is not None:\n",
        "        with open(curriculum_file, 'rb') as f:\n",
        "            task_with_embedding = dill.load(f) # a list of TaskSpec\n",
        "        assert 0 <= task_to_assign < len(task_with_embedding), \"Task index out of range\"\n",
        "        select_task = task_with_embedding[task_to_assign]\n",
        "\n",
        "        # Assign the task to the env\n",
        "        tasks = make_task_from_spec(nmmo_env.possible_agents,\n",
        "                                    [select_task] * len(nmmo_env.possible_agents))\n",
        "        nmmo_env.tasks = tasks  # this is a hack\n",
        "        print(\"seed:\", args.seed,\n",
        "              \", task:\", nmmo_env.tasks[0].spec_name)\n",
        "\n",
        "    # Run an episode to generate the replay\n",
        "    replay_helper.reset()\n",
        "    while True:\n",
        "        with torch.no_grad():\n",
        "            actions, logprob, value, _ = evaluator.policy_pool.forwards(\n",
        "                torch.Tensor(o).to(evaluator.device),\n",
        "                None,  # dummy lstm state\n",
        "                torch.Tensor(d).to(evaluator.device),\n",
        "            )\n",
        "            value = value.flatten()\n",
        "        evaluator.buffers[0].send(actions.cpu().numpy(), None)\n",
        "        o, r, d, i = evaluator.buffers[0].recv()\n",
        "\n",
        "        num_alive = len(nmmo_env.realm.players)\n",
        "        task_done = sum(1 for task in nmmo_env.tasks if task.completed)\n",
        "        alive_done = sum(1 for task in nmmo_env.tasks\n",
        "                         if task.completed and task.assignee[0] in nmmo_env.realm.players)\n",
        "        print(\"Tick:\", nmmo_env.realm.tick, \", alive agents:\", num_alive, \", task done:\", task_done)\n",
        "        if num_alive == alive_done:\n",
        "            print(\"All alive agents completed the task.\")\n",
        "            break\n",
        "        if num_alive == 0 or nmmo_env.realm.tick == args.max_episode_length:\n",
        "            print(\"All agents died or reached the max episode length.\")\n",
        "            break\n",
        "\n",
        "    # Count how many agents completed the task\n",
        "    print(\"--------------------------------------------------\")\n",
        "    print(\"Task:\", nmmo_env.tasks[0].spec_name)\n",
        "    num_completed = sum(1 for task in nmmo_env.tasks if task.completed)\n",
        "    print(\"Number of agents completed the task:\", num_completed)\n",
        "    avg_progress = np.mean([task.progress_info[\"max_progress\"] for task in nmmo_env.tasks])\n",
        "    print(f\"Average maximum progress (max=1): {avg_progress:.3f}\")\n",
        "    avg_completed_tick = np.mean([task.progress_info[\"completed_tick\"]\n",
        "                                  for task in nmmo_env.tasks if task.completed])\n",
        "    print(f\"Average completed tick: {avg_completed_tick:.1f}\")\n",
        "\n",
        "    # Save the replay file\n",
        "    replay_file = os.path.join(save_dir, f\"replay_{time.strftime('%Y%m%d_%H%M%S')}\")\n",
        "    logging.info(\"Saving replay to %s\", replay_file)\n",
        "    replay_helper.save(replay_file, compress=False)\n",
        "    evaluator.close()\n",
        "\n",
        "def create_policy_ranker(policy_store_dir, ranker_file=\"ranker.pickle\", db_file=\"ranking.sqlite\"):\n",
        "    file = os.path.join(policy_store_dir, ranker_file)\n",
        "    if os.path.exists(file):\n",
        "        logging.info(\"Using existing policy ranker from %s\", file)\n",
        "        policy_ranker = pufferlib.policy_ranker.OpenSkillRanker.load_from_file(file)\n",
        "    else:\n",
        "        logging.info(\"Creating a new policy ranker and db under %s\", policy_store_dir)\n",
        "        db_file = os.path.join(policy_store_dir, db_file)\n",
        "        policy_ranker = pufferlib.policy_ranker.OpenSkillRanker(db_file, \"anchor\")\n",
        "    return policy_ranker\n",
        "\n",
        "class AllPolicySelector(pufferlib.policy_ranker.PolicySelector):\n",
        "    def select_policies(self, policies):\n",
        "        # Return all policy names in the alpahebetical order\n",
        "        # Loops circularly if more policies are needed than available\n",
        "        loop = cycle([\n",
        "            policies[name] for name in sorted(policies.keys()\n",
        "        )])\n",
        "        return [next(loop) for _ in range(self._num)]\n",
        "\n",
        "def rank_policies(policy_store_dir, eval_curriculum_file, device):\n",
        "    # CHECK ME: can be custom models with different architectures loaded here?\n",
        "    policy_store = setup_policy_store(policy_store_dir)\n",
        "    policy_ranker = create_policy_ranker(policy_store_dir)\n",
        "    num_policies = len(policy_store._all_policies())\n",
        "    policy_selector = AllPolicySelector(num_policies)\n",
        "\n",
        "    args = SimpleNamespace(**config.Config.asdict())\n",
        "    args.data_dir = policy_store_dir\n",
        "    args.eval_mode = True\n",
        "    args.num_envs = 5  # sample a bit longer in each env\n",
        "    args.num_buffers = 1\n",
        "    args.learner_weight = 0  # evaluate mode\n",
        "    args.selfplay_num_policies = num_policies + 1\n",
        "    args.early_stop_agent_num = 0  # run the full episode\n",
        "    args.resilient_population = 0  # no resilient agents\n",
        "    args.tasks_path = eval_curriculum_file  # task-conditioning\n",
        "\n",
        "    # NOTE: This creates a dummy learner agent. Is it necessary?\n",
        "    from reinforcement_learning import policy  # import your policy\n",
        "    def make_policy(envs):\n",
        "        learner_policy = policy.Baseline(\n",
        "            envs.driver_env,\n",
        "            input_size=args.input_size,\n",
        "            hidden_size=args.hidden_size,\n",
        "            task_size=args.task_size\n",
        "        )\n",
        "        return cleanrl.Policy(learner_policy)\n",
        "\n",
        "    # Setup the evaluator. No training during evaluation\n",
        "    evaluator = clean_pufferl.CleanPuffeRL(\n",
        "        device=torch.device(device),\n",
        "        seed=args.seed,\n",
        "        env_creator=environment.make_env_creator(args),\n",
        "        env_creator_kwargs={},\n",
        "        agent_creator=make_policy,\n",
        "        data_dir=policy_store_dir,\n",
        "        vectorization=Multiprocessing,\n",
        "        num_envs=args.num_envs,\n",
        "        num_cores=args.num_envs,\n",
        "        num_buffers=args.num_buffers,\n",
        "        selfplay_learner_weight=args.learner_weight,\n",
        "        selfplay_num_policies=args.selfplay_num_policies,\n",
        "        batch_size=args.eval_batch_size,\n",
        "        policy_store=policy_store,\n",
        "        policy_ranker=policy_ranker, # so that a new ranker is created\n",
        "        policy_selector=policy_selector,\n",
        "    )\n",
        "\n",
        "    ranker_file = os.path.join(policy_store_dir, \"ranker.pickle\")\n",
        "    # This is for quick viewing of the ranks, not for the actual ranking\n",
        "    rank_txt = os.path.join(policy_store_dir, \"ranking.txt\")\n",
        "    with open(rank_txt, \"w\") as f:\n",
        "        pass\n",
        "\n",
        "    results = defaultdict(list)\n",
        "    while evaluator.global_step < args.eval_num_steps:\n",
        "        _, stats, infos = evaluator.evaluate()\n",
        "\n",
        "        for pol, vals in infos.items():\n",
        "            results[pol].extend([\n",
        "                e[1] for e in infos[pol]['team_results']\n",
        "            ])\n",
        "\n",
        "        ratings = evaluator.policy_ranker.ratings()\n",
        "        dataframe = pd.DataFrame(\n",
        "            {\n",
        "                (\"Rating\"): [ratings.get(n).get(\"mu\") for n in ratings],\n",
        "                (\"Policy\"): ratings.keys(),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        ratings = evaluator.policy_ranker.save_to_file(ranker_file)\n",
        "        with open(rank_txt, \"a\") as f:\n",
        "            f.write(\n",
        "                \"\\n\\n\"\n",
        "                + dataframe.round(2)\n",
        "                .sort_values(by=[\"Rating\"], ascending=False)\n",
        "                .to_string(index=False)\n",
        "                + \"\\n\\n\"\n",
        "            )\n",
        "\n",
        "    evaluator.close()\n",
        "    for pol, res in results.items():\n",
        "        aggregated = {}\n",
        "        keys = asdict(res[0]).keys()\n",
        "        for k in keys:\n",
        "            if k == 'policy_id':\n",
        "                continue\n",
        "            aggregated[k] = np.mean([asdict(e)[k] for e in res])\n",
        "        results[pol] = aggregated\n",
        "    print('Evaluation complete. Average stats:\\n', results)\n",
        "\n",
        "\n",
        "def start_it(policy_store_dir, replay_save_dir = \"replays\" , replay_mode=False, device = \"cude\",\n",
        "             task_file ='reinforcement_learning/eval_task_with_embedding.pkl',task_index =None ):\n",
        "    \"\"\"Usage: python evaluate.py -p <policy_store_dir> -s <replay_save_dir>\n",
        "\n",
        "    -p, --policy-store-dir: Directory to load policy checkpoints from for evaluation/ranking\n",
        "    -s, --replay-save-dir: Directory to save replays (Default: replays/)\n",
        "    -r, --replay-mode: Replay save mode (Default: False)\n",
        "    -d, --device: Device to use for evaluation/ranking (Default: cuda if available, otherwise cpu)\n",
        "    -t, --task-file: Task file to use for evaluation (Default: reinforcement_learning/eval_task_with_embedding.pkl)\n",
        "    -i, --task-index: The index of the task to assign in the curriculum file (Default: None)\n",
        "\n",
        "    To generate replay from your checkpoints, put them together in policy_store_dir, run the following command,\n",
        "    and replays will be saved under the replays/. The script will only use 1 environment.\n",
        "    $ python evaluate.py -p <policy_store_dir>\n",
        "\n",
        "    To rank your checkpoints, set the eval-mode to true, and the rankings will be printed out.\n",
        "    The replay files will NOT be generated in the eval mode.:\n",
        "    $ python evaluate.py -p <policy_store_dir> -e true\n",
        "\n",
        "    TODO: Pass in the task embedding?\n",
        "    \"\"\"\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    # Process the evaluate.py command line arguments\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument(\n",
        "    #     \"-p\",\n",
        "    #     \"--policy-store-dir\",\n",
        "    #     dest=\"policy_store_dir\",\n",
        "    #     type=str,\n",
        "    #     default=None,\n",
        "    #     help=\"Directory to load policy checkpoints from\",\n",
        "    # )\n",
        "\n",
        "    # parser.add_argument(\n",
        "    #     \"-s\",\n",
        "    #     \"--replay-save-dir\",\n",
        "    #     dest=\"replay_save_dir\",\n",
        "    #     type=str,\n",
        "    #     default=\"replays\",\n",
        "    #     help=\"Directory to save replays (Default: replays/)\",\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"-r\",\n",
        "    #     \"--replay-mode\",\n",
        "    #     dest=\"replay_mode\",\n",
        "    #     action=\"store_true\",\n",
        "    #     help=\"Replay mode (Default: False)\",\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"-d\",\n",
        "    #     \"--device\",\n",
        "    #     dest=\"device\",\n",
        "    #     type=str,\n",
        "    #     default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    #     help=\"Device to use for evaluation/ranking (Default: cuda if available, otherwise cpu)\",\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"-t\",\n",
        "    #     \"--task-file\",\n",
        "    #     dest=\"task_file\",\n",
        "    #     type=str,\n",
        "    #     default=\"reinforcement_learning/eval_task_with_embedding.pkl\",\n",
        "    #     help=\"Task file to use for evaluation\",\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"-i\",\n",
        "    #     \"--task-index\",\n",
        "    #     dest=\"task_index\",\n",
        "    #     type=int,\n",
        "    #     default=None,\n",
        "    #     help=\"The index of the task to assign in the curriculum file\",\n",
        "    # )\n",
        "\n",
        "    # Parse and check the arguments\n",
        "    # eval_args = parser.parse_args()\n",
        "    # assert eval_args.policy_store_dir is not None, \"Policy store directory must be specified\"\n",
        "    if policy_store_dir is None:\n",
        "      print(\"Policy store directory must be specified\")\n",
        "      return\n",
        "\n",
        "    if replay_mode == False:\n",
        "        logging.info(\"Generating replays from the checkpoints in %s\", policy_store_dir)\n",
        "        save_replays(policy_store_dir,replay_save_dir,\n",
        "                     task_file, task_index)\n",
        "    else:\n",
        "        logging.info(\"Ranking checkpoints from %s\", policy_store_dir)\n",
        "        logging.info(\"Replays will NOT be generated\")\n",
        "        rank_policies(policy_store_dir, task_file, device)"
      ],
      "metadata": {
        "id": "JdOnuxB1UVvT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "832feb8b-5108-4e0f-eafb-5805f635d03a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-34addaabae4e>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dill'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NfrqdN7tAsOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3drPqFEGmhZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 4  "
      ],
      "metadata": {
        "id": "-tEO8mNOab4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail\n",
        "\n",
        "*   IPPO\n",
        "* THE OTHER FORM OF MAPPO\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ea74kcUMTxUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pylint: disable=all\n",
        "# PufferLib's customized CleanRL Transfo\n",
        "from pdb import set_trace as T\n",
        "\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from datetime import timedelta\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import numpy as np\n",
        "import psutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pufferlib\n",
        "import pufferlib.emulation\n",
        "import pufferlib.frameworks.cleanrl\n",
        "import pufferlib.policy_pool\n",
        "import pufferlib.policy_ranker\n",
        "import pufferlib.utils\n",
        "import pufferlib.vectorization\n",
        "\n",
        "\n",
        "def unroll_nested_dict(d):\n",
        "    if not isinstance(d, dict):\n",
        "        return d\n",
        "\n",
        "    for k, v in d.items():\n",
        "        if isinstance(v, dict):\n",
        "            for k2, v2 in unroll_nested_dict(v):\n",
        "                yield f\"{k}/{k2}\", v2\n",
        "        else:\n",
        "            yield k, v\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class CleanPuffeRL:\n",
        "    env_creator: callable = None\n",
        "    env_creator_kwargs: dict = None\n",
        "    agent: nn.Module = None\n",
        "    agent_creator: callable = None\n",
        "    agent_kwargs: dict = None\n",
        "\n",
        "    path = '/content/drive/MyDrive/nmmo'\n",
        "\n",
        "\n",
        "    exp_name: str = os.path.basename(path)\n",
        "\n",
        "    data_dir: str = 'data'\n",
        "    record_loss: bool = False\n",
        "    checkpoint_interval: int = 1\n",
        "    seed: int = 1\n",
        "    torch_deterministic: bool = True\n",
        "    vectorization: ... = pufferlib.vectorization.Serial\n",
        "    device: str = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
        "    total_timesteps: int = 10_000_000\n",
        "    learning_rate: float = 2.5e-4\n",
        "    num_buffers: int = 1\n",
        "    num_envs: int = 8\n",
        "    num_cores: int = psutil.cpu_count(logical=False)\n",
        "    cpu_offload: bool = True\n",
        "    verbose: bool = True\n",
        "    batch_size: int = 2**14\n",
        "    policy_store: pufferlib.policy_store.PolicyStore = None\n",
        "    policy_ranker: pufferlib.policy_ranker.PolicyRanker = None\n",
        "\n",
        "    policy_pool: pufferlib.policy_pool.PolicyPool = None\n",
        "    policy_selector: pufferlib.policy_ranker.PolicySelector = None\n",
        "\n",
        "    # Wandb\n",
        "    wandb_entity: str = None\n",
        "    wandb_project: str = None\n",
        "    wandb_extra_data: dict = None\n",
        "\n",
        "    # Selfplay\n",
        "    selfplay_learner_weight: float = 1.0\n",
        "    selfplay_num_policies: int = 1\n",
        "\n",
        "    def __post_init__(self, *args, **kwargs):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "        # If data_dir is provided, load the resume state\n",
        "        resume_state = {}\n",
        "        if self.data_dir is not None:\n",
        "          path = os.path.join(self.data_dir, f\"trainer.pt\")\n",
        "          if os.path.exists(path):\n",
        "            print(f\"Loaded checkpoint from {path}\")\n",
        "            resume_state = torch.load(path)\n",
        "            print(f\"Resuming from update {resume_state['update']} \"\n",
        "                  f\"with policy {resume_state['policy_checkpoint_name']}\")\n",
        "\n",
        "        self.wandb_run_id = resume_state.get(\"wandb_run_id\", None)\n",
        "        self.learning_rate = resume_state.get(\"learning_rate\", self.learning_rate)\n",
        "\n",
        "        self.global_step = resume_state.get(\"global_step\", 0)\n",
        "        self.agent_step = resume_state.get(\"agent_step\", 0)\n",
        "        self.update = resume_state.get(\"update\", 0)\n",
        "\n",
        "        self.total_updates = self.total_timesteps // self.batch_size\n",
        "        self.envs_per_worker = self.num_envs // self.num_cores\n",
        "        assert self.num_cores * self.envs_per_worker == self.num_envs\n",
        "\n",
        "        # Seed everything\n",
        "        random.seed(self.seed)\n",
        "        np.random.seed(self.seed)\n",
        "        if self.seed is not None:\n",
        "            torch.manual_seed(self.seed)\n",
        "        torch.backends.cudnn.deterministic = self.torch_deterministic\n",
        "\n",
        "        # Create environments\n",
        "        self.process = psutil.Process()\n",
        "        allocated = self.process.memory_info().rss\n",
        "        self.buffers = [\n",
        "            self.vectorization(\n",
        "                self.env_creator,\n",
        "                env_kwargs=self.env_creator_kwargs,\n",
        "                num_workers=self.num_cores,\n",
        "                envs_per_worker=self.envs_per_worker,\n",
        "            )\n",
        "            for _ in range(self.num_buffers)\n",
        "        ]\n",
        "        self.num_agents = self.buffers[0].num_agents\n",
        "\n",
        "        # If an agent_creator is provided, use envs (=self.buffers[0]) to create the agent\n",
        "        self.agent = pufferlib.emulation.make_object(\n",
        "            self.agent, self.agent_creator, self.buffers[:1], self.agent_kwargs)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                \"Allocated %.2f MB to environments. Only accurate for Serial backend.\"\n",
        "                % ((self.process.memory_info().rss - allocated) / 1e6)\n",
        "            )\n",
        "\n",
        "        # Create policy store\n",
        "        if self.policy_store is None:\n",
        "            if self.data_dir is not None:\n",
        "                self.policy_store = pufferlib.policy_store.DirectoryPolicyStore(\n",
        "                    os.path.join(self.data_dir, \"policies\")\n",
        "                )\n",
        "\n",
        "        # Create policy ranker\n",
        "        if self.policy_ranker is None:\n",
        "            if self.data_dir is not None:\n",
        "                db_file = os.path.join(self.data_dir, \"ranking.sqlite\")\n",
        "                self.policy_ranker = pufferlib.policy_ranker.OpenSkillRanker(db_file, \"anchor\")\n",
        "            if \"learner\" not in self.policy_ranker.ratings():\n",
        "                self.policy_ranker.add_policy(\"learner\")\n",
        "\n",
        "        # Setup agent\n",
        "        if \"policy_checkpoint_name\" in resume_state:\n",
        "          self.agent = self.policy_store.get_policy(\n",
        "            resume_state[\"policy_checkpoint_name\"]\n",
        "          ).policy(policy_args=[self.buffers[0]])\n",
        "\n",
        "        # TODO: this can be cleaned up\n",
        "        self.agent.is_recurrent = hasattr(self.agent, \"lstm\")\n",
        "        self.agent = self.agent.to(self.device)\n",
        "\n",
        "        # Setup policy pool\n",
        "        if self.policy_pool is None:\n",
        "            self.policy_pool = pufferlib.policy_pool.PolicyPool(\n",
        "                self.agent,\n",
        "                \"learner\",\n",
        "                num_envs=self.num_envs,\n",
        "                num_agents=self.num_agents,\n",
        "                learner_weight=self.selfplay_learner_weight,\n",
        "                num_policies=self.selfplay_num_policies,\n",
        "            )\n",
        "\n",
        "        # Setup policy selector\n",
        "        if self.policy_selector is None:\n",
        "            self.policy_selector = pufferlib.policy_ranker.PolicySelector(\n",
        "                self.selfplay_num_policies - 1, exclude_names=\"learner\"\n",
        "            )\n",
        "\n",
        "        # Setup optimizer\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.agent.parameters(), lr=self.learning_rate, eps=1e-5\n",
        "        )\n",
        "        if \"optimizer_state_dict\" in resume_state:\n",
        "          self.optimizer.load_state_dict(resume_state[\"optimizer_state_dict\"])\n",
        "\n",
        "        ### Allocate Storage\n",
        "        next_obs, next_done, next_lstm_state = [], [], []\n",
        "        for i, envs in enumerate(self.buffers):\n",
        "            envs.async_reset(self.seed + i)\n",
        "            next_done.append(\n",
        "                torch.zeros((self.num_envs * self.num_agents,)).to(self.device)\n",
        "            )\n",
        "            next_obs.append([])\n",
        "\n",
        "            if self.agent.is_recurrent:\n",
        "                shape = (\n",
        "                    self.agent.lstm.num_layers,\n",
        "                    self.num_envs * self.num_agents,\n",
        "                    self.agent.lstm.hidden_size,\n",
        "                )\n",
        "                next_lstm_state.append(\n",
        "                    (\n",
        "                        torch.zeros(shape).to(self.device),\n",
        "                        torch.zeros(shape).to(self.device),\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                next_lstm_state.append(None)\n",
        "\n",
        "        allocated_torch = torch.cuda.memory_allocated(self.device)\n",
        "        allocated_cpu = self.process.memory_info().rss\n",
        "        self.data = SimpleNamespace(\n",
        "            buf=0,\n",
        "            sort_keys=[],\n",
        "            next_obs=next_obs,\n",
        "            next_done=next_done,\n",
        "            next_lstm_state=next_lstm_state,\n",
        "            obs=torch.zeros(\n",
        "                self.batch_size + 1, *self.buffers[0].single_observation_space.shape\n",
        "            ).to(\"cpu\" if self.cpu_offload else self.device),\n",
        "            actions=torch.zeros(\n",
        "                self.batch_size + 1, *self.buffers[0].single_action_space.shape, dtype=int\n",
        "            ).to(self.device),\n",
        "            logprobs=torch.zeros(self.batch_size + 1).to(self.device),\n",
        "            rewards=torch.zeros(self.batch_size + 1).to(self.device),\n",
        "            dones=torch.zeros(self.batch_size + 1).to(self.device),\n",
        "            values=torch.zeros(self.batch_size + 1).to(self.device),\n",
        "        )\n",
        "\n",
        "        allocated_torch = torch.cuda.memory_allocated(self.device) - allocated_torch\n",
        "        allocated_cpu = self.process.memory_info().rss - allocated_cpu\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                \"Allocated to storage - Pytorch: %.2f GB, System: %.2f GB\"\n",
        "                % (allocated_torch / 1e9, allocated_cpu / 1e9)\n",
        "            )\n",
        "\n",
        "        if self.record_loss and self.data_dir is not None:\n",
        "            self.loss_file = os.path.join(self.data_dir, \"loss.txt\")\n",
        "            with open(self.loss_file, \"w\") as f:\n",
        "                pass\n",
        "            self.action_file = os.path.join(self.data_dir, \"actions.txt\")\n",
        "            with open(self.action_file, \"w\") as f:\n",
        "                pass\n",
        "\n",
        "        if self.wandb_entity is not None:\n",
        "            self.wandb_run_id = self.wandb_run_id or wandb.util.generate_id()\n",
        "\n",
        "            wandb.init(\n",
        "                id=self.wandb_run_id,\n",
        "                project=self.wandb_project,\n",
        "                entity=self.wandb_entity,\n",
        "                config=self.wandb_extra_data or {},\n",
        "                sync_tensorboard=True,\n",
        "                name=self.exp_name,\n",
        "                monitor_gym=True,\n",
        "                save_code=True,\n",
        "                resume=\"allow\",\n",
        "            )\n",
        "\n",
        "    @pufferlib.utils.profile\n",
        "    def evaluate(self, show_progress=False):\n",
        "        # Pick new policies for the policy pool\n",
        "        # TODO: find a way to not switch mid-stream\n",
        "        self.policy_pool.update_policies({\n",
        "            p.name: p.policy(\n",
        "                policy_args=[self.buffers[0]],\n",
        "                device=self.device,\n",
        "            ) for p in self.policy_store.select_policies(self.policy_selector)\n",
        "        })\n",
        "\n",
        "        allocated_torch = torch.cuda.memory_allocated(self.device)\n",
        "        allocated_cpu = self.process.memory_info().rss\n",
        "        ptr = env_step_time = inference_time = agent_steps_collected = 0\n",
        "        padded_steps_collected = 0\n",
        "\n",
        "        step = 0\n",
        "        infos = defaultdict(lambda: defaultdict(list))\n",
        "        stats = defaultdict(lambda: defaultdict(list))\n",
        "        performance = defaultdict(list)\n",
        "        progress_bar = tqdm(total=self.batch_size, disable=not show_progress)\n",
        "\n",
        "        data = self.data\n",
        "        while True:\n",
        "            buf = data.buf\n",
        "\n",
        "            step += 1\n",
        "            if ptr == self.batch_size + 1:\n",
        "                break\n",
        "\n",
        "            start = time.time()\n",
        "            o, r, d, i = self.buffers[buf].recv() # obs, rewards, dones, infos\n",
        "            print(\"dones:\", d)\n",
        "            print(\"infos\", i)\n",
        "            env_step_time += time.time() - start\n",
        "\n",
        "            i = self.policy_pool.update_scores(i, \"return\")\n",
        "\n",
        "            '''\n",
        "            for profile in self.buffers[buf].profile():\n",
        "                for k, v in profile.items():\n",
        "                    performance[k].append(v[\"delta\"])\n",
        "            '''\n",
        "\n",
        "            o = torch.Tensor(o)\n",
        "            if not self.cpu_offload:\n",
        "                o = o.to(self.device)\n",
        "\n",
        "            r = torch.Tensor(r).float().to(self.device).view(-1) # is this encoding the rewarrd?\n",
        "\n",
        "            if len(d) != 0 and len(data.next_done[buf]) != 0:\n",
        "                alive_mask = (data.next_done[buf].cpu() + torch.Tensor(d)) != 2\n",
        "                data.next_done[buf] = torch.Tensor(d).to(self.device)\n",
        "            else:\n",
        "                alive_mask = [1 for _ in range(len(o))]\n",
        "\n",
        "            agent_steps_collected += sum(alive_mask)\n",
        "            padded_steps_collected += len(alive_mask)\n",
        "\n",
        "            # ALGO LOGIC: action logic\n",
        "            start = time.time()\n",
        "            with torch.no_grad():\n",
        "                (\n",
        "                    actions,\n",
        "                    logprob,\n",
        "                    value,\n",
        "                    data.next_lstm_state[buf], #lstm here ?\n",
        "                ) = self.policy_pool.forwards(\n",
        "                    o.to(self.device),\n",
        "                    data.next_lstm_state[buf],\n",
        "                    data.next_done[buf],\n",
        "                )\n",
        "                value = value.flatten()\n",
        "            inference_time += time.time() - start\n",
        "\n",
        "            # TRY NOT TO MODIFY: execute the game\n",
        "            start = time.time()\n",
        "            self.buffers[buf].send(actions.cpu().numpy(), None)\n",
        "            env_step_time += time.time() - start\n",
        "            data.buf = (data.buf + 1) % self.num_buffers\n",
        "\n",
        "            # Index alive mask with policy pool idxs...\n",
        "            # TODO: Find a way to avoid having to do this\n",
        "            if self.selfplay_learner_weight > 0:\n",
        "              alive_mask = np.array(alive_mask) * self.policy_pool.learner_mask\n",
        "\n",
        "            for idx in np.where(alive_mask)[0]:\n",
        "                if ptr == self.batch_size + 1:\n",
        "                    break\n",
        "\n",
        "                data.obs[ptr] = o[idx]\n",
        "                data.values[ptr] = value[idx]\n",
        "                data.actions[ptr] = actions[idx]\n",
        "                data.logprobs[ptr] = logprob[idx]\n",
        "                data.sort_keys.append((buf, idx, step))\n",
        "\n",
        "                if len(d) != 0:\n",
        "                    data.rewards[ptr] = r[idx]\n",
        "                    data.dones[ptr] = d[idx]\n",
        "\n",
        "                ptr += 1\n",
        "                progress_bar.update(1)\n",
        "\n",
        "            '''\n",
        "            for ii in i:\n",
        "                if not ii:\n",
        "                    continue\n",
        "\n",
        "                for agent_i, values in ii.items():\n",
        "                    for name, stat in unroll_nested_dict(values):\n",
        "                        infos[name].append(stat)\n",
        "                        try:\n",
        "                            stat = float(stat)\n",
        "                            stats[name].append(stat)\n",
        "                        except:\n",
        "                            continue\n",
        "            '''\n",
        "\n",
        "            for policy_name, policy_i in i.items():\n",
        "                for agent_i in policy_i:\n",
        "                    if not agent_i:\n",
        "                        continue\n",
        "\n",
        "                    for name, stat in unroll_nested_dict(agent_i):\n",
        "                        infos[policy_name][name].append(stat)\n",
        "                        if 'Task_eval_fn' in name:\n",
        "                            # Temporary hack for NMMO competition\n",
        "                            continue\n",
        "                        try:\n",
        "                            stat = float(stat)\n",
        "                            stats[policy_name][name].append(stat)\n",
        "                        except:\n",
        "                            continue\n",
        "\n",
        "        if self.policy_pool.scores and self.policy_ranker is not None:\n",
        "          self.policy_ranker.update_ranks(\n",
        "              self.policy_pool.scores,\n",
        "              wandb_policies=[self.policy_pool._learner_name]\n",
        "              if self.wandb_entity\n",
        "              else [],\n",
        "              step=self.global_step,\n",
        "          )\n",
        "          self.policy_pool.scores = {}\n",
        "\n",
        "        env_sps = int(agent_steps_collected / env_step_time)\n",
        "        inference_sps = int(padded_steps_collected / inference_time)\n",
        "\n",
        "        progress_bar.set_description(\n",
        "            \"Eval: \"\n",
        "            + \", \".join(\n",
        "                [\n",
        "                    f\"Env SPS: {env_sps}\",\n",
        "                    f\"Inference SPS: {inference_sps}\",\n",
        "                    f\"Agent Steps: {agent_steps_collected}\",\n",
        "                    *[f\"{k}: {np.mean(v):.2f}\" for k, v in stats['learner'].items()],\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.global_step += self.batch_size\n",
        "\n",
        "        if self.wandb_entity:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"performance/env_time\": env_step_time,\n",
        "                    \"performance/env_sps\": env_sps,\n",
        "                    \"performance/inference_time\": inference_time,\n",
        "                    \"performance/inference_sps\": inference_sps,\n",
        "                    **{\n",
        "                        f\"performance/env/{k}\": np.mean(v)\n",
        "                        for k, v in performance.items()\n",
        "                    },\n",
        "                    **{f\"charts/{k}\": np.mean(v) for k, v in stats['learner'].items()},\n",
        "                    \"charts/reward\": float(torch.mean(data.rewards)),\n",
        "                    \"agent_steps\": self.global_step,\n",
        "                    \"global_step\": self.global_step,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        allocated_torch = torch.cuda.memory_allocated(self.device) - allocated_torch\n",
        "        allocated_cpu = self.process.memory_info().rss - allocated_cpu\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                \"Allocated during evaluation - Pytorch: %.2f GB, System: %.2f GB\"\n",
        "                % (allocated_torch / 1e9, allocated_cpu / 1e9)\n",
        "            )\n",
        "\n",
        "        uptime = timedelta(seconds=int(time.time() - self.start_time))\n",
        "        print(\n",
        "            f\"Epoch: {self.update} - {self.global_step // 1000}K steps - {uptime} Elapsed\\n\"\n",
        "            f\"\\tSteps Per Second: Env={env_sps}, Inference={inference_sps}\"\n",
        "        )\n",
        "\n",
        "        progress_bar.close()\n",
        "        return data, stats, infos\n",
        "\n",
        "    @pufferlib.utils.profile\n",
        "    def train(\n",
        "        self,\n",
        "        batch_rows=32,\n",
        "        update_epochs=4,\n",
        "        bptt_horizon=16,\n",
        "        gamma=0.99,\n",
        "        gae_lambda=0.95,\n",
        "        anneal_lr=True,\n",
        "        norm_adv=True,\n",
        "        clip_coef=0.1,\n",
        "        clip_vloss=True,\n",
        "        ent_coef=0.01,\n",
        "        vf_coef=0.5,\n",
        "        max_grad_norm=0.5,\n",
        "        target_kl=None,\n",
        "    ):\n",
        "        if self.done_training():\n",
        "            raise RuntimeError(\n",
        "                f\"Trying to train for more than max_updates={self.total_updates} updates\"\n",
        "            )\n",
        "\n",
        "        # assert self.num_steps % bptt_horizon == 0, \"num_steps must be divisible by bptt_horizon\"\n",
        "        allocated_torch = torch.cuda.memory_allocated(self.device)\n",
        "        allocated_cpu = self.process.memory_info().rss\n",
        "\n",
        "        # Annealing the rate if instructed to do so.\n",
        "        if anneal_lr:\n",
        "            frac = 1.0 - (self.update - 1.0) / self.total_updates\n",
        "            lrnow = frac * self.learning_rate\n",
        "            self.optimizer.param_groups[0][\"lr\"] = lrnow\n",
        "\n",
        "        # Sort here\n",
        "        data = self.data\n",
        "        idxs = sorted(range(len(data.sort_keys)), key=data.sort_keys.__getitem__)\n",
        "        data.sort_keys = []\n",
        "\n",
        "        num_minibatches = self.batch_size // bptt_horizon // batch_rows\n",
        "        b_idxs = (\n",
        "            torch.Tensor(idxs)\n",
        "            .long()[:-1]\n",
        "            .reshape(batch_rows, num_minibatches, bptt_horizon)\n",
        "            .transpose(0, 1)\n",
        "        )\n",
        "\n",
        "        # bootstrap value if not done\n",
        "        with torch.no_grad():\n",
        "            advantages = torch.zeros(self.batch_size, device=self.device)\n",
        "            lastgaelam = 0\n",
        "            for t in reversed(range(self.batch_size)):\n",
        "                i, i_nxt = idxs[t], idxs[t + 1]\n",
        "                nextnonterminal = 1.0 - data.dones[i_nxt]\n",
        "                nextvalues = data.values[i_nxt]\n",
        "                delta = (\n",
        "                    data.rewards[i_nxt]\n",
        "                    + gamma * nextvalues * nextnonterminal\n",
        "                    - data.values[i]\n",
        "                )\n",
        "                print(delta)\n",
        "                advantages[t] = lastgaelam = (\n",
        "                    delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
        "                )\n",
        "\n",
        "        # Flatten the batch\n",
        "        data.b_obs = b_obs = data.obs[b_idxs]\n",
        "        b_actions = data.actions[b_idxs]\n",
        "        b_logprobs = data.logprobs[b_idxs]\n",
        "        b_dones = data.dones[b_idxs]\n",
        "        b_values = data.values[b_idxs]\n",
        "        b_advantages = advantages.reshape(\n",
        "            batch_rows, num_minibatches, bptt_horizon\n",
        "        ).transpose(0, 1)\n",
        "        b_returns = b_advantages + b_values\n",
        "\n",
        "        # Optimizing the policy and value network\n",
        "        train_time = time.time()\n",
        "        clipfracs = []\n",
        "        for epoch in range(update_epochs): #epoch\n",
        "            lstm_state = None\n",
        "            for mb in range(num_minibatches):\n",
        "                mb_obs = b_obs[mb].to(self.device)\n",
        "                mb_actions = b_actions[mb].contiguous()\n",
        "                mb_values = b_values[mb].reshape(-1)\n",
        "                mb_advantages = b_advantages[mb].reshape(-1)\n",
        "                mb_returns = b_returns[mb].reshape(-1)\n",
        "\n",
        "                if self.agent.is_recurrent:\n",
        "                    (\n",
        "                        _,\n",
        "                        newlogprob,\n",
        "                        entropy,\n",
        "                        newvalue,\n",
        "                        lstm_state,\n",
        "                    ) = self.agent.get_action_and_value(\n",
        "                        mb_obs, state=lstm_state, done=b_dones[mb], action=mb_actions\n",
        "                    )\n",
        "                    lstm_state = (lstm_state[0].detach(), lstm_state[1].detach()) # lstm\n",
        "                    print(\"lstm_state:\", lstm_state)\n",
        "                else:\n",
        "                    _, newlogprob, entropy, newvalue = self.agent.get_action_and_value(\n",
        "                        mb_obs.reshape(\n",
        "                            -1, *self.buffers[0].single_observation_space.shape\n",
        "                        ),\n",
        "                        action=mb_actions,\n",
        "                    )\n",
        "\n",
        "                logratio = newlogprob - b_logprobs[mb].reshape(-1)\n",
        "                ratio = logratio.exp()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
        "                    old_approx_kl = (-logratio).mean()\n",
        "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
        "                    clipfracs += [\n",
        "                        ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
        "                    ]\n",
        "\n",
        "                mb_advantages = mb_advantages.reshape(-1)\n",
        "                if norm_adv:\n",
        "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (\n",
        "                        mb_advantages.std() + 1e-8\n",
        "                    )\n",
        "\n",
        "                # Policy loss\n",
        "                pg_loss1 = -mb_advantages * ratio\n",
        "                pg_loss2 = -mb_advantages * torch.clamp(\n",
        "                    ratio, 1 - clip_coef, 1 + clip_coef\n",
        "                )\n",
        "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "                # Value loss\n",
        "                newvalue = newvalue.view(-1)\n",
        "                if clip_vloss:\n",
        "                    v_loss_unclipped = (newvalue - mb_returns) ** 2\n",
        "                    v_clipped = mb_values + torch.clamp(\n",
        "                        newvalue - mb_values,\n",
        "                        -clip_coef,\n",
        "                        clip_coef,\n",
        "                    )\n",
        "                    v_loss_clipped = (v_clipped - mb_returns) ** 2\n",
        "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
        "                    v_loss = 0.5 * v_loss_max.mean()\n",
        "                else:\n",
        "                    v_loss = 0.5 * ((newvalue - mb_returns) ** 2).mean()\n",
        "\n",
        "                # loss here\n",
        "                entropy_loss = entropy.mean()\n",
        "                loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
        "\n",
        "                if self.record_loss:\n",
        "                    with open(self.loss_file, \"a\") as f:\n",
        "                        print(f\"# mini batch ({epoch}, {mb}) -- pg_loss:{pg_loss.item():.4f}, value_loss:{v_loss.item():.4f}, \" + \\\n",
        "                              f\"entropy:{entropy_loss.item():.4f}, approx_kl: {approx_kl.item():.4f}\",\n",
        "                                file=f)\n",
        "                    with open(self.action_file, \"a\") as f:\n",
        "                        print(f\"# mini batch ({epoch}, {mb}) -- pg_loss:{pg_loss.item():.4f}, value_loss:{v_loss.item():.4f}, \" + \\\n",
        "                              f\"entropy:{entropy_loss.item():.4f}, approx_kl: {approx_kl.item():.4f}\",\n",
        "                                file=f)\n",
        "                        atn_list = mb_actions.cpu().numpy().tolist()\n",
        "                        for atns in atn_list:\n",
        "                            for atn in atns:\n",
        "                                print(f\"{atn}\", file=f)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.agent.parameters(), max_grad_norm)\n",
        "                self.optimizer.step()\n",
        "\n",
        "            # kl divergence ?\n",
        "            if target_kl is not None:\n",
        "                if approx_kl > target_kl:\n",
        "                    break\n",
        "\n",
        "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
        "        var_y = np.var(y_true)\n",
        "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
        "\n",
        "        # TIMING: performance metrics to evaluate cpu/gpu usage\n",
        "        train_time = time.time() - train_time\n",
        "        train_sps = int(self.batch_size / train_time)\n",
        "        self.update += 1\n",
        "\n",
        "        print(f\"\\tTrain={train_sps}\\n\")\n",
        "\n",
        "        allocated_torch = torch.cuda.memory_allocated(self.device) - allocated_torch\n",
        "        allocated_cpu = self.process.memory_info().rss - allocated_cpu\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                \"Allocated during training - Pytorch: %.2f GB, System: %.2f GB\"\n",
        "                % (allocated_torch / 1e9, allocated_cpu / 1e9)\n",
        "            )\n",
        "\n",
        "        if self.record_loss:\n",
        "            with open(self.loss_file, \"a\") as f:\n",
        "                print(f\"Epoch -- policy_loss:{pg_loss.item():.4f}, value_loss:{v_loss.item():.4f}, \",\n",
        "                      f\"entropy:{entropy_loss.item():.4f}, approx_kl:{approx_kl.item():.4f}\",\n",
        "                      f\"clipfrac:{np.mean(clipfracs):.4f}, explained_var:{explained_var:.4f}\",\n",
        "                      file=f)\n",
        "\n",
        "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
        "        if self.wandb_entity:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"performance/train_sps\": train_sps,\n",
        "                    \"performance/train_time\": train_time,\n",
        "                    \"charts/learning_rate\": self.optimizer.param_groups[0][\"lr\"],\n",
        "                    \"losses/value_loss\": v_loss.item(),\n",
        "                    \"losses/policy_loss\": pg_loss.item(),\n",
        "                    \"losses/entropy\": entropy_loss.item(),\n",
        "                    \"losses/old_approx_kl\": old_approx_kl.item(),\n",
        "                    \"losses/approx_kl\": approx_kl.item(),\n",
        "                    \"losses/clipfrac\": np.mean(clipfracs),\n",
        "                    \"losses/explained_variance\": explained_var,\n",
        "                    \"agent_steps\": self.global_step,\n",
        "                    \"global_step\": self.global_step,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        if self.update % self.checkpoint_interval == 1 or self.done_training():\n",
        "           self._save_checkpoint()\n",
        "\n",
        "    def done_training(self):\n",
        "        return self.update >= self.total_updates\n",
        "\n",
        "    def close(self):\n",
        "        for envs in self.buffers:\n",
        "            envs.close()\n",
        "\n",
        "        if self.wandb_entity:\n",
        "            wandb.finish()\n",
        "\n",
        "    def _save_checkpoint(self):\n",
        "        if self.data_dir is None:\n",
        "            return\n",
        "\n",
        "        policy_name = f\"{self.exp_name}.{self.update:06d}\"\n",
        "        state = {\n",
        "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
        "            \"global_step\": self.global_step,\n",
        "            \"agent_step\": self.agent_step,\n",
        "            \"update\": self.update,\n",
        "            \"learning_rate\": self.learning_rate,\n",
        "            \"policy_checkpoint_name\": policy_name,\n",
        "            \"wandb_run_id\": self.wandb_run_id,\n",
        "        }\n",
        "        path = os.path.join(self.data_dir, f\"trainer.pt\")\n",
        "        tmp_path = path + \".tmp\"\n",
        "        torch.save(state, tmp_path)\n",
        "        os.rename(tmp_path, path)\n",
        "\n",
        "        # NOTE: as the agent_creator has args internally, the policy args are not passed\n",
        "        self.policy_store.add_policy(policy_name, self.agent)\n",
        "\n",
        "        if self.policy_ranker:\n",
        "            self.policy_ranker.add_policy_copy(\n",
        "                policy_name, self.policy_pool._learner_name\n",
        "            )"
      ],
      "metadata": {
        "id": "0SxX_CulIXD3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "7f1e88d7-d8c1-4803-d5fa-48a36481fc71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-83a280065938>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wandb'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_aADpefQX7zB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B_ovPqXwHTCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Config"
      ],
      "metadata": {
        "id": "g6JccKeMID3y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCESVk9t6Er3"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "\n",
        "class Config:\n",
        "    # Run a smaller config on your local machine\n",
        "    local_mode = False  # Run in local mode\n",
        "    # Track to run - options: reinforcement_learning, curriculum_generation\n",
        "    track = \"rl\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    #record_loss = False  # log all minibatch loss and actions, for debugging\n",
        "\n",
        "    # Trainer Args\n",
        "    seed = 1\n",
        "    num_cores = None  # Number of cores to use for training\n",
        "    num_envs = 6  # Number of environments to use for training\n",
        "    num_buffers = 2  # Number of buffers to use for training\n",
        "    rollout_batch_size = 2**15 # Number of steps to rollout\n",
        "    eval_batch_size = 2**15 # Number of steps to rollout for eval\n",
        "    train_num_steps = 10_000_000  # Number of steps to train\n",
        "    eval_num_steps = 1_000_000  # Number of steps to evaluate\n",
        "    checkpoint_interval = 30  # Interval to save models\n",
        "    run_name = f\"nmmo_{time.strftime('%Y%m%d_%H%M%S')}\"  # Run name\n",
        "    runs_dir = \"/tmp/runs\"  # Directory for runs\n",
        "    policy_store_dir = None # Policy store directory\n",
        "    use_serial_vecenv = False  # Use serial vecenv implementation\n",
        "    learner_weight = 1.0  # Weight of learner policy\n",
        "    max_opponent_policies = 0  # Maximum number of opponent policies to train against\n",
        "    eval_num_policies = 2 # Number of policies to use for evaluation\n",
        "    eval_num_rounds = 1 # Number of rounds to use for evaluation\n",
        "    wandb_project = None  # WandB project name\n",
        "    wandb_entity = None  # WandB entity name\n",
        "\n",
        "    # PPO Args\n",
        "    bptt_horizon = 8  # Train on this number of steps of a rollout at a time. Used to reduce GPU memory.\n",
        "    ppo_training_batch_size = 128  # Number of rows in a training batch\n",
        "    ppo_update_epochs = 3  # Number of update epochs to use for training\n",
        "    ppo_learning_rate = 0.00015  # Learning rate\n",
        "    clip_coef = 0.1  # PPO clip coefficient\n",
        "\n",
        "    # Environment Args\n",
        "    num_agents = 128  # Number of agents to use for training\n",
        "    num_npcs = 256  # Number of NPCs to use for training\n",
        "    max_episode_length = 1024  # Number of steps per episode\n",
        "    death_fog_tick = None  # Number of ticks before death fog starts\n",
        "    num_maps = 128  # Number of maps to use for training\n",
        "    maps_path = \"maps/train/\"  # Path to maps to use for training\n",
        "    map_size = 128  # Size of maps to use for training\n",
        "    resilient_population = 0.2  # Percentage of agents to be resilient to starvation/dehydration\n",
        "    tasks_path = None  # Path to tasks to use for training\n",
        "    eval_mode = False # Run the postprocessor in the eval mode\n",
        "    early_stop_agent_num = 8  # Stop the episode when the number of agents reaches this number\n",
        "    sqrt_achievement_rewards=False # Use the log of achievement rewards\n",
        "    heal_bonus_weight = 0.03\n",
        "    meander_bonus_weight = 0.02\n",
        "    explore_bonus_weight = 0.01\n",
        "    spawn_immunity = 20\n",
        "\n",
        "    # Policy Args\n",
        "    input_size = 256\n",
        "    hidden_size = 256\n",
        "    num_lstm_layers = 0  # Number of LSTM layers to use\n",
        "    task_size = 4096  # Size of task embedding\n",
        "    encode_task = True  # Encode task\n",
        "    attend_task = \"none\"  # Attend task - options: none, pytorch, nikhil\n",
        "    attentional_decode = True  # Use attentional action decoder\n",
        "    extra_encoders = True  # Use inventory and market encoders\n",
        "\n",
        "    @classmethod\n",
        "    def asdict(cls):\n",
        "        return {attr: getattr(cls, attr) for attr in dir(cls)\n",
        "                if not callable(getattr(cls, attr)) and not attr.startswith(\"__\")}\n",
        "\n",
        "def create_config(config_cls):\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Get attribute names and their values from the static class\n",
        "    attrs = config_cls.asdict()\n",
        "\n",
        "    # Iterate over these attributes and set the default values of arguments to the corresponding attribute values\n",
        "    for attr, value in attrs.items():\n",
        "        # Convert underscores to hyphens to match the argparse argument format\n",
        "        arg_name = f'--{attr.replace(\"_\", \"-\")}'\n",
        "\n",
        "        parser.add_argument(\n",
        "            arg_name,\n",
        "            dest=attr,\n",
        "            type=type(value) if value is not None else str,\n",
        "            default=value,\n",
        "            help=f\"{arg_name} (default: {value})\"\n",
        "        )\n",
        "\n",
        "    return parser.parse_args()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Policy and encoding"
      ],
      "metadata": {
        "id": "BHnaNd3aIFhB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRgM8xNq6GDY"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from typing import Dict\n",
        "\n",
        "import pufferlib\n",
        "import pufferlib.emulation\n",
        "import pufferlib.models\n",
        "\n",
        "import nmmo\n",
        "from nmmo.entity.entity import EntityState\n",
        "\n",
        "EntityId = EntityState.State.attr_name_to_col[\"id\"]\n",
        "\n",
        "\n",
        "class Random(pufferlib.models.Policy):\n",
        "  '''A random policy that resets weights on every call'''\n",
        "  def __init__(self, envs):\n",
        "    super().__init__()\n",
        "    self.envs = envs\n",
        "    self.decoders = torch.nn.ModuleList(\n",
        "        [torch.nn.Linear(1, n) for n in envs.single_action_space.nvec]\n",
        "    )\n",
        "\n",
        "  def encode_observations(self, env_outputs):\n",
        "    return torch.randn((env_outputs.shape[0], 1)).to(env_outputs.device), None\n",
        "\n",
        "  def decode_actions(self, hidden, lookup):\n",
        "    torch.nn.init.xavier_uniform_(hidden)\n",
        "    actions = [dec(hidden) for dec in self.decoders]\n",
        "    return actions, None\n",
        "\n",
        "  def critic(self, hidden):\n",
        "    return torch.zeros((hidden.shape[0], 1)).to(hidden.device)\n",
        "\n",
        "\n",
        "class Baseline(pufferlib.models.Policy):\n",
        "  def __init__(self, env, input_size=256, hidden_size=256, task_size=4096):\n",
        "    super().__init__(env)\n",
        "\n",
        "    self.flat_observation_space = env.flat_observation_space\n",
        "    self.flat_observation_structure = env.flat_observation_structure\n",
        "\n",
        "    self.tile_encoder = TileEncoder(input_size)\n",
        "    self.player_encoder = PlayerEncoder(input_size, hidden_size)\n",
        "    self.item_encoder = ItemEncoder(input_size, hidden_size)\n",
        "    self.inventory_encoder = InventoryEncoder(input_size, hidden_size)\n",
        "    self.market_encoder = MarketEncoder(input_size, hidden_size)\n",
        "    self.task_encoder = TaskEncoder(input_size, hidden_size, task_size)\n",
        "    self.proj_fc = torch.nn.Linear(5 * input_size, input_size)\n",
        "    self.action_decoder = ActionDecoder(input_size, hidden_size)\n",
        "    self.value_head = torch.nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def encode_observations(self, flat_observations): #TODO: change this\n",
        "    env_outputs = pufferlib.emulation.unpack_batched_obs(flat_observations,\n",
        "        self.flat_observation_space, self.flat_observation_structure)\n",
        "    tile = self.tile_encoder(env_outputs[\"Tile\"])\n",
        "    player_embeddings, my_agent = self.player_encoder(\n",
        "        env_outputs[\"Entity\"], env_outputs[\"AgentId\"][:, 0]\n",
        "    )\n",
        "\n",
        "    item_embeddings = self.item_encoder(env_outputs[\"Inventory\"])\n",
        "    inventory = self.inventory_encoder(item_embeddings)\n",
        "\n",
        "    market_embeddings = self.item_encoder(env_outputs[\"Market\"])\n",
        "    market = self.market_encoder(market_embeddings)\n",
        "\n",
        "    task = self.task_encoder(env_outputs[\"Task\"])\n",
        "\n",
        "    obs = torch.cat([tile, my_agent, inventory, market, task], dim=-1)\n",
        "    obs = self.proj_fc(obs)\n",
        "\n",
        "    return obs, (\n",
        "        player_embeddings,\n",
        "        item_embeddings,\n",
        "        market_embeddings,\n",
        "        env_outputs[\"ActionTargets\"],\n",
        "    )\n",
        "\n",
        "  def decode_actions(self, hidden, lookup):\n",
        "    actions = self.action_decoder(hidden, lookup)\n",
        "    value = self.value_head(hidden)\n",
        "    return actions, value\n",
        "\n",
        "\n",
        "class TileEncoder(torch.nn.Module):\n",
        "  def __init__(self, input_size):\n",
        "    super().__init__()\n",
        "    self.tile_offset = torch.tensor([i * 256 for i in range(3)])\n",
        "    self.embedding = torch.nn.Embedding(3 * 256, 32)\n",
        "\n",
        "    self.tile_conv_1 = torch.nn.Conv2d(96, 32, 3)\n",
        "    self.tile_conv_2 = torch.nn.Conv2d(32, 8, 3)\n",
        "    self.tile_fc = torch.nn.Linear(8 * 11 * 11, input_size)\n",
        "\n",
        "  def forward(self, tile):\n",
        "    tile[:, :, :2] -= tile[:, 112:113, :2].clone()\n",
        "    tile[:, :, :2] += 7\n",
        "    tile = self.embedding(\n",
        "        tile.long().clip(0, 255) + self.tile_offset.to(tile.device)\n",
        "    )\n",
        "\n",
        "    agents, tiles, features, embed = tile.shape\n",
        "    tile = (\n",
        "        tile.view(agents, tiles, features * embed)\n",
        "        .transpose(1, 2)\n",
        "        .view(agents, features * embed, 15, 15)\n",
        "    )\n",
        "\n",
        "    tile = F.relu(self.tile_conv_1(tile))\n",
        "    tile = F.relu(self.tile_conv_2(tile))\n",
        "    tile = tile.contiguous().view(agents, -1)\n",
        "    tile = F.relu(self.tile_fc(tile))\n",
        "\n",
        "    return tile\n",
        "\n",
        "\n",
        "class PlayerEncoder(torch.nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.entity_dim = 31\n",
        "    self.player_offset = torch.tensor([i * 256 for i in range(self.entity_dim)])\n",
        "    self.embedding = torch.nn.Embedding(self.entity_dim * 256, 32)\n",
        "\n",
        "    self.agent_fc = torch.nn.Linear(self.entity_dim * 32, hidden_size)\n",
        "    self.my_agent_fc = torch.nn.Linear(self.entity_dim * 32, input_size)\n",
        "\n",
        "  def forward(self, agents, my_id):\n",
        "    # Pull out rows corresponding to the agent\n",
        "    agent_ids = agents[:, :, EntityId]\n",
        "    mask = (agent_ids == my_id.unsqueeze(1)) & (agent_ids != 0)\n",
        "    mask = mask.int()\n",
        "    row_indices = torch.where(\n",
        "        mask.any(dim=1), mask.argmax(dim=1), torch.zeros_like(mask.sum(dim=1))\n",
        "    )\n",
        "\n",
        "    agent_embeddings = self.embedding(\n",
        "        agents.long().clip(0, 255) + self.player_offset.to(agents.device)\n",
        "    )\n",
        "    batch, agent, attrs, embed = agent_embeddings.shape\n",
        "\n",
        "    # Embed each feature separately\n",
        "    agent_embeddings = agent_embeddings.view(batch, agent, attrs * embed)\n",
        "    my_agent_embeddings = agent_embeddings[\n",
        "        torch.arange(agents.shape[0]), row_indices\n",
        "    ]\n",
        "\n",
        "    # Project to input of recurrent size\n",
        "    agent_embeddings = self.agent_fc(agent_embeddings)\n",
        "    my_agent_embeddings = self.my_agent_fc(my_agent_embeddings)\n",
        "    my_agent_embeddings = F.relu(my_agent_embeddings)\n",
        "\n",
        "    return agent_embeddings, my_agent_embeddings\n",
        "\n",
        "\n",
        "class ItemEncoder(torch.nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.item_offset = torch.tensor([i * 256 for i in range(16)])\n",
        "    self.embedding = torch.nn.Embedding(256, 32)\n",
        "\n",
        "    self.fc = torch.nn.Linear(2 * 32 + 12, hidden_size)\n",
        "\n",
        "    self.discrete_idxs = [1, 14]\n",
        "    self.discrete_offset = torch.Tensor([2, 0])\n",
        "    self.continuous_idxs = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15]\n",
        "    self.continuous_scale = torch.Tensor(\n",
        "        [\n",
        "            1 / 10,\n",
        "            1 / 10,\n",
        "            1 / 10,\n",
        "            1 / 100,\n",
        "            1 / 100,\n",
        "            1 / 100,\n",
        "            1 / 40,\n",
        "            1 / 40,\n",
        "            1 / 40,\n",
        "            1 / 100,\n",
        "            1 / 100,\n",
        "            1 / 100,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  def forward(self, items):\n",
        "    if self.discrete_offset.device != items.device:\n",
        "      self.discrete_offset = self.discrete_offset.to(items.device)\n",
        "      self.continuous_scale = self.continuous_scale.to(items.device)\n",
        "\n",
        "    # Embed each feature separately\n",
        "    discrete = items[:, :, self.discrete_idxs] + self.discrete_offset\n",
        "    discrete = self.embedding(discrete.long().clip(0, 255))\n",
        "    batch, item, attrs, embed = discrete.shape\n",
        "    discrete = discrete.view(batch, item, attrs * embed)\n",
        "\n",
        "    continuous = items[:, :, self.continuous_idxs] / self.continuous_scale\n",
        "\n",
        "    item_embeddings = torch.cat([discrete, continuous], dim=-1)\n",
        "    item_embeddings = self.fc(item_embeddings)\n",
        "    return item_embeddings\n",
        "\n",
        "\n",
        "class InventoryEncoder(torch.nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.fc = torch.nn.Linear(12 * hidden_size, input_size)\n",
        "\n",
        "  def forward(self, inventory):\n",
        "    agents, items, hidden = inventory.shape\n",
        "    inventory = inventory.view(agents, items * hidden)\n",
        "    return self.fc(inventory)\n",
        "\n",
        "\n",
        "class MarketEncoder(torch.nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.fc = torch.nn.Linear(hidden_size, input_size)\n",
        "\n",
        "  def forward(self, market):\n",
        "    return self.fc(market).mean(-2)\n",
        "\n",
        "\n",
        "class TaskEncoder(torch.nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, task_size):\n",
        "    super().__init__()\n",
        "    self.fc = torch.nn.Linear(task_size, input_size)\n",
        "\n",
        "  def forward(self, task):\n",
        "    return self.fc(task.clone())\n",
        "\n",
        "\n",
        "class ActionDecoder(torch.nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.layers = torch.nn.ModuleDict(\n",
        "        {\n",
        "            \"attack_style\": torch.nn.Linear(hidden_size, 3),\n",
        "            \"attack_target\": torch.nn.Linear(hidden_size, hidden_size),\n",
        "            \"market_buy\": torch.nn.Linear(hidden_size, hidden_size),\n",
        "            \"inventory_destroy\": torch.nn.Linear(hidden_size, hidden_size),\n",
        "            \"inventory_give_item\": torch.nn.Linear(hidden_size, hidden_size),\n",
        "            \"inventory_give_player\": torch.nn.Linear(hidden_size, hidden_size),\n",
        "            \"gold_quantity\": torch.nn.Linear(hidden_size, 99),\n",
        "            \"gold_target\": torch.nn.Linear(hidden_size, hidden_size),\n",
        "            \"move\": torch.nn.Linear(hidden_size, 5),\n",
        "            \"inventory_sell\": torch.nn.Linear(hidden_size, hidden_size),\n",
        "            \"inventory_price\": torch.nn.Linear(hidden_size, 99),\n",
        "            \"inventory_use\": torch.nn.Linear(hidden_size, hidden_size),\n",
        "        }\n",
        "    )\n",
        "\n",
        "  def apply_layer(self, layer, embeddings, mask, hidden):\n",
        "    hidden = layer(hidden)\n",
        "    if hidden.dim() == 2 and embeddings is not None:\n",
        "      hidden = torch.matmul(embeddings, hidden.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    if mask is not None:\n",
        "      hidden = hidden.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    return hidden\n",
        "\n",
        "  def forward(self, hidden, lookup):\n",
        "    (\n",
        "        player_embeddings,\n",
        "        inventory_embeddings,\n",
        "        market_embeddings,\n",
        "        action_targets,\n",
        "    ) = lookup\n",
        "\n",
        "    embeddings = {\n",
        "        \"attack_target\": player_embeddings,\n",
        "        \"market_buy\": market_embeddings,\n",
        "        \"inventory_destroy\": inventory_embeddings,\n",
        "        \"inventory_give_item\": inventory_embeddings,\n",
        "        \"inventory_give_player\": player_embeddings,\n",
        "        \"gold_target\": player_embeddings,\n",
        "        \"inventory_sell\": inventory_embeddings,\n",
        "        \"inventory_use\": inventory_embeddings,\n",
        "    }\n",
        "\n",
        "    action_targets = {\n",
        "        \"attack_style\": action_targets[\"Attack\"][\"Style\"],\n",
        "        \"attack_target\": action_targets[\"Attack\"][\"Target\"],\n",
        "        \"market_buy\": action_targets[\"Buy\"][\"MarketItem\"],\n",
        "        \"inventory_destroy\": action_targets[\"Destroy\"][\"InventoryItem\"],\n",
        "        \"inventory_give_item\": action_targets[\"Give\"][\"InventoryItem\"],\n",
        "        \"inventory_give_player\": action_targets[\"Give\"][\"Target\"],\n",
        "        \"gold_quantity\": action_targets[\"GiveGold\"][\"Price\"],\n",
        "        \"gold_target\": action_targets[\"GiveGold\"][\"Target\"],\n",
        "        \"move\": action_targets[\"Move\"][\"Direction\"],\n",
        "        \"inventory_sell\": action_targets[\"Sell\"][\"InventoryItem\"],\n",
        "        \"inventory_price\": action_targets[\"Sell\"][\"Price\"],\n",
        "        \"inventory_use\": action_targets[\"Use\"][\"InventoryItem\"],\n",
        "    }\n",
        "\n",
        "    actions = []\n",
        "    for key, layer in self.layers.items():\n",
        "      mask = None\n",
        "      mask = action_targets[key]\n",
        "      embs = embeddings.get(key)\n",
        "      if embs is not None and embs.shape[1] != mask.shape[1]:\n",
        "        b, _, f = embs.shape\n",
        "        zeros = torch.zeros([b, 1, f], dtype=embs.dtype, device=embs.device)\n",
        "        embs = torch.cat([embs, zeros], dim=1)\n",
        "\n",
        "      action = self.apply_layer(layer, embs, mask, hidden)\n",
        "      actions.append(action)\n",
        "\n",
        "    return actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vKEjpj0ADix"
      },
      "source": [
        "### Environment Files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### leader_board.py"
      ],
      "metadata": {
        "id": "u9nXh7DSItlu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWezig6PAInK"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, List\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import pufferlib\n",
        "import pufferlib.emulation\n",
        "\n",
        "from nmmo.core.realm import Realm\n",
        "from nmmo.lib.log import EventCode\n",
        "import nmmo.systems.item as Item\n",
        "\n",
        "@dataclass\n",
        "class TeamResult:\n",
        "    policy_id: str = None\n",
        "\n",
        "    # event-log based, coming from process_event_log\n",
        "    total_score: int = 0\n",
        "    agent_kill_count: int = 0,\n",
        "    npc_kill_count: int = 0,\n",
        "    max_combat_level: int = 0,\n",
        "    max_harvest_level: int = 0,\n",
        "    max_damage: int = 0,\n",
        "    max_progress_to_center: int = 0,\n",
        "    eat_food_count: int = 0,\n",
        "    drink_water_count: int = 0,\n",
        "    attack_count: int = 0,\n",
        "    item_harvest_count: int = 0,\n",
        "    item_list_count: int = 0,\n",
        "    item_buy_count: int = 0,\n",
        "\n",
        "    # agent object based (fill these in the environment)\n",
        "    # CHECK ME: perhaps create a stat wrapper for putting all stats in one place?\n",
        "    time_alive: int = 0,\n",
        "    earned_gold: int = 0,\n",
        "    completed_task_count: int = 0,\n",
        "    max_task_progress: float = 0,\n",
        "    damage_received: int = 0,\n",
        "    damage_inflicted: int = 0,\n",
        "    ration_consumed: int = 0,\n",
        "    potion_consumed: int = 0,\n",
        "    melee_level: int = 0,\n",
        "    range_level: int = 0,\n",
        "    mage_level: int = 0,\n",
        "    fishing_level: int = 0,\n",
        "    herbalism_level: int = 0,\n",
        "    prospecting_level: int = 0,\n",
        "    carving_level: int = 0,\n",
        "    alchemy_level: int = 0,\n",
        "\n",
        "    # system-level\n",
        "    n_timeout: Optional[int] = 0\n",
        "\n",
        "    @classmethod\n",
        "    def names(cls) -> List[str]:\n",
        "        return [\n",
        "            \"total_score\",\n",
        "            \"agent_kill_count\",\n",
        "            \"npc_kill_count\",\n",
        "            \"max_combat_level\",\n",
        "            \"max_harvest_level\",\n",
        "            \"max_damage\",\n",
        "            \"max_progress_to_center\",\n",
        "            \"eat_food_count\",\n",
        "            \"drink_water_count\",\n",
        "            \"attack_count\",\n",
        "            \"item_equip_count\",\n",
        "            \"item_harvest_count\",\n",
        "            \"item_list_count\",\n",
        "            \"item_buy_count\",\n",
        "            \"time_alive\",\n",
        "            \"earned_gold\",\n",
        "            \"completed_task_count\",\n",
        "            \"max_task_progress\",\n",
        "            \"damage_received\",\n",
        "            \"damage_inflicted\",\n",
        "            \"ration_consumed\",\n",
        "            \"potion_consumed\",\n",
        "            \"melee_level\",\n",
        "            \"range_level\",\n",
        "            \"mage_level\",\n",
        "            \"fishing_level\",\n",
        "            \"herbalism_level\",\n",
        "            \"prospecting_level\",\n",
        "            \"carving_level\",\n",
        "            \"alchemy_level\",\n",
        "        ]\n",
        "\n",
        "def get_episode_result(realm: Realm, agent_id):\n",
        "    achieved, performed, event_cnt = process_event_log(realm, [agent_id])\n",
        "    # NOTE: Not actually a \"team\" result. Just a \"team\" of one agent\n",
        "    result = TeamResult(\n",
        "        policy_id = str(agent_id),  # TODO: put actual team/policy name here\n",
        "        agent_kill_count = achieved[\"achieved/agent_kill_count\"],\n",
        "        npc_kill_count = achieved[\"achieved/npc_kill_count\"],\n",
        "        max_damage = achieved[\"achieved/max_damage\"],\n",
        "        max_progress_to_center = achieved[\"achieved/max_progress_to_center\"],\n",
        "        eat_food_count = event_cnt[\"event/eat_food\"],\n",
        "        drink_water_count = event_cnt[\"event/drink_water\"],\n",
        "        attack_count = event_cnt[\"event/score_hit\"],\n",
        "        item_harvest_count = event_cnt[\"event/harvest_item\"],\n",
        "        item_list_count = event_cnt[\"event/list_item\"],\n",
        "        item_buy_count = event_cnt[\"event/buy_item\"],\n",
        "    )\n",
        "\n",
        "    return result, achieved, performed, event_cnt\n",
        "\n",
        "\n",
        "class StatPostprocessor(pufferlib.emulation.Postprocessor):\n",
        "    \"\"\"Postprocessing actions and metrics of Neural MMO.\n",
        "       Process wandb/leader board stats, and save replays.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, agent_id, eval_mode=False):\n",
        "        super().__init__(env, is_multiagent=True, agent_id=agent_id)\n",
        "        self.eval_mode = eval_mode\n",
        "        self._reset_episode_stats()\n",
        "\n",
        "    def reset(self, observation):\n",
        "        self._reset_episode_stats()\n",
        "\n",
        "    def _reset_episode_stats(self):\n",
        "        self.epoch_return = 0\n",
        "        self.epoch_length = 0\n",
        "\n",
        "        self._cod_attacked = 0\n",
        "        self._cod_starved = 0\n",
        "        self._cod_dehydrated = 0\n",
        "        self._task_completed = 0\n",
        "        self._max_task_progress = 0\n",
        "        self._task_with_2_reward_signal = 0\n",
        "        self._task_with_0p2_max_progress = 0\n",
        "        self._curriculum = defaultdict(list)\n",
        "        self._combat_level = []\n",
        "        self._harvest_level = []\n",
        "        self._prev_unique_count = 0\n",
        "        self._curr_unique_count = 0\n",
        "\n",
        "        # for agent results\n",
        "        self._time_alive = 0\n",
        "        self._damage_received = 0\n",
        "        self._damage_inflicted = 0\n",
        "        self._ration_consumed = 0\n",
        "        self._potion_consumed = 0\n",
        "        self._melee_level = 0\n",
        "        self._range_level = 0\n",
        "        self._mage_level = 0\n",
        "        self._fishing_level = 0\n",
        "        self._herbalism_level = 0\n",
        "        self._prospecting_level = 0\n",
        "        self._carving_level = 0\n",
        "        self._alchemy_level = 0\n",
        "\n",
        "        # saving actions for masking/scoring\n",
        "        self._last_moves = []\n",
        "        self._last_price = 0\n",
        "\n",
        "    def _update_stats(self, agent):\n",
        "        task = self.env.agent_task_map[agent.ent_id][0]\n",
        "        # For each task spec, record whether its max progress and reward count\n",
        "        self._curriculum[task.spec_name].append((task._max_progress, task.reward_signal_count))\n",
        "        self._max_task_progress = task._max_progress\n",
        "        if task.reward_signal_count >= 2:\n",
        "            self._task_with_2_reward_signal = 1.0\n",
        "        if task._max_progress >= 0.2:\n",
        "            self._task_with_0p2_max_progress = 1.0\n",
        "        if task.completed:\n",
        "            self._task_completed = 1.0\n",
        "\n",
        "        if agent.damage.val > 0:\n",
        "            self._cod_attacked = 1.0\n",
        "        elif agent.food.val == 0:\n",
        "            self._cod_starved = 1.0\n",
        "        elif agent.water.val == 0:\n",
        "            self._cod_dehydrated = 1.0\n",
        "\n",
        "        self._combat_level.append(agent.attack_level)\n",
        "        self._harvest_level.append(max(\n",
        "            agent.fishing_level.val,\n",
        "            agent.herbalism_level.val,\n",
        "            agent.prospecting_level.val,\n",
        "            agent.carving_level.val,\n",
        "            agent.alchemy_level.val,\n",
        "        ))\n",
        "\n",
        "        # For TeamResult\n",
        "        self._time_alive += agent.history.time_alive.val\n",
        "        self._damage_received += agent.history.damage_received\n",
        "        self._damage_inflicted += agent.history.damage_inflicted\n",
        "        self._ration_consumed += agent.ration_consumed\n",
        "        self._potion_consumed += agent.poultice_consumed\n",
        "        self._melee_level += agent.melee_level.val\n",
        "        self._range_level += agent.range_level.val\n",
        "        self._mage_level += agent.mage_level.val\n",
        "        self._fishing_level += agent.fishing_level.val\n",
        "        self._herbalism_level += agent.herbalism_level.val\n",
        "        self._prospecting_level += agent.prospecting_level.val\n",
        "        self._carving_level += agent.carving_level.val\n",
        "        self._alchemy_level += agent.alchemy_level.val\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # Mask out the last selected price\n",
        "        observation[\"ActionTargets\"][\"Sell\"][\"Price\"][self._last_price] = 0\n",
        "        return observation\n",
        "\n",
        "    def action(self, action):\n",
        "        self._last_moves.append(action[8])  # 8 is the index for move direction\n",
        "        self._last_price = action[10]  # 10 is the index for selling price\n",
        "        return action\n",
        "\n",
        "    def reward_done_info(self, reward, done, info):\n",
        "        \"\"\"Update stats + info and save replays.\"\"\"\n",
        "\n",
        "        # Remove the task from info. Curriculum info is processed in _update_stats()\n",
        "        info.pop('task', None)\n",
        "\n",
        "        # Count and store unique event counts for easier use\n",
        "        log = self.env.realm.event_log.get_data(agents=[self.agent_id])\n",
        "        self._prev_unique_count = self._curr_unique_count\n",
        "        self._curr_unique_count = len(extract_unique_event(log, self.env.realm.event_log.attr_to_col))\n",
        "\n",
        "        if not done:\n",
        "            self.epoch_length += 1\n",
        "            self.epoch_return += reward\n",
        "            return reward, done, info\n",
        "\n",
        "        if 'stats' not in info:\n",
        "            info['stats'] = {}\n",
        "\n",
        "        agent = self.env.realm.players.dead_this_tick.get(\n",
        "            self.agent_id, self.env.realm.players.get(self.agent_id)\n",
        "        )\n",
        "        assert agent is not None\n",
        "        self._update_stats(agent)\n",
        "\n",
        "        info['return'] = self.epoch_return\n",
        "        info['length'] = self.epoch_length\n",
        "\n",
        "        info[\"stats\"][\"cod/attacked\"] = self._cod_attacked\n",
        "        info[\"stats\"][\"cod/starved\"] = self._cod_starved\n",
        "        info[\"stats\"][\"cod/dehydrated\"] = self._cod_dehydrated\n",
        "        info[\"stats\"][\"task/completed\"] = self._task_completed\n",
        "        info[\"stats\"][\"task/pcnt_2_reward_signal\"] = self._task_with_2_reward_signal\n",
        "        info[\"stats\"][\"task/pcnt_0p2_max_progress\"] = self._task_with_0p2_max_progress\n",
        "        info[\"stats\"][\"achieved/max_combat_level\"] = max(self._combat_level)\n",
        "        info[\"stats\"][\"achieved/max_harvest_level\"] = max(self._harvest_level)\n",
        "        info[\"stats\"][\"achieved/team_time_alive\"] = self._time_alive\n",
        "        info[\"stats\"][\"achieved/unique_events\"] = self._curr_unique_count\n",
        "        info[\"curriculum\"] = self._curriculum\n",
        "\n",
        "        result, achieved, performed, _ = get_episode_result(self.env.realm, self.agent_id)\n",
        "        for key, val in list(achieved.items()) + list(performed.items()):\n",
        "            info[\"stats\"][key] = float(val)\n",
        "\n",
        "        # Fill in the \"TeamResult\"\n",
        "        result.max_task_progress = self._max_task_progress\n",
        "        result.total_score = self._curr_unique_count\n",
        "        result.time_alive = self._time_alive\n",
        "        result.earned_gold = achieved[\"achieved/earned_gold\"]\n",
        "        result.completed_task_count = self._task_completed\n",
        "        result.damage_received = self._damage_received\n",
        "        result.damage_inflicted = self._damage_inflicted\n",
        "        result.ration_consumed = self._ration_consumed\n",
        "        result.potion_consumed = self._potion_consumed\n",
        "        result.melee_level = self._melee_level\n",
        "        result.range_level = self._range_level\n",
        "        result.mage_level = self._mage_level\n",
        "        result.fishing_level = self._fishing_level\n",
        "        result.herbalism_level = self._herbalism_level\n",
        "        result.prospecting_level = self._prospecting_level\n",
        "        result.carving_level = self._carving_level\n",
        "        result.alchemy_level = self._alchemy_level\n",
        "\n",
        "        info[\"team_results\"] = (self.agent_id, result)\n",
        "\n",
        "        if self.eval_mode:\n",
        "            # \"return\" is used for ranking in the eval mode, so put the task progress here\n",
        "            info[\"return\"] = self._max_task_progress  # this is 1 if done\n",
        "\n",
        "        return reward, done, info\n",
        "\n",
        "# Event processing utilities for Neural MMO.\n",
        "\n",
        "INFO_KEY_TO_EVENT_CODE = {\n",
        "    \"event/\" + evt.lower(): val\n",
        "    for evt, val in EventCode.__dict__.items()\n",
        "    if isinstance(val, int)\n",
        "}\n",
        "\n",
        "# convert the numbers into binary (performed or not) for the key events\n",
        "KEY_EVENT = [\n",
        "    \"eat_food\",\n",
        "    \"drink_water\",\n",
        "    \"score_hit\",\n",
        "    \"player_kill\",\n",
        "    \"consume_item\",\n",
        "    \"harvest_item\",\n",
        "    \"list_item\",\n",
        "    \"buy_item\",\n",
        "]\n",
        "\n",
        "ITEM_TYPE = {\n",
        "    \"armor\": [item.ITEM_TYPE_ID for item in [Item.Hat, Item.Top, Item.Bottom]],\n",
        "    \"weapon\": [item.ITEM_TYPE_ID for item in [Item.Spear, Item.Bow, Item.Wand]],\n",
        "    \"tool\": [item.ITEM_TYPE_ID for item in \\\n",
        "             [Item.Axe, Item.Gloves, Item.Rod, Item.Pickaxe, Item.Chisel]],\n",
        "    \"ammo\": [item.ITEM_TYPE_ID for item in [Item.Runes, Item.Arrow, Item.Whetstone]],\n",
        "    \"consumable\": [item.ITEM_TYPE_ID for item in [Item.Potion, Item.Ration]],\n",
        "}\n",
        "\n",
        "def process_event_log(realm, agent_list):\n",
        "    \"\"\"Process the event log and extract performed actions and achievements.\"\"\"\n",
        "    log = realm.event_log.get_data(agents=agent_list)\n",
        "    attr_to_col = realm.event_log.attr_to_col\n",
        "\n",
        "    # count the number of events\n",
        "    event_cnt = {}\n",
        "    for key, code in INFO_KEY_TO_EVENT_CODE.items():\n",
        "        # count the freq of each event\n",
        "        event_cnt[key] = int(sum(log[:, attr_to_col[\"event\"]] == code))\n",
        "\n",
        "    # record true or false for each event\n",
        "    performed = {}\n",
        "    for evt in KEY_EVENT:\n",
        "        key = \"event/\" + evt\n",
        "        performed[key] = event_cnt[key] > 0\n",
        "\n",
        "    # check if tools, weapons, ammos, ammos were equipped\n",
        "    for item_type, item_ids in ITEM_TYPE.items():\n",
        "        if item_type == \"consumable\":\n",
        "            continue\n",
        "        key = \"event/equip_\" + item_type\n",
        "        idx = (log[:, attr_to_col[\"event\"]] == EventCode.EQUIP_ITEM) & \\\n",
        "              np.in1d(log[:, attr_to_col[\"item_type\"]], item_ids)\n",
        "        performed[key] = sum(idx) > 0\n",
        "\n",
        "    # check if weapon was harvested\n",
        "    key = \"event/harvest_weapon\"\n",
        "    idx = (log[:, attr_to_col[\"event\"]] == EventCode.HARVEST_ITEM) & \\\n",
        "          np.in1d(log[:, attr_to_col[\"item_type\"]], ITEM_TYPE[\"weapon\"])\n",
        "    performed[key] = sum(idx) > 0\n",
        "\n",
        "    # record important achievements\n",
        "    achieved = {}\n",
        "\n",
        "    # get progress to center\n",
        "    idx = log[:, attr_to_col[\"event\"]] == EventCode.GO_FARTHEST\n",
        "    achieved[\"achieved/max_progress_to_center\"] = \\\n",
        "        int(max(log[idx, attr_to_col[\"distance\"]])) if sum(idx) > 0 else 0\n",
        "\n",
        "    # get earned gold\n",
        "    idx = log[:, attr_to_col[\"event\"]] == EventCode.EARN_GOLD\n",
        "    achieved[\"achieved/earned_gold\"] = int(sum(log[idx, attr_to_col[\"gold\"]]))\n",
        "\n",
        "    # get max damage\n",
        "    idx = log[:, attr_to_col[\"event\"]] == EventCode.SCORE_HIT\n",
        "    achieved[\"achieved/max_damage\"] = int(max(log[idx, attr_to_col[\"damage\"]])) if sum(idx) > 0 else 0\n",
        "\n",
        "    # get max possessed item levels: from harvesting, looting, buying\n",
        "    idx = np.in1d(log[:, attr_to_col[\"event\"]],\n",
        "                  [EventCode.HARVEST_ITEM, EventCode.LOOT_ITEM, EventCode.BUY_ITEM])\n",
        "    if sum(idx) > 0:\n",
        "      for item_type, item_ids in ITEM_TYPE.items():\n",
        "          idx_item = np.in1d(log[idx, attr_to_col[\"item_type\"]], item_ids)\n",
        "          achieved[\"achieved/max_\" + item_type + \"_level\"] = \\\n",
        "            int(max(log[idx][idx_item, attr_to_col[\"level\"]])) if sum(idx_item) > 0 else 1  # min level = 1\n",
        "\n",
        "    # other notable achievements\n",
        "    idx = (log[:, attr_to_col[\"event\"]] == EventCode.PLAYER_KILL)\n",
        "    achieved[\"achieved/agent_kill_count\"] = int(sum(idx & (log[:, attr_to_col[\"target_ent\"]] > 0)))\n",
        "    achieved[\"achieved/npc_kill_count\"] = int(sum(idx & (log[:, attr_to_col[\"target_ent\"]] < 0)))\n",
        "\n",
        "    return achieved, performed, event_cnt\n",
        "\n",
        "def extract_unique_event(log, attr_to_col):\n",
        "    if len(log) == 0:  # no event logs\n",
        "        return set()\n",
        "\n",
        "    # mask some columns to make the event redundant\n",
        "    cols_to_ignore = {\n",
        "        EventCode.GO_FARTHEST: [\"distance\"],\n",
        "        EventCode.SCORE_HIT: [\"damage\"],\n",
        "        # treat each (item, level) differently\n",
        "        EventCode.CONSUME_ITEM: [\"quantity\"],\n",
        "        # but, count each (item, level) only once\n",
        "        EventCode.HARVEST_ITEM: [\"quantity\"],\n",
        "        EventCode.EQUIP_ITEM: [\"quantity\"],\n",
        "        EventCode.LOOT_ITEM: [\"quantity\"],\n",
        "        EventCode.LIST_ITEM: [\"quantity\", \"price\"],\n",
        "        EventCode.BUY_ITEM: [\"quantity\", \"price\"],\n",
        "    }\n",
        "\n",
        "    for code, attrs in cols_to_ignore.items():\n",
        "        idx = log[:, attr_to_col[\"event\"]] == code\n",
        "        for attr in attrs:\n",
        "            log[idx, attr_to_col[attr]] = 0\n",
        "\n",
        "    # make every EARN_GOLD events unique, from looting and selling\n",
        "    idx = log[:, attr_to_col[\"event\"]] == EventCode.EARN_GOLD\n",
        "    log[idx, attr_to_col[\"number\"]] = log[\n",
        "        idx, attr_to_col[\"tick\"]\n",
        "    ].copy()  # this is a hack\n",
        "\n",
        "    # return unique events after masking\n",
        "    return set(tuple(row) for row in log[:, attr_to_col[\"event\"]:])\n",
        "\n",
        "def calculate_entropy(sequence):\n",
        "    frequencies = Counter(sequence)\n",
        "    total_elements = len(sequence)\n",
        "    entropy = 0\n",
        "    for freq in frequencies.values():\n",
        "        probability = freq / total_elements\n",
        "        entropy -= probability * math.log2(probability)\n",
        "    return entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### environment.py"
      ],
      "metadata": {
        "id": "KECzdqelJK-k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3nsYTOx6G4U"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "import math\n",
        "\n",
        "import nmmo\n",
        "import pufferlib\n",
        "import pufferlib.emulation\n",
        "\n",
        "# from leader_board import StatPostprocessor, calculate_entropy # TODO: IN SUBMISSION ADD THIS BACK\n",
        "\n",
        "class Config(nmmo.config.Default):\n",
        "    \"\"\"Configuration for Neural MMO.\"\"\"\n",
        "\n",
        "    def __init__(self, args: Namespace):\n",
        "        super().__init__()\n",
        "\n",
        "        self.PROVIDE_ACTION_TARGETS = True\n",
        "        self.PROVIDE_NOOP_ACTION_TARGET = True\n",
        "        self.MAP_FORCE_GENERATION = False\n",
        "        self.PLAYER_N = args.num_agents\n",
        "        self.HORIZON = args.max_episode_length\n",
        "        self.MAP_N = args.num_maps\n",
        "        self.PLAYER_DEATH_FOG = args.death_fog_tick\n",
        "        self.PATH_MAPS = f\"{args.maps_path}/{args.map_size}/\"\n",
        "        self.MAP_CENTER = args.map_size\n",
        "        self.NPC_N = args.num_npcs\n",
        "        self.CURRICULUM_FILE_PATH = args.tasks_path\n",
        "        self.TASK_EMBED_DIM = args.task_size\n",
        "        self.RESOURCE_RESILIENT_POPULATION = args.resilient_population\n",
        "\n",
        "        self.COMMUNICATION_SYSTEM_ENABLED = False\n",
        "\n",
        "        self.COMBAT_SPAWN_IMMUNITY = args.spawn_immunity\n",
        "\n",
        "class Postprocessor(StatPostprocessor):\n",
        "    def __init__(self, env, is_multiagent, agent_id,\n",
        "        eval_mode=False,\n",
        "        early_stop_agent_num=0,\n",
        "        sqrt_achievement_rewards=False,\n",
        "        heal_bonus_weight=0,\n",
        "        meander_bonus_weight=0,\n",
        "        explore_bonus_weight=0,\n",
        "        clip_unique_event=3,\n",
        "    ):\n",
        "        super().__init__(env, agent_id, eval_mode)\n",
        "        self.early_stop_agent_num = early_stop_agent_num\n",
        "        self.sqrt_achievement_rewards = sqrt_achievement_rewards\n",
        "        self.heal_bonus_weight = heal_bonus_weight\n",
        "        self.meander_bonus_weight = meander_bonus_weight\n",
        "        self.explore_bonus_weight = explore_bonus_weight\n",
        "        self.clip_unique_event = clip_unique_event\n",
        "\n",
        "    def reset(self, obs):\n",
        "        '''Called at the start of each episode'''\n",
        "        super().reset(obs)\n",
        "\n",
        "    @property\n",
        "    def observation_space(self):\n",
        "        '''If you modify the shape of features, you need to specify the new obs space'''\n",
        "        return super().observation_space\n",
        "\n",
        "    \"\"\"\n",
        "    def observation(self, obs):\n",
        "        '''Called before observations are returned from the environment\n",
        "\n",
        "        Use this to define custom featurizers. Changing the space itself requires you to\n",
        "        define the observation space again (i.e. Gym.spaces.Dict(gym.spaces....))\n",
        "        '''\n",
        "        return obs\n",
        "\n",
        "    def action(self, action):\n",
        "        '''Called before actions are passed from the model to the environment'''\n",
        "        return action\n",
        "    \"\"\"\n",
        "\n",
        "    def reward_done_info(self, reward, done, info):\n",
        "        '''Called on reward, done, and info before they are returned from the environment'''\n",
        "\n",
        "        # Stop early if there are too few agents generating the training data\n",
        "        if len(self.env.agents) <= self.early_stop_agent_num:\n",
        "            done = True\n",
        "\n",
        "        reward, done, info = super().reward_done_info(reward, done, info)\n",
        "\n",
        "        # Default reward shaper sums team rewards.\n",
        "        # Add custom reward shaping here.\n",
        "\n",
        "        # Add \"Healing\" score based on health increase and decrease due to food and water\n",
        "        healing_bonus = 0\n",
        "        if self.agent_id in self.env.realm.players:\n",
        "            if self.env.realm.players[self.agent_id].resources.health_restore > 0:\n",
        "                healing_bonus = self.heal_bonus_weight\n",
        "\n",
        "        # Add meandering bonus to encourage moving to various directions\n",
        "        meander_bonus = 0\n",
        "        if len(self._last_moves) > 5:\n",
        "          move_entropy = calculate_entropy(self._last_moves[-8:])  # of last 8 moves\n",
        "          meander_bonus = self.meander_bonus_weight * (move_entropy - 1)\n",
        "\n",
        "        # Unique event-based rewards, similar to exploration bonus\n",
        "        # The number of unique events are available in self._curr_unique_count, self._prev_unique_count\n",
        "        if self.sqrt_achievement_rewards:\n",
        "            explore_bonus = math.sqrt(self._curr_unique_count) - math.sqrt(self._prev_unique_count)\n",
        "        else:\n",
        "            explore_bonus = min(self.clip_unique_event,\n",
        "                                self._curr_unique_count - self._prev_unique_count)\n",
        "        explore_bonus *= self.explore_bonus_weight\n",
        "\n",
        "        reward = reward + explore_bonus + healing_bonus + meander_bonus\n",
        "\n",
        "        return reward, done, info\n",
        "\n",
        "def calculate_entropy(sequence):\n",
        "    frequencies = Counter(sequence)\n",
        "    total_elements = len(sequence)\n",
        "    entropy = 0\n",
        "    for freq in frequencies.values():\n",
        "        probability = freq / total_elements\n",
        "        entropy -= probability * math.log2(probability)\n",
        "    return entropy\n",
        "\n",
        "\n",
        "def make_env_creator(args: Namespace):\n",
        "    # TODO: Max episode length\n",
        "    def env_creator():\n",
        "        \"\"\"Create an environment.\"\"\"\n",
        "        env = nmmo.Env(Config(args))\n",
        "        env = pufferlib.emulation.PettingZooPufferEnv(env,\n",
        "            postprocessor_cls=Postprocessor,\n",
        "            postprocessor_kwargs={\n",
        "                'eval_mode': args.eval_mode,\n",
        "                'early_stop_agent_num': args.early_stop_agent_num,\n",
        "                'sqrt_achievement_rewards': args.sqrt_achievement_rewards,\n",
        "                'heal_bonus_weight': args.heal_bonus_weight,\n",
        "                'meander_bonus_weight': args.meander_bonus_weight,\n",
        "                'explore_bonus_weight': args.explore_bonus_weight,\n",
        "            },\n",
        "        )\n",
        "        return env\n",
        "    return env_creator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### evaluate.py"
      ],
      "metadata": {
        "id": "cDF4s-EcJiBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install dill"
      ],
      "metadata": {
        "id": "12cZBYPVEQEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "vmYr__ub6ICc",
        "outputId": "9353f00e-ce2d-46eb-c5dd-e6202318bc6b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b29e69555050>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnmmo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_helper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileReplayHelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnmmo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_spec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_task_from_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nmmo'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# pylint: disable=bad-builtin, no-member, protected-access\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "from types import SimpleNamespace\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from dataclasses import asdict\n",
        "from itertools import cycle\n",
        "\n",
        "import dill\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from nmmo.render.replay_helper import FileReplayHelper\n",
        "from nmmo.task.task_spec import make_task_from_spec\n",
        "\n",
        "import pufferlib\n",
        "from pufferlib.vectorization import Serial, Multiprocessing\n",
        "from pufferlib.policy_store import DirectoryPolicyStore\n",
        "from pufferlib.frameworks import cleanrl\n",
        "import pufferlib.policy_ranker\n",
        "import pufferlib.utils\n",
        "\n",
        "# import environment # TODO: ADD THIS BACCK WHEN SUBMIT\n",
        "\n",
        "# from reinforcement_learning import config, clean_pufferl # TODO: ADD THIS BACCK WHEN SUBMIT\n",
        "\n",
        "def setup_policy_store(policy_store_dir):\n",
        "    # CHECK ME: can be custom models with different architectures loaded here?\n",
        "    if not os.path.exists(policy_store_dir):\n",
        "        raise ValueError(\"Policy store directory does not exist\")\n",
        "    if os.path.exists(os.path.join(policy_store_dir, \"trainer.pt\")):\n",
        "        raise ValueError(\"Policy store directory should not contain trainer.pt\")\n",
        "    logging.info(\"Using policy store from %s\", policy_store_dir)\n",
        "    policy_store = DirectoryPolicyStore(policy_store_dir)\n",
        "    return policy_store\n",
        "\n",
        "def save_replays(policy_store_dir, save_dir, curriculum_file, task_to_assign=None):\n",
        "    # load the checkpoints into the policy store\n",
        "    policy_store = setup_policy_store(policy_store_dir)\n",
        "    policy_ranker = create_policy_ranker(policy_store_dir)\n",
        "    num_policies = len(policy_store._all_policies())\n",
        "\n",
        "    # setup the replay path\n",
        "    save_dir = os.path.join(save_dir, policy_store_dir)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    logging.info(\"Replays will be saved to %s\", save_dir)\n",
        "\n",
        "    # Use 1 env and 1 buffer for replay generation\n",
        "    # TODO: task-condition agents when generating replays\n",
        "    args = SimpleNamespace(**config.Config.asdict())\n",
        "    args.num_envs = 1\n",
        "    args.num_buffers = 1\n",
        "    args.use_serial_vecenv = True\n",
        "    args.learner_weight = 0  # evaluate mode\n",
        "    args.selfplay_num_policies = num_policies + 1\n",
        "    args.early_stop_agent_num = 0  # run the full episode\n",
        "    args.resilient_population = 0  # no resilient agents\n",
        "    args.tasks_path = curriculum_file  # task-conditioning\n",
        "\n",
        "    # NOTE: This creates a dummy learner agent. Is it necessary?\n",
        "    from reinforcement_learning import policy  # import your policy\n",
        "    def make_policy(envs):\n",
        "        learner_policy = policy.Baseline(\n",
        "            envs.driver_env,\n",
        "            input_size=args.input_size,\n",
        "            hidden_size=args.hidden_size,\n",
        "            task_size=args.task_size\n",
        "        )\n",
        "        return cleanrl.Policy(learner_policy)\n",
        "\n",
        "    # Setup the evaluator. No training during evaluation\n",
        "    evaluator = clean_pufferl.CleanPuffeRL(\n",
        "        seed=args.seed,\n",
        "        env_creator=environment.make_env_creator(args),\n",
        "        env_creator_kwargs={},\n",
        "        agent_creator=make_policy,\n",
        "        vectorization=Serial,\n",
        "        num_envs=args.num_envs,\n",
        "        num_cores=args.num_envs,\n",
        "        num_buffers=args.num_buffers,\n",
        "        selfplay_learner_weight=args.learner_weight,\n",
        "        selfplay_num_policies=args.selfplay_num_policies,\n",
        "        policy_store=policy_store,\n",
        "        policy_ranker=policy_ranker, # so that a new ranker is created\n",
        "        data_dir=save_dir,\n",
        "    )\n",
        "\n",
        "    # Load the policies into the policy pool\n",
        "    evaluator.policy_pool.update_policies({\n",
        "        p.name: p.policy(\n",
        "            policy_args=[evaluator.buffers[0]],\n",
        "            device=evaluator.device\n",
        "        ) for p in list(policy_store._all_policies().values())\n",
        "    })\n",
        "\n",
        "    # Set up the replay helper\n",
        "    o, r, d, i = evaluator.buffers[0].recv()  # reset the env\n",
        "    replay_helper = FileReplayHelper()\n",
        "    nmmo_env = evaluator.buffers[0].envs[0].envs[0].env\n",
        "    nmmo_env.realm.record_replay(replay_helper)\n",
        "\n",
        "    if task_to_assign is not None:\n",
        "        with open(curriculum_file, 'rb') as f:\n",
        "            task_with_embedding = dill.load(f) # a list of TaskSpec\n",
        "        assert 0 <= task_to_assign < len(task_with_embedding), \"Task index out of range\"\n",
        "        select_task = task_with_embedding[task_to_assign]\n",
        "\n",
        "        # Assign the task to the env\n",
        "        tasks = make_task_from_spec(nmmo_env.possible_agents,\n",
        "                                    [select_task] * len(nmmo_env.possible_agents))\n",
        "        nmmo_env.tasks = tasks  # this is a hack\n",
        "        print(\"seed:\", args.seed,\n",
        "              \", task:\", nmmo_env.tasks[0].spec_name)\n",
        "\n",
        "    # Run an episode to generate the replay\n",
        "    replay_helper.reset()\n",
        "    while True:\n",
        "        with torch.no_grad():\n",
        "            actions, logprob, value, _ = evaluator.policy_pool.forwards(\n",
        "                torch.Tensor(o).to(evaluator.device),\n",
        "                None,  # dummy lstm state\n",
        "                torch.Tensor(d).to(evaluator.device),\n",
        "            )\n",
        "            value = value.flatten()\n",
        "        evaluator.buffers[0].send(actions.cpu().numpy(), None)\n",
        "        o, r, d, i = evaluator.buffers[0].recv()\n",
        "\n",
        "        num_alive = len(nmmo_env.realm.players)\n",
        "        task_done = sum(1 for task in nmmo_env.tasks if task.completed)\n",
        "        alive_done = sum(1 for task in nmmo_env.tasks\n",
        "                         if task.completed and task.assignee[0] in nmmo_env.realm.players)\n",
        "        print(\"Tick:\", nmmo_env.realm.tick, \", alive agents:\", num_alive, \", task done:\", task_done)\n",
        "        if num_alive == alive_done:\n",
        "            print(\"All alive agents completed the task.\")\n",
        "            break\n",
        "        if num_alive == 0 or nmmo_env.realm.tick == args.max_episode_length:\n",
        "            print(\"All agents died or reached the max episode length.\")\n",
        "            break\n",
        "\n",
        "    # Count how many agents completed the task\n",
        "    print(\"--------------------------------------------------\")\n",
        "    print(\"Task:\", nmmo_env.tasks[0].spec_name)\n",
        "    num_completed = sum(1 for task in nmmo_env.tasks if task.completed)\n",
        "    print(\"Number of agents completed the task:\", num_completed)\n",
        "    avg_progress = np.mean([task.progress_info[\"max_progress\"] for task in nmmo_env.tasks])\n",
        "    print(f\"Average maximum progress (max=1): {avg_progress:.3f}\")\n",
        "    avg_completed_tick = np.mean([task.progress_info[\"completed_tick\"]\n",
        "                                  for task in nmmo_env.tasks if task.completed])\n",
        "    print(f\"Average completed tick: {avg_completed_tick:.1f}\")\n",
        "\n",
        "    # Save the replay file\n",
        "    replay_file = os.path.join(save_dir, f\"replay_{time.strftime('%Y%m%d_%H%M%S')}\")\n",
        "    logging.info(\"Saving replay to %s\", replay_file)\n",
        "    replay_helper.save(replay_file, compress=False)\n",
        "    evaluator.close()\n",
        "\n",
        "def create_policy_ranker(policy_store_dir, ranker_file=\"ranker.pickle\", db_file=\"ranking.sqlite\"):\n",
        "    file = os.path.join(policy_store_dir, ranker_file)\n",
        "    if os.path.exists(file):\n",
        "        logging.info(\"Using existing policy ranker from %s\", file)\n",
        "        policy_ranker = pufferlib.policy_ranker.OpenSkillRanker.load_from_file(file)\n",
        "    else:\n",
        "        logging.info(\"Creating a new policy ranker and db under %s\", policy_store_dir)\n",
        "        db_file = os.path.join(policy_store_dir, db_file)\n",
        "        policy_ranker = pufferlib.policy_ranker.OpenSkillRanker(db_file, \"anchor\")\n",
        "    return policy_ranker\n",
        "\n",
        "class AllPolicySelector(pufferlib.policy_ranker.PolicySelector):\n",
        "    def select_policies(self, policies):\n",
        "        # Return all policy names in the alpahebetical order\n",
        "        # Loops circularly if more policies are needed than available\n",
        "        loop = cycle([\n",
        "            policies[name] for name in sorted(policies.keys()\n",
        "        )])\n",
        "        return [next(loop) for _ in range(self._num)]\n",
        "\n",
        "def rank_policies(policy_store_dir, eval_curriculum_file, device):\n",
        "    # CHECK ME: can be custom models with different architectures loaded here?\n",
        "    policy_store = setup_policy_store(policy_store_dir)\n",
        "    policy_ranker = create_policy_ranker(policy_store_dir)\n",
        "    num_policies = len(policy_store._all_policies())\n",
        "    policy_selector = AllPolicySelector(num_policies)\n",
        "\n",
        "    args = SimpleNamespace(**config.Config.asdict())\n",
        "    args.data_dir = policy_store_dir\n",
        "    args.eval_mode = True\n",
        "    args.num_envs = 5  # sample a bit longer in each env\n",
        "    args.num_buffers = 1\n",
        "    args.learner_weight = 0  # evaluate mode\n",
        "    args.selfplay_num_policies = num_policies + 1\n",
        "    args.early_stop_agent_num = 0  # run the full episode\n",
        "    args.resilient_population = 0  # no resilient agents\n",
        "    args.tasks_path = eval_curriculum_file  # task-conditioning\n",
        "\n",
        "    # NOTE: This creates a dummy learner agent. Is it necessary?\n",
        "    from reinforcement_learning import policy  # import your policy\n",
        "    def make_policy(envs):\n",
        "        learner_policy = policy.Baseline(\n",
        "            envs.driver_env,\n",
        "            input_size=args.input_size,\n",
        "            hidden_size=args.hidden_size,\n",
        "            task_size=args.task_size\n",
        "        )\n",
        "        return cleanrl.Policy(learner_policy)\n",
        "\n",
        "    # Setup the evaluator. No training during evaluation\n",
        "    evaluator = clean_pufferl.CleanPuffeRL(\n",
        "        device=torch.device(device),\n",
        "        seed=args.seed,\n",
        "        env_creator=environment.make_env_creator(args),\n",
        "        env_creator_kwargs={},\n",
        "        agent_creator=make_policy,\n",
        "        data_dir=policy_store_dir,\n",
        "        vectorization=Multiprocessing,\n",
        "        num_envs=args.num_envs,\n",
        "        num_cores=args.num_envs,\n",
        "        num_buffers=args.num_buffers,\n",
        "        selfplay_learner_weight=args.learner_weight,\n",
        "        selfplay_num_policies=args.selfplay_num_policies,\n",
        "        batch_size=args.eval_batch_size,\n",
        "        policy_store=policy_store,\n",
        "        policy_ranker=policy_ranker, # so that a new ranker is created\n",
        "        policy_selector=policy_selector,\n",
        "    )\n",
        "\n",
        "    ranker_file = os.path.join(policy_store_dir, \"ranker.pickle\")\n",
        "    # This is for quick viewing of the ranks, not for the actual ranking\n",
        "    rank_txt = os.path.join(policy_store_dir, \"ranking.txt\")\n",
        "    with open(rank_txt, \"w\") as f:\n",
        "        pass\n",
        "\n",
        "    results = defaultdict(list)\n",
        "    while evaluator.global_step < args.eval_num_steps:\n",
        "        _, stats, infos = evaluator.evaluate()\n",
        "\n",
        "        for pol, vals in infos.items():\n",
        "            results[pol].extend([\n",
        "                e[1] for e in infos[pol]['team_results']\n",
        "            ])\n",
        "\n",
        "        ratings = evaluator.policy_ranker.ratings()\n",
        "        dataframe = pd.DataFrame(\n",
        "            {\n",
        "                (\"Rating\"): [ratings.get(n).get(\"mu\") for n in ratings],\n",
        "                (\"Policy\"): ratings.keys(),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        ratings = evaluator.policy_ranker.save_to_file(ranker_file)\n",
        "        with open(rank_txt, \"a\") as f:\n",
        "            f.write(\n",
        "                \"\\n\\n\"\n",
        "                + dataframe.round(2)\n",
        "                .sort_values(by=[\"Rating\"], ascending=False)\n",
        "                .to_string(index=False)\n",
        "                + \"\\n\\n\"\n",
        "            )\n",
        "\n",
        "    evaluator.close()\n",
        "    for pol, res in results.items():\n",
        "        aggregated = {}\n",
        "        keys = asdict(res[0]).keys()\n",
        "        for k in keys:\n",
        "            if k == 'policy_id':\n",
        "                continue\n",
        "            aggregated[k] = np.mean([asdict(e)[k] for e in res])\n",
        "        results[pol] = aggregated\n",
        "    print('Evaluation complete. Average stats:\\n', results)\n",
        "\n",
        "\n",
        "def start_it(policy_store_dir, replay_save_dir = \"replays\" , replay_mode=False, device = \"cude\",\n",
        "             task_file ='reinforcement_learning/eval_task_with_embedding.pkl',task_index =None ):\n",
        "    \"\"\"Usage: python evaluate.py -p <policy_store_dir> -s <replay_save_dir>\n",
        "\n",
        "    -p, --policy-store-dir: Directory to load policy checkpoints from for evaluation/ranking\n",
        "    -s, --replay-save-dir: Directory to save replays (Default: replays/)\n",
        "    -r, --replay-mode: Replay save mode (Default: False)\n",
        "    -d, --device: Device to use for evaluation/ranking (Default: cuda if available, otherwise cpu)\n",
        "    -t, --task-file: Task file to use for evaluation (Default: reinforcement_learning/eval_task_with_embedding.pkl)\n",
        "    -i, --task-index: The index of the task to assign in the curriculum file (Default: None)\n",
        "\n",
        "    To generate replay from your checkpoints, put them together in policy_store_dir, run the following command,\n",
        "    and replays will be saved under the replays/. The script will only use 1 environment.\n",
        "    $ python evaluate.py -p <policy_store_dir>\n",
        "\n",
        "    To rank your checkpoints, set the eval-mode to true, and the rankings will be printed out.\n",
        "    The replay files will NOT be generated in the eval mode.:\n",
        "    $ python evaluate.py -p <policy_store_dir> -e true\n",
        "\n",
        "    TODO: Pass in the task embedding?\n",
        "    \"\"\"\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    # Process the evaluate.py command line arguments\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument(\n",
        "    #     \"-p\",\n",
        "    #     \"--policy-store-dir\",\n",
        "    #     dest=\"policy_store_dir\",\n",
        "    #     type=str,\n",
        "    #     default=None,\n",
        "    #     help=\"Directory to load policy checkpoints from\",\n",
        "    # )\n",
        "\n",
        "    # parser.add_argument(\n",
        "    #     \"-s\",\n",
        "    #     \"--replay-save-dir\",\n",
        "    #     dest=\"replay_save_dir\",\n",
        "    #     type=str,\n",
        "    #     default=\"replays\",\n",
        "    #     help=\"Directory to save replays (Default: replays/)\",\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"-r\",\n",
        "    #     \"--replay-mode\",\n",
        "    #     dest=\"replay_mode\",\n",
        "    #     action=\"store_true\",\n",
        "    #     help=\"Replay mode (Default: False)\",\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"-d\",\n",
        "    #     \"--device\",\n",
        "    #     dest=\"device\",\n",
        "    #     type=str,\n",
        "    #     default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    #     help=\"Device to use for evaluation/ranking (Default: cuda if available, otherwise cpu)\",\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"-t\",\n",
        "    #     \"--task-file\",\n",
        "    #     dest=\"task_file\",\n",
        "    #     type=str,\n",
        "    #     default=\"reinforcement_learning/eval_task_with_embedding.pkl\",\n",
        "    #     help=\"Task file to use for evaluation\",\n",
        "    # )\n",
        "    # parser.add_argument(\n",
        "    #     \"-i\",\n",
        "    #     \"--task-index\",\n",
        "    #     dest=\"task_index\",\n",
        "    #     type=int,\n",
        "    #     default=None,\n",
        "    #     help=\"The index of the task to assign in the curriculum file\",\n",
        "    # )\n",
        "\n",
        "    # Parse and check the arguments\n",
        "    # eval_args = parser.parse_args()\n",
        "    # assert eval_args.policy_store_dir is not None, \"Policy store directory must be specified\"\n",
        "    if policy_store_dir is None:\n",
        "      print(\"Policy store directory must be specified\")\n",
        "      return\n",
        "\n",
        "    if replay_mode == False:\n",
        "        logging.info(\"Generating replays from the checkpoints in %s\", policy_store_dir)\n",
        "        save_replays(policy_store_dir,replay_save_dir,\n",
        "                     task_file, task_index)\n",
        "    else:\n",
        "        logging.info(\"Ranking checkpoints from %s\", policy_store_dir)\n",
        "        logging.info(\"Replays will NOT be generated\")\n",
        "        rank_policies(policy_store_dir, task_file, device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bjqJdYggUqLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKtuEShUUqTI",
        "outputId": "8cb7b579-5a54-4914-fcfe-812488d4f337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### train.py"
      ],
      "metadata": {
        "id": "btNPC8RNL90r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import torch\n",
        "\n",
        "from pufferlib.vectorization import Serial, Multiprocessing\n",
        "from pufferlib.policy_store import DirectoryPolicyStore\n",
        "from pufferlib.frameworks import cleanrl\n",
        "\n",
        "import environment\n",
        "\n",
        "from reinforcement_learning import clean_pufferl, policy, config\n",
        "\n",
        "# NOTE: this file changes when running curriculum generation track\n",
        "# Run test_task_encoder.py to regenerate this file (or get it from the repo)\n",
        "BASELINE_CURRICULUM_FILE = \"reinforcement_learning/curriculum_with_embedding.pkl\"\n",
        "CUSTOM_CURRICULUM_FILE = \"curriculum_generation/custom_curriculum_with_embedding.pkl\"\n",
        "\n",
        "def setup_env(args):\n",
        "    run_dir = os.path.join(args.runs_dir, args.run_name)\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "    logging.info(\"Training run: %s (%s)\", args.run_name, run_dir)\n",
        "    logging.info(\"Training args: %s\", args)\n",
        "\n",
        "    policy_store = None\n",
        "    if args.policy_store_dir is None:\n",
        "        args.policy_store_dir = os.path.join(run_dir, \"policy_store\")\n",
        "        logging.info(\"Using policy store from %s\", args.policy_store_dir)\n",
        "        policy_store = DirectoryPolicyStore(args.policy_store_dir)\n",
        "\n",
        "    def make_policy(envs):\n",
        "        learner_policy = policy.Baseline(\n",
        "            envs.driver_env,\n",
        "            input_size=args.input_size,\n",
        "            hidden_size=args.hidden_size,\n",
        "            task_size=args.task_size\n",
        "        )\n",
        "        return cleanrl.Policy(learner_policy)\n",
        "\n",
        "    # TODO: CHANGE POLCIY HERE\n",
        "\n",
        "    trainer = clean_pufferl.CleanPuffeRL(\n",
        "        device=torch.device(args.device),\n",
        "        seed=args.seed,\n",
        "        env_creator=environment.make_env_creator(args),\n",
        "        env_creator_kwargs={},\n",
        "        agent_creator=make_policy,\n",
        "        data_dir=run_dir,\n",
        "        exp_name=args.run_name,\n",
        "        policy_store=policy_store,\n",
        "        wandb_entity=args.wandb_entity,\n",
        "        wandb_project=args.wandb_project,\n",
        "        wandb_extra_data=args,\n",
        "        checkpoint_interval=args.checkpoint_interval,\n",
        "        vectorization=Serial if args.use_serial_vecenv else Multiprocessing,\n",
        "        total_timesteps=args.train_num_steps,\n",
        "        num_envs=args.num_envs,\n",
        "        num_cores=args.num_cores or args.num_envs,\n",
        "        num_buffers=args.num_buffers,\n",
        "        batch_size=args.rollout_batch_size,\n",
        "        learning_rate=args.ppo_learning_rate,\n",
        "        selfplay_learner_weight=args.learner_weight,\n",
        "        selfplay_num_policies=args.max_opponent_policies + 1,\n",
        "        #record_loss = args.record_loss,\n",
        "    )\n",
        "    return trainer\n",
        "\n",
        "def reinforcement_learning_track(trainer, args):\n",
        "    while not trainer.done_training():\n",
        "        trainer.evaluate()\n",
        "        trainer.train(\n",
        "            update_epochs=args.ppo_update_epochs,\n",
        "            bptt_horizon=args.bptt_horizon,\n",
        "            batch_rows=args.ppo_training_batch_size // args.bptt_horizon,\n",
        "            clip_coef=args.clip_coef,\n",
        "        )\n",
        "\n",
        "def curriculum_generation_track(trainer, args, use_elm=True):\n",
        "    from curriculum_generation.task_encoder import TaskEncoder\n",
        "    LLM_CHECKPOINT = \"Salesforce/codegen25-7b-instruct\"\n",
        "\n",
        "    if use_elm:\n",
        "        from curriculum_generation import manual_curriculum\n",
        "        from curriculum_generation.elm import OpenELMTaskGenerator\n",
        "        NUM_SEED_TASKS = 20\n",
        "        NUM_NEW_TASKS = 5\n",
        "        ELM_DEBUG = True\n",
        "\n",
        "        task_encoder = TaskEncoder(LLM_CHECKPOINT, manual_curriculum, batch_size=2)\n",
        "        task_generator = OpenELMTaskGenerator(manual_curriculum.curriculum, LLM_CHECKPOINT)\n",
        "\n",
        "        # Generating new tasks and evaluating all candidate training tasks\n",
        "        for _ in range(3):\n",
        "            # NOTE: adjust NUM_SEED_TASKS to fit your gpu\n",
        "            seed_task_list = task_generator.sample_tasks(NUM_SEED_TASKS, random_ratio=1)\n",
        "            new_task_list = task_generator.evolve_tasks(seed_task_list, NUM_NEW_TASKS, debug=ELM_DEBUG)\n",
        "            task_generator.add_tasks(new_task_list)\n",
        "            task_encoder.get_task_embedding(seed_task_list + new_task_list, save_to_file=CUSTOM_CURRICULUM_FILE)\n",
        "            # CHECK ME: the trainer will automatically use the new task embedding file\n",
        "            _, _, infos = trainer.evaluate()\n",
        "            task_generator.update(infos) # update the task stats\n",
        "\n",
        "        # NOTE: sample_tasks() uses task stats to sample learnable tasks\n",
        "        curriculum = task_generator.sample_tasks(NUM_SEED_TASKS*3, random_ratio=0.3) # NOTE: arbitrary numbers\n",
        "\n",
        "    else:\n",
        "        from curriculum_generation import curriculum_tutorial  # custom tutorial\n",
        "        task_encoder = TaskEncoder(LLM_CHECKPOINT, curriculum_tutorial, batch_size=2)\n",
        "        curriculum = curriculum_tutorial.curriculum\n",
        "\n",
        "    # Use the train_task_spec to train agents\n",
        "    task_encoder.get_task_embedding(curriculum, save_to_file=CUSTOM_CURRICULUM_FILE)\n",
        "    task_encoder.close()\n",
        "    trainer.data.sort_keys = []\n",
        "    reinforcement_learning_track(trainer, args)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    # You can either edit the defaults in config.py or set args\n",
        "    # from the commandline.\n",
        "    args = config.create_config(config.Config)\n",
        "\n",
        "    # Avoid OOMing your machine for local testing\n",
        "    if args.local_mode:\n",
        "        args.num_envs = 1\n",
        "        args.num_buffers = 1\n",
        "        args.use_serial_vecenv = True\n",
        "        args.rollout_batch_size = 2**10\n",
        "\n",
        "    if args.track == \"rl\":\n",
        "      args.tasks_path = BASELINE_CURRICULUM_FILE\n",
        "      trainer = setup_env(args)\n",
        "      reinforcement_learning_track(trainer, args)\n",
        "    elif args.track == \"curriculum\":\n",
        "      args.tasks_path = CUSTOM_CURRICULUM_FILE\n",
        "      trainer = setup_env(args)\n",
        "      curriculum_generation_track(trainer, args, use_elm=True)\n",
        "    else:\n",
        "      raise ValueError(f\"Unknown track {args.track}, must be 'rl' or 'curriculum'\")\n",
        "\n",
        "    trainer.close()"
      ],
      "metadata": {
        "id": "CjENcJY8L5tb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "056ef5a7-90d0-45a7-fe5d-a294bec1a71a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--attend-task ATTEND_TASK]\n",
            "                                [--attentional-decode ATTENTIONAL_DECODE]\n",
            "                                [--bptt-horizon BPTT_HORIZON]\n",
            "                                [--checkpoint-interval CHECKPOINT_INTERVAL]\n",
            "                                [--clip-coef CLIP_COEF] [--death-fog-tick DEATH_FOG_TICK]\n",
            "                                [--device DEVICE] [--early-stop-agent-num EARLY_STOP_AGENT_NUM]\n",
            "                                [--encode-task ENCODE_TASK] [--eval-batch-size EVAL_BATCH_SIZE]\n",
            "                                [--eval-mode EVAL_MODE] [--eval-num-policies EVAL_NUM_POLICIES]\n",
            "                                [--eval-num-rounds EVAL_NUM_ROUNDS]\n",
            "                                [--eval-num-steps EVAL_NUM_STEPS]\n",
            "                                [--explore-bonus-weight EXPLORE_BONUS_WEIGHT]\n",
            "                                [--extra-encoders EXTRA_ENCODERS]\n",
            "                                [--heal-bonus-weight HEAL_BONUS_WEIGHT]\n",
            "                                [--hidden-size HIDDEN_SIZE] [--input-size INPUT_SIZE]\n",
            "                                [--learner-weight LEARNER_WEIGHT] [--local-mode LOCAL_MODE]\n",
            "                                [--map-size MAP_SIZE] [--maps-path MAPS_PATH]\n",
            "                                [--max-episode-length MAX_EPISODE_LENGTH]\n",
            "                                [--max-opponent-policies MAX_OPPONENT_POLICIES]\n",
            "                                [--meander-bonus-weight MEANDER_BONUS_WEIGHT]\n",
            "                                [--num-agents NUM_AGENTS] [--num-buffers NUM_BUFFERS]\n",
            "                                [--num-cores NUM_CORES] [--num-envs NUM_ENVS]\n",
            "                                [--num-lstm-layers NUM_LSTM_LAYERS] [--num-maps NUM_MAPS]\n",
            "                                [--num-npcs NUM_NPCS] [--policy-store-dir POLICY_STORE_DIR]\n",
            "                                [--ppo-learning-rate PPO_LEARNING_RATE]\n",
            "                                [--ppo-training-batch-size PPO_TRAINING_BATCH_SIZE]\n",
            "                                [--ppo-update-epochs PPO_UPDATE_EPOCHS]\n",
            "                                [--resilient-population RESILIENT_POPULATION]\n",
            "                                [--rollout-batch-size ROLLOUT_BATCH_SIZE] [--run-name RUN_NAME]\n",
            "                                [--runs-dir RUNS_DIR] [--seed SEED]\n",
            "                                [--spawn-immunity SPAWN_IMMUNITY]\n",
            "                                [--sqrt-achievement-rewards SQRT_ACHIEVEMENT_REWARDS]\n",
            "                                [--task-size TASK_SIZE] [--tasks-path TASKS_PATH] [--track TRACK]\n",
            "                                [--train-num-steps TRAIN_NUM_STEPS]\n",
            "                                [--use-serial-vecenv USE_SERIAL_VECENV]\n",
            "                                [--wandb-entity WANDB_ENTITY] [--wandb-project WANDB_PROJECT]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-38bab612-01d0-4a37-a6b4-4f76fecdb725.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n"
      ],
      "metadata": {
        "id": "tk7EWwoMSadK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhjYS0vD6JR1"
      },
      "outputs": [],
      "source": [
        "!python evaluate.py -p <dir with policy >\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M7YkmMq0OfDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_dir_run = ckpt_dir + \"nmmo_20231123_075211\" # CHANGE WITH CURRENT RUN"
      ],
      "metadata": {
        "id": "3KZ_PN8_OfHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Y_phYGBcSZmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python evaluate.py -p $ckpt_dir_run -r\n"
      ],
      "metadata": {
        "id": "XKFPvpmFOdRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission"
      ],
      "metadata": {
        "id": "wtILQfPVSevI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0CfgWUYbTGDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the ssh key with your aicrowd email\n",
        "# WARNING: Having your ssh key way is not secure, so you should limit using this key for others\n",
        "\n",
        "my_email = \"catherinelee274@yahoo.com\"  # YOUR AICROWD EMAIL\n",
        "\n",
        "# See the top for the work_dir, which should be in your google drive\n",
        "ssh_dir = work_dir + \"ssh_key/\"\n",
        "key_file = ssh_dir + \"id_rsa\"\n",
        "\n",
        "%cd $work_dir\n",
        "!mkdir $ssh_dir\n",
        "!ssh-keygen -t rsa -b 4096 -C $my_email -f $key_filefs"
      ],
      "metadata": {
        "id": "Q0Wk8hLYSe6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the below text that starts with ssh-rsa to https://gitlab.aicrowd.com/-/profile/keys\n",
        "ssh_dir = work_dir + \"ssh_key/\"\n",
        "key_file = ssh_dir + \"id_rsa\"\n",
        "\n",
        "!cat {key_file}.pub"
      ],
      "metadata": {
        "id": "YJEBvfoYaSuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the key to default ssh key path - you should see id_rsa\n",
        "!mkdir /root/.ssh\n",
        "!cp {key_file}* /root/.ssh\n",
        "!ls /root/.ssh\n",
        "!chmod 700 /root/.ssh\n",
        "\n",
        "# Add the git server as a ssh known host\n",
        "!touch /root/.ssh/known_hosts\n",
        "!ssh-keyscan gitlab.aicrowd.com >> /root/.ssh/known_hosts\n",
        "!chmod 644 /root/.ssh/known_hosts\n",
        "\n",
        "# You should see something like: Welcome to GitLab, @catherine_lee!\n",
        "# to clone the repo and submit\n",
        "!ssh -T git@gitlab.aicrowd.com"
      ],
      "metadata": {
        "id": "xEDTMuPNaTyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the submission kit repo\n",
        "%cd $work_dir\n",
        "!git clone git@gitlab.aicrowd.com:Mudou/start-kit.git"
      ],
      "metadata": {
        "id": "LdQ8g34AaVAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $work_dir\n",
        "%cd start-kit/\n",
        "\n",
        "# Fix permissions\n",
        "!chmod +x .git/hooks/*\n",
        "\n",
        "# Install requirements\n",
        "!pip install -r requirements.txt > /dev/null"
      ],
      "metadata": {
        "id": "jn6FZhdyaT00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Edit the aicrowd.json -- INCLUDE YOUR NAME\n",
        "\n",
        "with open(work_dir+'< COD3E DIREDFTOY ?>/aicrowd.json', \"w\") as f:\n",
        "  f.write(\"\"\"\n",
        "{\n",
        "    \"challenge_id\" : \"neurips-2023-the-neural-mmo-challenge\",\n",
        "    \"authors\" : [\"catherine_lee\"],\n",
        "    \"description\" : \"starter test 2\"\n",
        "}\n",
        "  \"\"\")\n"
      ],
      "metadata": {
        "id": "X8RoSxnraaue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLICK THE LINK to authenticate into aicrowd\n",
        "%cd $work_dir\n",
        "%cd start-kit/\n",
        "\n",
        "!python tool.py submit \"track1-submission-tutorial-1\""
      ],
      "metadata": {
        "id": "xsa6pNbBafJt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Z6Ky2YGKH6JL"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}